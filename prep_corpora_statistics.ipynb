{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import spacy\n",
    "import gensim.models.phrases\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_lexicon(dataset_name, column_name, delimiter):\n",
    "    dataset = pd.read_csv(\"lll/\" + dataset_name + \".csv\", delimiter=delimiter)\n",
    "    terms = [dataset[column_name].tolist()]\n",
    "    dic = gs.corpora.Dictionary(terms)\n",
    "    num_unigrams = len(dic)\n",
    "    print(dataset_name)\n",
    "    print(\"...num unigrams\", num_unigrams)\n",
    "    \n",
    "def get_statistics_corpus(dataset_name, column_name, delimiter):\n",
    "    dataset = pd.read_csv(\"lll/\" + dataset_name + \".csv\", delimiter=delimiter)\n",
    "    terms = dataset[column_name].tolist()\n",
    "    all_sentences = []\n",
    "    for sentence in terms: \n",
    "        doc = nlp(sentence)\n",
    "        sent = [token.text for token in doc]\n",
    "        all_sentences.append(sent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crowdflower2 ['content', ',']\n",
      "content\n",
      "[['@tiffanylue', 'i', 'know', ' ', 'i', 'was', 'listenin', 'to', 'bad', 'habit', 'earlier', 'and', 'i', 'started', 'freakin', 'at', 'his', 'part', '=', '['], ['Layin', 'n', 'bed', 'with', 'a', 'headache', ' ', 'ughhhh', '...', 'waitin', 'on', 'your', 'call', '...'], ['Funeral', 'ceremony', '...', 'gloomy', 'friday', '...'], ['wants', 'to', 'hang', 'out', 'with', 'friends', 'SOON', '!'], ['@dannycastillo', 'We', 'want', 'to', 'trade', 'with', 'someone', 'who', 'has', 'Houston', 'tickets', ',', 'but', 'no', 'one', 'will', '.'], ['Re', '-', 'pinging', '@ghostridah14', ':', 'why', 'did', \"n't\", 'you', 'go', 'to', 'prom', '?', 'BC', 'my', 'bf', 'did', \"n't\", 'like', 'my', 'friends'], ['I', 'should', 'be', 'sleep', ',', 'but', 'i', 'm', 'not', '!', 'thinking', 'about', 'an', 'old', 'friend', 'who', 'I', 'want', '.', 'but', 'he', \"'s\", 'married', 'now', '.', 'damn', ',', '&', 'amp', ';', 'he', 'wants', 'me', '2', '!', 'scandalous', '!'], ['Hmmm', '.', 'http://www.djhero.com/', 'is', 'down'], ['@charviray', 'Charlene', 'my', 'love', '.', 'I', 'miss', 'you'], ['@kelcouch', 'I', \"'m\", 'sorry', ' ', 'at', 'least', 'it', \"'s\", 'Friday', '?'], ['ca', 'nt', 'fall', 'asleep'], ['Choked', 'on', 'her', 'retainers'], ['Ugh', '!', 'I', 'have', 'to', 'beat', 'this', 'stupid', 'song', 'to', 'get', 'to', 'the', 'next', ' ', 'rude', '!'], ['@BrodyJenner', 'if', 'u', 'watch', 'the', 'hills', 'in', 'london', 'u', 'will', 'realise', 'what', 'tourture', 'it', 'is', 'because', 'were', 'weeks', 'and', 'weeks', 'late', ' ', 'i', 'just', 'watch', 'itonlinelol'], ['Got', 'the', 'news'], ['The', 'storm', 'is', 'here', 'and', 'the', 'electricity', 'is', 'gone'], ['@annarosekerr', 'agreed'], ['So', 'sleepy', 'again', 'and', 'it', \"'s\", 'not', 'even', 'that', 'late', '.', 'I', 'fail', 'once', 'again', '.'], ['@PerezHilton', 'lady', 'gaga', 'tweeted', 'about', 'not', 'being', 'impressed', 'by', 'her', 'video', 'leaking', 'just', 'so', 'you', 'know'], ['How', 'are', 'YOU', 'convinced', 'that', 'I', 'have', 'always', 'wanted', 'you', '?', 'What', 'signals', 'did', 'I', 'give', 'off', '...', 'damn', 'I', 'think', 'I', 'just', 'lost', 'another', 'friend'], ['@raaaaaaek', 'oh', 'too', 'bad', '!', 'I', 'hope', 'it', 'gets', 'better', '.', 'I', \"'ve\", 'been', 'having', 'sleep', 'issues', 'lately', 'too'], ['Wondering', 'why', 'I', \"'m\", 'awake', 'at', '7am', ',', 'writing', 'a', 'new', 'song', ',', 'plotting', 'my', 'evil', 'secret', 'plots', 'muahahaha', '...', 'oh', 'damn', 'it', ',', 'not', 'secret', 'anymore'], ['No', 'Topic', 'Maps', 'talks', 'at', 'the', 'Balisage', 'Markup', 'Conference', '2009', '  ', 'Program', 'online', 'at', 'http://tr.im/mL6Z', '(', 'via', '@bobdc', ')', '#', 'topicmaps'], ['I', 'ate', 'Something', 'I', 'do', \"n't\", 'know', 'what', 'it', 'is', '...', 'Why', 'do', 'I', 'keep', 'Telling', 'things', 'about', 'food'], ['so', 'tired', 'and', 'i', 'think', 'i', \"'m\", 'definitely', 'going', 'to', 'get', 'an', 'ear', 'infection', '.', ' ', 'going', 'to', 'bed', '&', 'quot;early&quot', ';', 'for', 'once', '.'], ['On', 'my', 'way', 'home', 'n', 'having', '2', 'deal', 'w', 'underage', 'girls', 'drinking', 'gin', 'on', 'da', 'bus', 'while', 'talking', 'bout', 'keggers', '......', 'damn', 'i', 'feel', 'old'], ['@IsaacMascote', ' ', 'i', \"'m\", 'sorry', 'people', 'are', 'so', 'rude', 'to', 'you', ',', 'isaac', ',', 'they', 'should', 'get', 'some', 'manners', 'and', 'know', 'better', 'than', 'to', 'be', 'so', 'lewd', '!'], ['Damm', 'servers', 'still', 'down', ' ', 'i', 'need', 'to', 'hit', '80', 'before', 'all', 'the', 'koxpers', 'pass', 'me'], ['Fudge', '....', 'Just', \"BS'd\", 'that', 'whole', 'paper', '....', 'So', 'tired', '....', 'Ugh', 'I', 'hate', 'school', '.....', ' ', 'time', 'to', 'sleep', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!'], ['I', 'HATE', 'CANCER', '.', 'I', 'HATE', 'IT', 'I', 'HATE', 'IT', 'I', 'HATE', 'IT', '.'], ['It', 'is', 'so', 'annoying', 'when', 'she', 'starts', 'typing', 'on', 'her', 'computer', 'in', 'the', 'middle', 'of', 'the', 'night', '!'], ['@cynthia_123', 'i', 'ca', 'nt', 'sleep'], ['I', 'missed', 'the', 'bl***y', 'bus', '!', '!', '!', '!', '!', '!', '!', '!'], ['feels', 'strong', 'contractions', 'but', 'wants', 'to', 'go', 'out', '.', ' ', 'http://plurk.com/p/wxidk'], ['SoCal', '!', ' ', 'stoked', '.', 'or', 'maybe', 'not', '..', 'tomorrow']]\n",
      "...num unigrams: 305\n",
      "...num bigrams: 473\n",
      "...num trigrams: 464\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"crowdflower\": [\"content\", \",\"],\n",
    "    \"emoint\": [\"text\", \"\\t\"],\n",
    "    \"tec\": [\"text\", \"\\t\"],\n",
    "    \"emotion_classification\": [\"text\", \",\"]\n",
    "}\n",
    "\n",
    "#data = {\n",
    "#    \"crowdflower2\": [\"content\", \",\"]\n",
    "#}\n",
    "\n",
    "\n",
    "for key, value in data.items():\n",
    "    print(key, value)\n",
    "    print(value[0])\n",
    "    dataset = pd.read_csv(\"lll/\" + key + \".csv\", delimiter=value[1], error_bad_lines=False)\n",
    "    terms = dataset[value[0]].tolist()\n",
    "    all_sentences = []\n",
    "    for sentence in terms: \n",
    "        doc = nlp(sentence)\n",
    "        sent = [token.text for token in doc]\n",
    "        all_sentences.append(sent)\n",
    "    # get unigrams\n",
    "    dic = gs.corpora.Dictionary(all_sentences)\n",
    "    print(\"...num unigrams:\", len(dic))\n",
    "    # get bigrams\n",
    "    all_bgr = []\n",
    "    all_tgr = []\n",
    "    for sent in all_sentences: \n",
    "        all_bgr.append(list(ngrams(sent,2)))\n",
    "        all_tgr.append(list(ngrams(sent,3)))\n",
    "    flat_bgr = [item for sublist in all_bgr for item in sublist]\n",
    "    flat_tgr = [item for sublist in all_tgr for item in sublist]\n",
    "    flat_bgr = list(dict.fromkeys(flat_bgr))\n",
    "    flat_tgr = list(dict.fromkeys(flat_tgr))\n",
    "    print(\"...num bigrams:\", len(flat_bgr))\n",
    "    print(\"...num trigrams:\", len(flat_tgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
