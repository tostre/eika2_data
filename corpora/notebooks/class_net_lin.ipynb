{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import gensim.models.coherencemodel\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_hidden_layers):\n",
    "        super(Lin_Net, self).__init__()\n",
    "        self.act_function = nn.ReLU()\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.hidden_layers = [nn.Linear(hidden_dim, hidden_dim) for i in range(0, num_hidden_layers)]\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(0, num_hidden_layers)])\n",
    "        self.lin4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_function(self.lin1(x))\n",
    "        for hidden_layer in self.hidden_layers: \n",
    "            x = self.act_function(hidden_layer(x))\n",
    "        x = self.lin4(x)\n",
    "        return x\n",
    "    \n",
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lex_data(dataset_name, feature_set_name, features, batch_size, split_factor=0.2):\n",
    "    print(\"loading lex data\", dataset_name, feature_set_name)\n",
    "    inputs = []\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    targets = dataset[\"a\"]\n",
    "    inputs = dataset[features]\n",
    "    return make_loader(inputs, targets, split_factor)\n",
    "    \n",
    "def load_vector_data(dataset_name, bgr=False, split_factor=0.2):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"]\n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    tokenized = sentences\n",
    "    if bgr:\n",
    "        bigram = Phraser.load(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")\n",
    "        bigrammed = [bigram[sentence] for sentence in sentences]\n",
    "        tokenized = bigrammed\n",
    "    inputs = [np.sum(vector_model.wv[sent], 0).tolist() if sent else np.zeros(32) for sent in tokenized]   \n",
    "    inputs = np.array(inputs)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    return len(inputs[0]), train_loader, val_loader, test_loader\n",
    "\n",
    "def load_topic_data(dataset_name, split_factor=0.2):\n",
    "    print(\"loading lex data\", dataset_name, feature_set_name)\n",
    "    inputs = []\n",
    "    num_topics = num_topics_dict[dataset_name]\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    targets = dataset[\"a\"]\n",
    "    dataset = dataset.astype(str).values.tolist() \n",
    "    dic = gs.corpora.Dictionary.load(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")   \n",
    "    print(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    for index, sample in enumerate(dataset): \n",
    "        dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    for i, sample in enumerate(dataset):\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        inputs.append(topic_vec)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    topics_num = len(lda_model.get_topics())\n",
    "    return topics_num, train_loader, val_loader, test_loader\n",
    "    \n",
    "\n",
    "def make_loader(inputs, targets, test_size):\n",
    "    # make train and test sets\n",
    "    train_x, val_x, train_y, val_y = train_test_split(inputs, targets, test_size=split_factor)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=split_factor)\n",
    "    train_data = MyDataset(np.asarray(train_x), np.asarray(train_y))\n",
    "    val_data = MyDataset(np.asarray(val_x), np.asarray(val_y))\n",
    "    test_data = MyDataset(np.asarray(test_x), np.asarray(test_y))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=round(batch_size*split_factor))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda(cuda, inputs, targets, net):\n",
    "    if cuda: \n",
    "        return inputs.cuda(), targets.cuda(), net.cuda()\n",
    "    else: \n",
    "        return inputs, targets, net\n",
    "    \n",
    "def log(file_name, message, net=False, epoch=False):\n",
    "    print(message)\n",
    "    log = open(\"{}{}_{}\".format(\"../logs/\", file_name, \".txt\"), \"a\")\n",
    "    log.write(message)\n",
    "    log.close()\n",
    "    # save net\n",
    "    if net and epoch: \n",
    "        torch.save(net.state_dict(), \"{}{}_{}{}\".format(\"../nets/\", file_name, epoch, \".pt\"))\n",
    "\n",
    "def plot_intersection(file_name, plot_type, y1, y2, desc=True, intersection=False):\n",
    "    x, y1, y2  = list(range(1, len(y1)+1)), np.asarray(y1), np.asarray(y2)\n",
    "    epochs = len(x)\n",
    "    fig = plt.figure()\n",
    "    plt.clf()\n",
    "    if plot_type == \"f1_score\":\n",
    "        max_y, max_x = round(max(y2),2), y2.tolist().index(max(y2))+1\n",
    "        desc = \"\\ndataset: {}\\nbest validation_f1_score = {}, in epoch {}\\n blue=train_f1, green=val_f1\".format(\n",
    "                file_name, max_y, max_x)\n",
    "        plt.plot(x, y1, \"b-\", x, y2, \"g-\", max_x, max_y, \"ro\")\n",
    "        plt.ylim(0,1.1)\n",
    "    elif plot_type == \"loss\":\n",
    "        desc = \"\\ndataset: {}\\n blue=train_f1, green=val_f1\".format(file_name)\n",
    "        plt.plot(x, y1, \"b-\", x, y2, \"g-\")\n",
    "        plt.ylim(0,max([round(max(y1),2), round(max(y2),2)]))\n",
    "    plt.ylabel(plot_type)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.grid()\n",
    "    plt.xlim(0, epochs)\n",
    "    fig.text(0.5, -0.15, desc.replace(\"../logs/\", \"\"), ha='center')\n",
    "    fig.savefig(\"{}{}_{}{}\".format(\"../img/\", file_name, plot_type, \"_intersection.png\"), bbox_inches=\"tight\")\n",
    "    \n",
    "def draw_confusion_matrix(file_name, test_y, pred_y, f1_score): \n",
    "    fig = plt.figure()\n",
    "    hm = sn.heatmap(confusion_matrix(test_y, pred_y), fmt=\"d\", linewidth=0.5, annot=True, square=True, xticklabels=[\"h\", \"s\", \"a\", \"f\"], yticklabels=[\"h\", \"s\", \"a\", \"f\"], cmap=\"PuRd\")\n",
    "    ax1 = fig.add_axes(hm)\n",
    "    ax1.set(xlabel=\"predicted\", ylabel=\"target\")\n",
    "    desc = \"dataset: {}\".format(file_name)\n",
    "    #if feature_set_name == \"topics\": desc = \"dataset: {} ({}), trained with {} topics\\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, num_topics, round(score,2), round(f1_scoore,2))\n",
    "    #else: desc = \"dataset: {} ({}) \\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, round(score,2), round(f1_scoore,2)) \n",
    "    desc = \"dataset: {} ({}) \\nf1_score: {}\".format(dataset_name, feature_set_name, round(f1_score,2)) \n",
    "    fig.text(0.5, -0.1, desc, ha='center')\n",
    "    plt.show()\n",
    "    fig.savefig(\"{}{}_{}\".format(\"../img/\", file_name, \"confusion.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every):\n",
    "    print(\"training\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    train_f1, val_f1, train_e, val_e = [], [], [], []\n",
    "    # training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # for every batch in the train_loader\n",
    "        net.train()\n",
    "        for index, (train_inputs, train_targets) in enumerate(train_loader):\n",
    "            train_inputs, train_targets = train_inputs.float(), train_targets.long()\n",
    "            train_inputs, train_targets, net = convert_to_cuda(cuda, train_inputs, train_targets, net)\n",
    "            train_pred = net(train_inputs)\n",
    "            train_loss = criterion(train_pred.float(), train_targets)\n",
    "            optimizer.zero_grad(); train_loss.backward(); optimizer.step()# save error\n",
    "            train_pred = [item.index(max(item)) for item in train_pred.tolist()]\n",
    "            tf1, tloss = f1_score(train_targets.tolist(), train_pred, average=\"weighted\"), train_loss.item()\n",
    "        net.eval()\n",
    "        for index, (val_inputs, val_targets) in enumerate(val_loader):\n",
    "            val_inputs, val_targets = val_inputs.float(), val_targets.long()\n",
    "            val_inputs, val_targets, net = convert_to_cuda(cuda, val_inputs, val_targets, net)\n",
    "            val_pred = net(val_inputs)\n",
    "            val_loss = criterion(val_pred.float(), val_targets)\n",
    "            val_pred = [item.index(max(item)) for item in val_pred.tolist()]\n",
    "            vf1, vloss = f1_score(val_targets.tolist(), val_pred, average=\"weighted\"), val_loss.item()\n",
    "        train_f1.append(f1_score(train_targets.tolist(), train_pred, average=\"weighted\"))\n",
    "        val_f1.append(f1_score(val_targets.tolist(), val_pred, average=\"weighted\"))\n",
    "        train_e.append(train_loss.item())\n",
    "        val_e.append(val_loss.item())\n",
    "        # write logs and files \n",
    "        if epoch % print_every == 0:\n",
    "            log(file_name, \"\\nepoch: {}, \\n...train_f1: {}, train_loss: {}, \\tval_f1: {}, val_loss: {}\".format(\n",
    "                epoch, train_f1[-1:], train_e[-1:], val_f1[-1:], val_e[-1:]), net=net, epoch=epoch)\n",
    "    plot_intersection(file_name, \"f1_score\", train_f1, val_f1)\n",
    "    plot_intersection(file_name, \"loss\", train_e, val_e)\n",
    "\n",
    "def test(test_loader, net, file_name): \n",
    "    print(\"testing\")\n",
    "    all_targets, preds, test_e = [], [], []\n",
    "    net.eval()\n",
    "    for index, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, test_targets = inputs.float(), targets.long()\n",
    "        inputs, test_targets, net = convert_to_cuda(cuda, inputs, targets, net)\n",
    "        test_pred = net(inputs).tolist()[0]\n",
    "        preds.append(test_pred.index(max(test_pred)))\n",
    "        all_targets.append(test_targets.tolist()[0])\n",
    "    test_f1 = f1_score(all_targets, preds, average=\"weighted\")\n",
    "    with open(\"../reports/report_\"+file_name+\".txt\", 'w') as f:\n",
    "        print(file_name + \", f1_score: \" + str(test_f1) + \":\\n\\n\" + \n",
    "          classification_report(all_targets, preds,target_names=classes), file=f)\n",
    "    log(file_name, \"\\n...test_f1: {}\".format(test_f1))\n",
    "    draw_confusion_matrix(file_name, all_targets, preds, test_f1)\n",
    "\n",
    "def run(dataset_name, feature_set_name):\n",
    "    file_name = \"net_lin_{}({})\".format(dataset_name, feature_set_name)\n",
    "    print(\"running \", file_name)\n",
    "    open(\"{}{}_{}\".format(\"../logs/\", file_name, \".txt\"), \"w\").close()\n",
    "    if feature_set_name == \"topics\":\n",
    "        num_topics, train_loader, val_loader, test_loader = load_topic_data(dataset_name)\n",
    "        net = Lin_Net(num_topics, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-unigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name)\n",
    "        net = Lin_Net(input_dim, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-bigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name, True)\n",
    "        net = Lin_Net(input_dim, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "    else: \n",
    "        train_loader, val_loader, test_loader = load_lex_data(dataset_name, feature_set_name, feature_sets[dataset_name + \"_\" + feature_set_name], batch_size, split_factor)\n",
    "        net = Lin_Net(len(feature_sets[dataset_name + \"_\" + feature_set_name]), output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables\n"
     ]
    }
   ],
   "source": [
    "# create variables \n",
    "print(\"creating variables\")\n",
    "feature_set_names = [\"full\", \"half\", \"vec-unigram\", \"vec-bigram\", \"topic\"]\n",
    "feature_sets = {\n",
    "    \"norm_test_full\": [\"wc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"norm_test_lex\": [\"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"norm_emotion_full\": [\"wc\", \"ewc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"norm_emotion_lex\": [\"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"norm_tweet_full\": [\"wc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"norm_tweet_lex\": [\"hc\", \"sc\", \"ac\", \"fc\"]\n",
    "}\n",
    "num_topics_dict = {\n",
    "    \"norm_emotion\": 186,\n",
    "    \"norm_tweet\": 79,\n",
    "    \"norm_test\": 79\n",
    "}\n",
    "classes = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "\n",
    "types = {\n",
    "    \"text\": object, \n",
    "    \"a\": int, \n",
    "    \"wc\": float,\n",
    "    \"uwc\": float,\n",
    "    \"ewc\": float,\n",
    "    \"cpc\": float,\n",
    "    \"hc\": float,\n",
    "    \"sc\": float,\n",
    "    \"ac\": float,\n",
    "    \"c\": float\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "cuda = torch.cuda.is_available()\n",
    "batch_size = 16\n",
    "epochs = 5000 + 1\n",
    "print_every = 125\n",
    "split_factor = 0.2\n",
    "output_dim = 4\n",
    "hidden_dim = 256\n",
    "num_hidden_layers = 2\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  net_lin_norm_tweet(topics)\n",
      "loading lex data norm_tweet topics\n",
      "dataset_name norm_tweet\n",
      "num_topics 79\n",
      "../models/topic_models/norm_tweet_ldamodel\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-962c5cdf33e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"norm_tweet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"norm_emotion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature_set_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vec-unigram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vec-bigram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_set_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-326-0993613d61a7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_name, feature_set_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}{}_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../logs/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_set_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_topic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLin_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-324-378cbbe9d1ec>\u001b[0m in \u001b[0;36mload_topic_data\u001b[0;34m(dataset_name, split_factor)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print(num_topics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print(np.asarray(topics).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;31m# phinorm is the normalizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;31m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;31m# Iterate between gamma and phi until convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = [\"norm_tweet\", \"norm_emotion\"]\n",
    "feature_set_names = [\"full\", \"half\", \"topic\"]\n",
    "datasets = [\"norm_test\"]\n",
    "feature_set_names = [\"full\"]\n",
    "\n",
    "for dataset_name in [\"norm_tweet\", \"norm_emotion\"]: \n",
    "    for feature_set_name in [\"topics\", \"vec-unigram\", \"vec-bigram\", \"full\", \"lex\", ]: \n",
    "        run(dataset_name, feature_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
