{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(summary):\n",
    "    print(summary)\n",
    "    \n",
    "def combine_datasets(list_of_datasets, lexica=False):\n",
    "    print(\"make_data\", list_of_datasets)\n",
    "    if not lexica: \n",
    "        datasets = []\n",
    "        for dataset_name in list_of_datasets:\n",
    "            datasets.append(pd.read_csv(\"../raw/\" + dataset_name + \".csv\")) \n",
    "        dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        lexica = []\n",
    "        for dataset_name in list_of_datasets: \n",
    "            lexicon = pd.read_csv(\"../../lexica/\" + dataset_name + \".csv\")\n",
    "            lexica.append(lexicon[\"stems\"].tolist())\n",
    "        return lexica\n",
    "    \n",
    "def normalize_label_length(list_of_datasets, save_name):\n",
    "    print(\"normalize_label_length\")\n",
    "    df = combine_datasets(list_of_datasets)\n",
    "    df = df.sample(frac=1)\n",
    "    l = min(\n",
    "        len(df.loc[df[\"affect\"] == \"happiness\"]), \n",
    "        len(df.loc[df[\"affect\"] == \"sadness\"]), \n",
    "        len(df.loc[df[\"affect\"] == \"anger\"]), \n",
    "        len(df.loc[df[\"affect\"] == \"fear\"]))\n",
    "    norm_df = pd.concat([\n",
    "        df.loc[df[\"affect\"] == \"happiness\"][:l],\n",
    "        df.loc[df[\"affect\"] == \"sadness\"][:l],\n",
    "        df.loc[df[\"affect\"] == \"anger\"][:l],\n",
    "        df.loc[df[\"affect\"] == \"fear\"][:l]])\n",
    "    df = df.sample(frac=1)\n",
    "    norm_df.to_csv(\"../cleaned/norm_\" + save_name + \".csv\", index=False,  float_format='%.3f')\n",
    "    \n",
    "def extract_features(dataset_name, list_of_lexica, print_row): \n",
    "    affect_encoding = {\"happiness\": 0, \"sadness\": 1, \"anger\": 2, \"fear\": 3}\n",
    "    cleaned_dataset = []\n",
    "    wc, pos, stems = [], [], []\n",
    "    dataset = pd.read_csv(\"../cleaned/norm_\" + dataset_name + \".csv\")\n",
    "    # pos 97 = Satzzeichen, 103 = Leerzeichen\n",
    "    print(\"analyzing sentence features:\", dataset_name)\n",
    "    #for index, row in dataset.iterrows()[165800:]:\n",
    "    for index, row in dataset.iloc[165800:].iterrows():\n",
    "        if index % print_row == 0: log(\"... searching row \" + str(index) + \"/\" + str(len(dataset)))\n",
    "        doc = nlp(split_punct(row[\"text\"]))\n",
    "        doc = nlp(\" \".join([token.text for token in doc if not token.is_stop and token.pos != 103]))\n",
    "        if len(doc) != 0:\n",
    "            pos.append([token.pos for token in doc])\n",
    "            stems.append([stemmer.stem(token.text) for token in doc if token.pos != 97])\n",
    "            emotion_words = get_emotion_words(stems[-1:][0], list_of_lexica)\n",
    "            cleaned_dataset.append([\n",
    "                \" \".join([token.text for token in doc]), affect_encoding[row[\"affect\"]],\n",
    "                len(doc), (sum([token.text.isupper() for token in doc])/len(doc)), \n",
    "                (len(doc.ents)/len(doc)),get_cons_punct_count(pos[-1:][0]), \n",
    "                emotion_words[0]/len(doc), emotion_words[1]/len(doc), emotion_words[2]/len(doc), emotion_words[3]/len(doc)])\n",
    "        \n",
    "    seq_len = max([row[2] for row in cleaned_dataset])\n",
    "    pos = extend_list(pos, seq_len, 0)\n",
    "    stems = extend_list(stems, seq_len, \" \")    \n",
    "    \n",
    "    df = pd.DataFrame(data=cleaned_dataset, columns=[\"t\", \"a\", \"wc\", \"uwc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"fc\", \"ac\"])\n",
    "    df[\"wc\"] = [(item/seq_len) for item in df[\"wc\"].tolist()] # normalisiert über die größte anzahl von wörtern in einem sample\n",
    "    df.to_csv(\"../cleaned/norm_\" + dataset_name + \"2_clean.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    df = pd.DataFrame(data=pos)\n",
    "    df.to_csv(\"../cleaned/norm_\" + dataset_name + \"2_pos.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    df = pd.DataFrame(data=stems)\n",
    "    df.to_csv(\"../cleaned/norm_\" + dataset_name + \"2_stems.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    \n",
    "def extend_list(l, seq_len, extension):\n",
    "    for index, row in enumerate(l):\n",
    "        row.extend([extension] * (seq_len - len(row)))\n",
    "    return l\n",
    "    \n",
    "def split_punct(text):\n",
    "    replacement = [(\".\", \" . \"), (\",\", \" , \"), (\"!\", \" ! \"), (\"?\", \" ? \")]\n",
    "    for k, v in replacement: \n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "    \n",
    "def get_emotion_words(stems, list_of_lexica):\n",
    "    emotion_words = np.zeros(4)\n",
    "    for index, lexicon in enumerate(list_of_lexica): \n",
    "        for stem in stems:\n",
    "            if stem in lexicon:\n",
    "                emotion_words[index] = emotion_words[index] + 1\n",
    "    return emotion_words\n",
    "\n",
    "def get_cons_punct_count(pos):\n",
    "    cons_punct_count = 0\n",
    "    for index, item in enumerate(pos[:-1]):\n",
    "        if item == 97 and item == pos[index+1]:\n",
    "            cons_punct_count += 1\n",
    "    return cons_punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_data ['clean_happiness', 'clean_sadness', 'clean_anger', 'clean_fear']\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lexica = [\"clean_happiness\", \"clean_sadness\", \"clean_anger\", \"clean_fear\"]\n",
    "list_of_lexica = combine_datasets(lexica, True)\n",
    "\n",
    "raw_datasets = {\n",
    "    \"tweet\": [\"emoint\", \"crowdflower\", \"tec\"],\n",
    "    \"emotion\": [\"emotion_classification_1\", \"emotion_classification_2\", \"emotion_classification_3\", \"emotion_classification_4\", \"emotion_classification_5\",\"emotion_classification_6\",\"emotion_classification_7\",\"emotion_classification_8\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize_label_length\n",
      "make_data ['emotion_classification_1', 'emotion_classification_2', 'emotion_classification_3', 'emotion_classification_4', 'emotion_classification_5', 'emotion_classification_6', 'emotion_classification_7', 'emotion_classification_8']\n",
      "analyzing sentence features: emotion\n",
      "... searching row 166000/189400\n",
      "... searching row 167000/189400\n",
      "... searching row 168000/189400\n",
      "... searching row 169000/189400\n",
      "... searching row 170000/189400\n",
      "... searching row 171000/189400\n",
      "... searching row 172000/189400\n",
      "... searching row 173000/189400\n",
      "... searching row 174000/189400\n",
      "... searching row 175000/189400\n",
      "... searching row 176000/189400\n",
      "... searching row 177000/189400\n",
      "... searching row 178000/189400\n",
      "... searching row 179000/189400\n",
      "... searching row 180000/189400\n",
      "... searching row 181000/189400\n",
      "... searching row 182000/189400\n",
      "... searching row 183000/189400\n",
      "... searching row 184000/189400\n",
      "... searching row 185000/189400\n",
      "... searching row 186000/189400\n",
      "... searching row 187000/189400\n",
      "... searching row 188000/189400\n",
      "... searching row 189000/189400\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"emotion\"]\n",
    "for dataset_name in dataset_names:\n",
    "    normalize_label_length(raw_datasets[dataset_name], dataset_name)\n",
    "    extract_features(dataset_name, list_of_lexica, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
