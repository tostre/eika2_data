{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(summary):\n",
    "    print(summary)\n",
    "    \n",
    "def make_data(dataset, lexica=False):\n",
    "    if not lexica: \n",
    "        datasets = []\n",
    "        for file in dataset:\n",
    "            datasets.append(pd.read_csv(\"../raw/\" + file + \".csv\")) \n",
    "        dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        lexica = []\n",
    "        for file in dataset: \n",
    "            lexicon = pd.read_csv(\"../../lexica/\" + file + \".csv\")\n",
    "            lexica.append(lexicon[\"stems\"].tolist())\n",
    "        return lexica\n",
    "    \n",
    "def extract_features(dataset, list_of_lexica, save_name, print_row): \n",
    "    cleaned_dataset = []\n",
    "    pos, stems = [], []\n",
    "    lexica_words = np.zeros((len(dataset.index), 4))\n",
    "    # pos 97 = Satzzeichen, 103 = Leerzeichen\n",
    "    print(\"analyzing sentence features:\", save_name)\n",
    "    for index, row in dataset.iterrows():\n",
    "        if index % print_row == 0: log(\"... searching row \" + str(index) + \"/\" + str(len(dataset)))\n",
    "        doc = nlp(split_punct(row[\"text\"]))\n",
    "        doc = nlp(\" \".join([token.text for token in doc if not token.is_stop and token.pos != 103]))\n",
    "        if len(doc) != 0:\n",
    "            pos.append([token.pos for token in doc])\n",
    "            stems.append([stemmer.stem(token.text) for token in doc if token.pos != 97])\n",
    "            emotion_words = get_emotion_words(stems[-1:][0], 0, list_of_lexica)\n",
    "            cleaned_dataset.append([\n",
    "                \" \".join([token.text for token in doc]), row[\"affect\"],\n",
    "                len(doc), (sum([token.text.isupper() for token in doc])/len(doc)), \n",
    "                (len(doc.ents)/len(doc)),get_cons_punct_count(pos[-1:][0]), \n",
    "                emotion_words[0], emotion_words[1], emotion_words[2], emotion_words[3]])\n",
    "        \n",
    "    seq_len = max([row[2] for row in cleaned_dataset])\n",
    "    pos = extend_list(pos, seq_len, 999)\n",
    "    stems = extend_list(stems, seq_len, \"__\")\n",
    "    \n",
    "    df = pd.DataFrame(data=cleaned_dataset, columns=[\"t\", \"a\", \"wc\", \"uwc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"fc\", \"ac\"])\n",
    "    df.to_csv(\"../cleaned/\" + save_name + \"_clean.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    df = pd.DataFrame(data=pos)\n",
    "    df.to_csv(\"../cleaned/\" + save_name + \"_pos.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    df = pd.DataFrame(data=stems)\n",
    "    df.to_csv(\"../cleaned/\" + save_name + \"_stems.csv\", sep=\",\", index=False, float_format='%.3f')\n",
    "    \n",
    "def extend_list(l, seq_len, extension):\n",
    "    for index, row in enumerate(l):\n",
    "        row.extend([extension] * (seq_len - len(row)))\n",
    "    return l\n",
    "    \n",
    "def split_punct(text):\n",
    "    replacement = [(\".\", \" . \"), (\",\", \" , \"), (\"!\", \" ! \"), (\"?\", \" ? \")]\n",
    "    for k, v in replacement: \n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "    \n",
    "def get_emotion_words(stems, emotion_index, list_of_lexica):\n",
    "    emotion_words = np.zeros(4)\n",
    "    for index, lexicon in enumerate(list_of_lexica): \n",
    "        for stem in stems:\n",
    "            if stem in lexicon:\n",
    "                emotion_words[index] = emotion_words[index] + 1\n",
    "    return emotion_words\n",
    "\n",
    "def get_cons_punct_count(pos):\n",
    "    cons_punct_count = 0\n",
    "    for index, item in enumerate(pos[:-1]):\n",
    "        if item == 97 and item == pos[index+1]:\n",
    "            cons_punct_count += 1\n",
    "    return cons_punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_datasets = [\"emoint\", \"crowdflower\", \"tec\"]\n",
    "emotion_datasets = [\"emotion_classification_1\", \"emotion_classification_2\", \"emotion_classification_3\", \"emotion_classification_4\", \"emotion_classification_5\",\"emotion_classification_6\",\"emotion_classification_7\",\"emotion_classification_8\"]\n",
    "lexica = [\"clean_happiness\", \"clean_sadness\", \"clean_anger\", \"clean_fear\"]\n",
    "list_of_lexica = make_data(lexica, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing sentence features: test\n",
      "... searching row 0/33\n"
     ]
    }
   ],
   "source": [
    "#tweet_dataset = make_data(tweet_datasets)\n",
    "#emotion_dataset = make_data(emotion_datasets)\n",
    "#extract_features(tweet_dataset, list_of_lexica, \"tweet\", 500)\n",
    "#extract_features(emotion_dataset, list_of_lexica, \"emotion\", 1000)\n",
    "\n",
    "extract_features(make_data([\"test\"]), list_of_lexica, \"test\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
