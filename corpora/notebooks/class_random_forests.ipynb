{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=ok2s1vV9XW0\n",
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lex_data(dataset_name, feature_set):\n",
    "    print(\"loading lex data for\", dataset_name, feature_set)\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    \n",
    "    targets = dataset[\"a\"]\n",
    "    inputs = dataset[feature_set]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y    \n",
    "\n",
    "def load_vector_data(dataset_name, bgr=False):\n",
    "    print(\"loading vector data for\", dataset_name, bgr)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).fillna(\"\").values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", delimiter=\",\", dtype = types).astype(str)[\"a\"].tolist() \n",
    "    print(\"...attemtping to load: \", \"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\", binary=True)\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    sentences\n",
    "    \n",
    "    if bgr:\n",
    "        tokenized = [t.split() for t in inputs]\n",
    "        phrases = Phrases(tokenized)\n",
    "        bigram = Phraser(phrases)\n",
    "        bigrammed = []\n",
    "        # make bigrams for inputs\n",
    "        for sentence in inputs:\n",
    "            sentence = [t.split() for t in [sentence]]\n",
    "            bigrammed.append(bigram[sentence[0]])\n",
    "        \n",
    "        inputs = []\n",
    "        for sent in bigrammed:\n",
    "            # if sentence is empty\n",
    "            inputs.append(np.sum(vector_model.wv[sent], 0).tolist()) if sent else inputs.append(np.zeros(32))\n",
    "            #if not sent: \n",
    "            #    inputs.append(np.zeros(32))\n",
    "            #else: \n",
    "            #    a = np.sum(vector_model.wv[sent], 0).tolist()\n",
    "            #    inputs.append(a)\n",
    "    else: \n",
    "        inputs = [vector_model.wv[sample] for sample in inputs]\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y  \n",
    "\n",
    "def load_topic_data(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"].tolist()\n",
    "    # remove placeholders from the stems dataset\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    dic = gs.corpora.Dictionary(dataset)\n",
    "    corpus = [dic.doc2bow(sample) for sample in dataset]\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100, chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "    \n",
    "    vecs = []\n",
    "    # for every sentence in the dataset\n",
    "    for i, sample in enumerate(dataset):\n",
    "        # get the vector-representations from the doc\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        # get the topics from the document (they are ordered by the topic ic)\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        # write the probability for every topic into a single list\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        # append the prob-vector for this sentence into the all-vectors-list\n",
    "        vecs.append(topic_vec)\n",
    "    dataset = vecs\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def classify_with_rf(dataset_name, feature_set_name, train_x, test_x, train_y, test_y, num_trees): \n",
    "    print(\"building rf model\")\n",
    "    rf = RandomForestClassifier(n_estimators=num_trees)\n",
    "    print(\"... training model\")\n",
    "    rf.fit(train_x, train_y)\n",
    "    print(\"... calcularing score\")\n",
    "    pred_y = rf.predict(test_x)\n",
    "    # model metadata\n",
    "    score, f1_scoore = rf.score(train_x, train_y), f1_score(test_y, pred_y, average=\"weighted\")\n",
    "    dump(rf, \"../models/random_forests/\" + dataset_name + \"_\" + feature_set_name + \"_random_forests.joblib\") \n",
    "    return (test_y, pred_y, score, f1_scoore, num_trees), rf.feature_importances_ \n",
    "\n",
    "def get_best_tree_num(dataset_name, feature_set, feature_set_name, max_trees, f=\"features\"):\n",
    "    indexes, f1 = [], []\n",
    "    if f == \"vec\":\n",
    "        data = load_vector_data(dataset_name)\n",
    "    elif f == \"topic\":\n",
    "        data = load_topic_data(dataset_name, num_topics_dict.get(dataset_name))\n",
    "    else:        \n",
    "        data = load_lex_data(dataset_name, features[\"lex\"])\n",
    "    \n",
    "    for i in range(1,max_trees):\n",
    "        print(i)\n",
    "        results, importance = classify_with_rf(*data, i)\n",
    "        f1.append(results[3])\n",
    "        indexes.append(i)\n",
    "\n",
    "    draw_plot(dataset_name, feature_set_name, indexes, f1, max(f1), f1.index(max(f1))+1)\n",
    "    \n",
    "def draw_confusion_matrix(dataset_name, feature_set_name, test_y, pred_y, score, f1_scoore, num_trees, num_topics=None): \n",
    "    fig = plt.figure()\n",
    "    hm = sn.heatmap(confusion_matrix(test_y, pred_y), fmt=\"d\", linewidth=0.5, annot=True, square=True, xticklabels=[\"h\", \"s\", \"a\", \"f\"], yticklabels=[\"h\", \"s\", \"a\", \"f\"], cmap=\"PuRd\")\n",
    "    ax1 = fig.add_axes(hm)\n",
    "    ax1.set(xlabel=\"predicted\", ylabel=\"target\")\n",
    "    if feature_set_name == \"topics\": desc = \"dataset: {} ({}), trained with {} tress, {} topics\\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, num_trees, num_topics, round(score,2), round(f1_scoore,2))\n",
    "    else: desc = \"dataset: {} ({}), trained with {} trees \\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, num_trees, round(score,2), round(f1_scoore,2)) \n",
    "    fig.text(0.5, -0.1, desc, ha='center')\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/cm_rf_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def draw_plot(dataset_name, feature_set_name, x, y, best_f1, best_index):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.set(xlabel=\"num_trees\", ylabel=\"f1_score\")\n",
    "    desc = \"dataset: {} ({})\\nbest f1_score: {}, with num_trees: {}\".format(dataset_name, feature_set_name, best_f1, best_index)\n",
    "    fig.text(0.5, -0.07, desc, ha='center')\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/num_trees_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")  \n",
    "\n",
    "# achtung: Die methode plottet alle coefficients. Immer also nur ein dataset durchlaufen lassen\n",
    "def draw_coefficients_plot(dataset_name, results, coefficients):\n",
    "    print(coefficients)\n",
    "    num_features = len(coefficients[0])\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.plot(range(len(coefficients[0])), coefficients[0], \"o\", label=results[0][0] + \"_\" + results[0][1])#, label=classes[i]\n",
    "    plt.plot(range(len(coefficients[1])), coefficients[1], \"o\", label=results[1][0] + \"_\" + results[1][1])#, label=classes[i]\n",
    "    plt.plot(range(4, 4 + len(coefficients[2])), coefficients[2], \"o\", label=results[2][0] + \"_\" + results[2][1])#, label=classes[i]\n",
    "    desc = \"dataset: {}\".format(dataset_name)\n",
    "    fig.text(0.5, -0.05, desc, ha='center')\n",
    "    plt.xticks(range(0, num_features), features.get(\"full\"), rotation=90)\n",
    "    #plt.legend()#loc=1\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/importances_rf_\" + dataset_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def load_vector_data(dataset_name, bgr=False):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", delimiter=\",\", dtype = types).astype(str)[\"a\"].tolist() \n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    tokenized = sentences\n",
    "    if bgr:\n",
    "        bigram = Phraser.load(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")\n",
    "        bigrammed = [bigram[sentence] for sentence in sentences]\n",
    "        tokenized = bigrammed\n",
    "    inputs = [np.sum(vector_model.wv[sent], 0).tolist() if sent else np.zeros(32) for sent in tokenized]   \n",
    "    inputs = np.array(inputs)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y  \n",
    "\n",
    "def draw_importances(dataset_name, features, x, importances):\n",
    "    print(\"draw i\", features)\n",
    "    # sortiere beide eintrÃ¤ge\n",
    "    x = [e for _,e in sorted(zip(importances,x))]\n",
    "    importances.sort()\n",
    "    fig, ax = plt.subplots()\n",
    "    desc = \"dataset: {}({})\".format(dataset_name, features)\n",
    "    fig.text(0.5, -0.05, desc, ha='center')\n",
    "    plt.scatter(importances, x)\n",
    "    plt.xlabel(\"feature importances\")\n",
    "    plt.ylabel(\"features\")\n",
    "    plt.grid()\n",
    "    for i, v in enumerate(importances):    \n",
    "        ax.annotate(str(round(v,2)), (importances[i], x[i]), (importances[i], x[i]))\n",
    "    fig.savefig(\"../img/importances_rf_\" + dataset_name + \"_\" + features + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "importances = []\n",
    "\n",
    "dataset_name = \"norm_tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "datasets = [\"emotion\", \"norm_emotion\", \"tweet\", \"norm_tweet\"]\n",
    "features = {\n",
    "    \"full\": [\"wc\", \"uwc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"nolex\": [\"wc\", \"uwc\", \"ewc\", \"cpc\"],\n",
    "    \"lex\": [\"hc\", \"sc\", \"ac\", \"fc\"]\n",
    "}\n",
    "classes = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "trees_for_dataset = {\n",
    "    \"norm_emotion_full\": 18,\n",
    "    \"norm_emotion_nolex\": 33,\n",
    "    \"norm_emotion_lex\": 28, \n",
    "    \"norm_emotion_vec-unigram\": 198,\n",
    "    \"norm_emotion_vec-bigram\": 126,\n",
    "    \"norm_emotion_topic\": 100,\n",
    "    \"norm_tweet_full\": 21,\n",
    "    \"norm_tweet_nolex\": 12,\n",
    "    \"norm_tweet_lex\": 10,\n",
    "    \"norm_tweet_vec-unigram\": 150,\n",
    "    \"norm_tweet_vec-bigram\": 131,\n",
    "    \"norm_tweet_topic\": 87\n",
    "}\n",
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 79,\n",
    "    \"norm_emotion\": 186\n",
    "}\n",
    "types = {\n",
    "    \"text\": object, \n",
    "    \"a\": int, \n",
    "    \"wc\": float,\n",
    "    \"uwc\": float,\n",
    "    \"ewc\": float,\n",
    "    \"cpc\": float,\n",
    "    \"hc\": float,\n",
    "    \"sc\": float,\n",
    "    \"ac\": float,\n",
    "    \"c\": float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lex data for norm_tweet ['wc', 'uwc', 'ewc', 'cpc', 'hc', 'sc', 'ac', 'fc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet ['wc', 'uwc', 'ewc', 'cpc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet ['hc', 'sc', 'ac', 'fc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_tweet\n",
      "... replacing placeholders\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_tweet\n",
      "... replacing placeholders\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading topic data for norm_tweet\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet ['wc', 'uwc', 'ewc', 'cpc', 'hc', 'sc', 'ac', 'fc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet ['wc', 'uwc', 'ewc', 'cpc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet ['hc', 'sc', 'ac', 'fc']\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_emotion\n",
      "... replacing placeholders\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_emotion\n",
      "... replacing placeholders\n",
      "building rf model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading topic data for norm_emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/home/marcel/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------\u001b[0m",
      "\u001b[0;31mFull\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                         \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                         \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFull\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-d1f105fc3a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# topic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_with_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mload_topic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrees_for_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mimportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-888e69391f0b>\u001b[0m in \u001b[0;36mload_topic_data\u001b[0;34m(dataset_name, num_topics)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamulticore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#update_every=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0;31m# in case the input job queue is full, keep clearing the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                         \u001b[0;31m# result queue, to make sure we don't deadlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                         \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmerged_new\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# the topics change through this update, to assess convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mprevious_Elogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mcurrent_Elogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mblend\u001b[0;34m(self, rhot, other, targetsize)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtargetsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrhot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# stretch the incoming n*phi counts to target size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "importances = []\n",
    "\n",
    "for dataset in [\"norm_tweet\", \"norm_emotion\"]:\n",
    "    # lex data\n",
    "    for key, feature_set in features.items(): \n",
    "        results, importance = classify_with_rf(dataset_name, key, *load_lex_data(dataset_name, feature_set), trees_for_dataset.get(dataset_name + \"_\" + key, 10))\n",
    "        all_results.append([dataset_name, key, *results])\n",
    "        importances.append(importance)\n",
    "    # unigram dataset\n",
    "    results, coef = classify_with_rf(dataset, \"vec-unigram\", *load_vector_data(dataset), trees_for_dataset.get(dataset_name + \"_\" + key, 10))\n",
    "    all_results.append([dataset, \"vec-unigram\", *results])\n",
    "    importances.append(coef)\n",
    "    # bigram dataset\n",
    "    results, coef = classify_with_rf(dataset,  \"vec-bigram\", *load_vector_data(dataset, True), trees_for_dataset.get(dataset_name + \"_\" + key, 10))\n",
    "    all_results.append([dataset, \"vec-bigram\", *results])\n",
    "    importances.append(coef)\n",
    "    # topic dataset\n",
    "    results, coef = classify_with_rf(dataset, \"topics\", *load_topic_data(dataset, num_topics_dict[dataset]), trees_for_dataset.get(dataset_name + \"_\" + key, 10))\n",
    "    all_results.append([dataset, \"topics\", *results])\n",
    "    importances.append(coef)\n",
    "    \n",
    "for index, result in enumerate(all_results):\n",
    "    with open(\"../reports/report_rf_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "        print((result[0] + \"_\" + result[1] + \" (\" + str(result[5]) + \"):\\n\" + \n",
    "          classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "    draw_confusion_matrix(*result)\n",
    "    if result[1] == \"full\" or result[1] == \"lex\" or result[1] == \"nolex\":\n",
    "        draw_importances(result[0], result[1], features[result[1]], importances[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_best_tree_num(\"norm_emotion\", 1, \"topic\", 200, \"topic\")\n",
    "#dataset_name, feature_set, feature_set_name, max_trees, f=\"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lrandom forests over the topic distributions\n",
    "#all_results = []\n",
    "#importances = []\n",
    "#for dataset_name in [\"norm_emotion\"]: \n",
    "#    train_x, test_x, train_y, test_y = load_topic_data(dataset_name, num_topics_dict.get(dataset_name))\n",
    "#    results, importance = classify_with_rf(train_x, test_x, train_y, test_y, trees_for_dataset.get(dataset_name + \"_topic\", 10))\n",
    "#    all_results.append([dataset_name, \"topics\", *results])\n",
    "#    importances.append(importance)\n",
    "\n",
    "#for index, result in enumerate(all_results): \n",
    "#    with open(\"../img/report_rf_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "#        print((result[0] + \"_\" + result[1] + \" (\" + str(result[5]) + \"):\\n\" + \n",
    "#          classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "#    draw_confusion_matrix(*result, num_topics_dict.get(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
