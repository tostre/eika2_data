{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    # remove placeholders from the stems dataset\n",
    "    print(\"removing placeholders\")\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    print(\"making dic\")\n",
    "    dic = gs.corpora.Dictionary(dataset)\n",
    "    dic.save(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    print(\"making corpus\")\n",
    "    corpus = [dic.doc2bow(sample) for sample in dataset]\n",
    "    print(\"making lda\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100, chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "    lda_model.save(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    print(\"making fasttext\")\n",
    "    inputs = [\" \".join(sentence) for sentence in dataset]\n",
    "    vector_model = FastText(size=32, window=3, min_count=1)\n",
    "    vector_model.build_vocab(inputs)  \n",
    "    vector_model.train(sentences=inputs, total_examples=len(inputs), total_words=vector_model.corpus_total_words, epochs=10)\n",
    "    vector_model.save(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # make bigram model\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"t\"].tolist()\n",
    "    tokenized = [t.split() for t in sentences]\n",
    "    phrases = Phrases(tokenized)\n",
    "    bigram = Phraser(phrases)\n",
    "    bigram.save(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(dataset_name, num_topics):\n",
    "    print(\"loading dic\")\n",
    "    #dic = gs.corpora.Dictionary.load(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    print(\"loading topic model\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    #topics = lda_model.show_topics(num_topics = num_topics)\n",
    "    #print(len(topics))\n",
    "    #print(topics)\n",
    "    #vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    return lda_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dic\n",
      "loading topic model\n",
      "[['probabl', 'birthday', 'tiangong', 'prohibit', '2017', 'decommiss', 'laboratori', 'satellit', 'profan', 'unfortun'], ['mean', 'bed', 'readi', 'experi', 'fail', 'funni', 'clear', 'give', 'tree', 'consid'], ['year', \"'\", 'see', 'fantast', 'young', 'servic', 'everybodi', 'cuddl', 'usa', 'genius'], ['long', 'twitter', 'damn', 'bout', 'ya', 'parti', 'sent', 'app', 'doin', 'grad'], ['have', 'hous', 'water', 'nice', 'citi', 'pay', 'stomach', 'hors', 'bf', 'carri'], ['hour', 'wo', 'black', 'friday', 'dentist', 'abl', 'burst', 'limit', 'energi', 'mental'], ['biggest', 'step', 'okay', 'woke', 'practic', 'natur', 'shower', 'boyfriend', 'exact', '2012'], ['god', 'talk', 'phone', 'pop', 'drop', 'teacher', 'winner', 'batteri', 'tyler', 'tickl'], ['fun', 'soon', 'decid', 'keep', 'yay', '1st', 'date', 'worth', 'floor', 'hilari'], ['night', 'find', '/', 'idea', 'aw', 'wear', 'came', 'size', 'switch', 'brain']]\n",
      "0 u 1 set()\n",
      "1 u 2 set()\n",
      "2 u 3 set()\n",
      "3 u 4 set()\n",
      "4 u 5 set()\n",
      "5 u 6 set()\n",
      "6 u 7 set()\n",
      "7 u 8 set()\n",
      "8 u 9 set()\n"
     ]
    }
   ],
   "source": [
    "lda_model = load_test(\"norm_tweet\", 10)\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "t = []\n",
    "for topic in topics: \n",
    "    t.append([word[0] for word in topic[1]])\n",
    "print(t)\n",
    "\n",
    "for i, e in enumerate(t[:-1]):\n",
    "    a = t[i]\n",
    "    b = t[i+1]\n",
    "    print(i, \"u\", i+1, set(a) & set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dic\n",
      "loading topic model\n",
      "[['m', 'feel', 'read', 'let', 'ill', 'book', 'hour', 'stay', 'readi', 'ladi'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['know', 'feel', 'past', 'hand', 'came', 'deep', 'rough', 'ghost', 'brick', 'kurang'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus'], ['lagi', 'greenlight', 'causal', 'rez', 'kurang', 'laa', 'chantel', 'tahan', 'sanskrit', 'disqus']]\n",
      "0 u 1 set()\n",
      "1 u 2 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n",
      "2 u 3 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n",
      "3 u 4 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n",
      "4 u 5 {'kurang'}\n",
      "5 u 6 {'kurang'}\n",
      "6 u 7 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n",
      "7 u 8 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n",
      "8 u 9 {'causal', 'tahan', 'lagi', 'laa', 'chantel', 'rez', 'kurang', 'greenlight', 'sanskrit', 'disqus'}\n"
     ]
    }
   ],
   "source": [
    "lda_model = load_test(\"norm_emotion\", 10)\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "t = []\n",
    "for topic in topics: \n",
    "    t.append([word[0] for word in topic[1]])\n",
    "print(t)\n",
    "\n",
    "for i, e in enumerate(t[:-1]):\n",
    "    a = t[i]\n",
    "    b = t[i+1]\n",
    "    print(i, \"u\", i+1, set(a) & set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 79,\n",
    "    \"norm_emotion\": 186\n",
    "}\n",
    "\n",
    "datasets = [\"norm_emotion\"]\n",
    "\n",
    "#for dataset in datasets: \n",
    "#    save_models(dataset, num_topics_dict[dataset])\n",
    "#dataset_name = \"test\"\n",
    "#save_models(dataset_name, num_topics_dict[dataset_name])\n",
    "\n",
    "#load_test(dataset_name, num_topics_dict[dataset_name] + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
