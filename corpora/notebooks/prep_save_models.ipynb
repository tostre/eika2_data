{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    # remove placeholders from the stems dataset\n",
    "    print(\"removing placeholders\")\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    print(\"making dic\")\n",
    "    dic = gs.corpora.Dictionary(dataset)\n",
    "    #dic.save(\"../models/test/\" + dataset_name + \"_dictionary\")\n",
    "    print(\"making corpus\")\n",
    "    corpus = [dic.doc2bow(sample) for sample in dataset]\n",
    "    print(\"making lda\")\n",
    "    #lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100, chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "    #lda_model.save(\"../models/test/\" + dataset_name + \"_ldamodel\")\n",
    "    print(\"making fasttext\")\n",
    "    inputs = [\" \".join(sentence) for sentence in dataset]\n",
    "    vector_model = FastText(size=32, window=3, min_count=1)\n",
    "    vector_model.build_vocab(inputs)  \n",
    "    vector_model.train(sentences=inputs, total_examples=len(inputs), total_words=vector_model.corpus_total_words, epochs=10)\n",
    "    vector_model.save(\"../models/test/\" + dataset_name + \"_fasttext\")\n",
    "    \n",
    "def load_test(dataset_name):\n",
    "    print(\"loading dic\")\n",
    "    dic = gs.corpora.Dictionary.load(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    print(\"loading topic model\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    print(\"loading fasttext\")\n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dic\n",
      "loading topic model\n",
      "loading fasttext\n"
     ]
    }
   ],
   "source": [
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 79,\n",
    "    \"norm_emotion\": 186,\n",
    "}\n",
    "\n",
    "dataset_name = \"norm_tweet\"\n",
    "#save_models(dataset_name, num_topics_dict[dataset_name])\n",
    "\n",
    "load_test(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
