{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc\n",
    "import torch.utils.data as D\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim, embedding_dim, hidden_dim, n_layers, batch_size, drop_prob=0.5):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.act_function = nn.ReLU()\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # initiate layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # embedding dim = input dim im linearen n\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers)#, batch_first=True\n",
    "        self.lin1 = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self, batch_size, cuda):\n",
    "        if cuda: return (torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda(), torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
    "        else: return (torch.randn(self.n_layers, batch_size, self.hidden_dim), torch.randn(self.n_layers, batch_size, self.hidden_dim))\n",
    "        #return (torch.randn(self.n_layers, self.embedding_dim, self.hidden_dim), torch.randn(self.n_layers, self.embedding_dim, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.long() \n",
    "        x = self.embedding(x) \n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.act_function(self.lin1(x[:,-1,:]))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        return x, hidden\n",
    "    \n",
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_data(dataset_name, bgr=False, split_factor=0.2):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"]\n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    tokenized = sentences\n",
    "    if bgr:\n",
    "        bigram = Phraser.load(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")\n",
    "        bigrammed = [bigram[sentence] for sentence in sentences]\n",
    "        tokenized = bigrammed\n",
    "    inputs = [np.sum(vector_model.wv[sent], 0).tolist() if sent else np.zeros(32) for sent in tokenized]   \n",
    "    inputs = np.array(inputs)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    return len(inputs[0]), train_loader, val_loader, test_loader\n",
    "\n",
    "def load_topic_data(dataset_name, split_factor=0.2):\n",
    "    print(\"loading lex data\", dataset_name, feature_set_name)\n",
    "    inputs = []\n",
    "    num_topics = num_topics_dict[dataset_name]\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    targets = dataset[\"a\"]\n",
    "    dataset = dataset.astype(str).values.tolist() \n",
    "    dic = gs.corpora.Dictionary.load(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")   \n",
    "    print(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    for index, sample in enumerate(dataset): \n",
    "        dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    for i, sample in enumerate(dataset):\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        inputs.append(topic_vec)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    topics_num = len(lda_model.get_topics())\n",
    "    return topics_num, train_loader, val_loader, test_loader\n",
    "\n",
    "def make_loader(inputs, targets, test_size):\n",
    "    # make train and test sets\n",
    "    train_x, val_x, train_y, val_y = train_test_split(inputs, targets, test_size=test_size)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=test_size)\n",
    "    train_data = MyDataset(np.asarray(train_x), np.asarray(train_y))\n",
    "    val_data = MyDataset(np.asarray(val_x), np.asarray(val_y))\n",
    "    test_data = MyDataset(np.asarray(test_x), np.asarray(test_y))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=round(batch_size*test_size))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda(cuda, inputs, targets, net):\n",
    "    if cuda: return inputs.cuda(), targets.cuda(), net.cuda()\n",
    "    else: return inputs, targets, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_laoder, net, epochs, criterion, print_every, save_name, cuda, lr, batch_size, clip):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    error_curve = []\n",
    "    net.train()\n",
    "    hidden = net.init_hidden(batch_size, cuda)\n",
    "    for epoch in range(epochs): \n",
    "        for index, (inputs, targets) in enumerate(train_loader): \n",
    "            hidden = tuple([item.data for item in hidden])\n",
    "            net.zero_grad()\n",
    "            output, hidden = net(inputs, hidden)\n",
    "            if(cuda):\n",
    "                ouput = output.to(\"cuda\")\n",
    "                targets = targets.to(\"cuda\")\n",
    "            loss = criterion(output.float(), targets)\n",
    "            loss.backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "def run(dataset_name, feature_set_name):\n",
    "    file_name = \"net_lin_{}({})\".format(dataset_name, feature_set_name)\n",
    "    print(\"running \", file_name)\n",
    "    open(\"{}{}_{}\".format(\"../logs/\", file_name, \".txt\"), \"w\").close()\n",
    "    if feature_set_name == \"topics\":\n",
    "        num_topics, train_loader, val_loader, test_loader = load_topic_data(dataset_name)\n",
    "        net = Lstm(num_topics, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-unigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name)\n",
    "        net = Lstm(input_dim, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-bigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name, True)\n",
    "        net = Lstm(input_dim, output_dim, hidden_dim, num_hidden_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every)\n",
    "        test(test_loader, net, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"norm_tweet\", \"norm_emotion\"]\n",
    "feature_set_names = [\"vec-unigram\", \"vec-bigram\", \"topic\"]\n",
    "dataset_names = [\"norm_tweet\"]\n",
    "feature_set_names = [\"vec-unigram\"]\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  net_lin_norm_tweet(vec-unigram)\n",
      "loading vector data for norm_tweet\n",
      "... replacing placeholders\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Lin_Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-544600da17a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature_set_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_set_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_set_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c9d12c2f9486>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_name, feature_set_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfeature_set_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vec-unigram\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vector_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLin_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lin_Net' is not defined"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names: \n",
    "    for feature_set_name in feature_set_names: \n",
    "        run(dataset_name, feature_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
