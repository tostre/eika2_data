{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc\n",
    "import torch.utils.data as D\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.act_function = nn.ReLU()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # initiate layers\n",
    "        # embedding dim = input dim im linearen ntz (79 f√ºr topic tweet zB )\n",
    "        # input dim = dimension of vector at each time step\n",
    "        #  (1, 1, 5) which represents (batch size, sequence length, input dimension). bei mir also: (16, 81, 79/32)\n",
    "        lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.n_layers)#, batch_first=True\n",
    "        self.lin1 = nn.Linear(self.hidden_dim, self.hidden_dim) \n",
    "        self.lin2 = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self, batch_size, cuda):\n",
    "        #if cuda: return (torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda(), torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
    "        #else: return (torch.randn(self.n_layers, batch_size, self.hidden_dim), torch.randn(self.n_layers, batch_size, self.hidden_dim))\n",
    "        # The hidden state and cell state is stored in a tuple with the format (hidden_state, cell_state).\n",
    "        # hidden_state = torch.randn(n_layers, batch_size, hidden_dim) ???\n",
    "        if cuda: return (torch.randn(self.n_layers, self.input_dim, self.hidden_dim).cuda(), torch.randn(self.n_layers, self.input_dim, self.hidden_dim)).cuda()\n",
    "        else: return (torch.randn(self.n_layers, self.input_dim, self.hidden_dim), torch.randn(self.n_layers, self.input_dim, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # seq len ist im falle der topics 79/186\n",
    "        # im falle der vec 32\n",
    "        x = x.long() \n",
    "        #x = self.embedding(x) \n",
    "        # first axis: sequence itself, second: instances in mini-batch, third: elements of the input.\n",
    "        print(x.shape)\n",
    "        #print(hidden.shape)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.act_function(self.lin1(x[:,-1,:]))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        return x, hidden\n",
    "    \n",
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_data(dataset_name, bgr=False, split_factor=0.2):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()[:2]\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"]\n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    tokenized = sentences\n",
    "    print(tokenized)\n",
    "    print(type(tokenized))\n",
    "    print(type(tokenized[0]))\n",
    "    a = np.array(tokenized)\n",
    "    print(a.shape)\n",
    "    if bgr:\n",
    "        bigram = Phraser.load(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")\n",
    "        bigrammed = [bigram[sentence] for sentence in sentences]\n",
    "        tokenized = bigrammed\n",
    "    inputs = a\n",
    "    #print(h.shape)\n",
    "    #print(tokenized)\n",
    "    print(\"tokenizing\")\n",
    "    for i, sample in enumerate(inputs):\n",
    "        #print(sample, \"...wird zu...\")\n",
    "        inputs[i] = [vector_model.wv[token] for token in sample]\n",
    "        #print(type(inputs[i]))\n",
    "        #print(\"...\", inputs[i])\n",
    "    #print(\"number samples:\", len(inputs))\n",
    "    print(inputs.shape)\n",
    "    #print(\"every seq consists of: (sequence length)\", len(inputs[0]), \"vectors\")\n",
    "    #print(\"every vector is\", len(inputs[0][0]), \"elements big\")\n",
    "    a = np.array(inputs)\n",
    "    #print(a[:10])\n",
    "    #print(\"a shape\", a.shape)\n",
    "    #inputs = [np.sum(vector_model.wv[sent], 0).tolist() if sent else np.zeros(32) for sent in tokenized]   \n",
    "    #inputs = np.array(inputs)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    return len(inputs[0]), train_loader, val_loader, test_loader\n",
    "\n",
    "def load_topic_data(dataset_name, split_factor=0.2):\n",
    "    print(\"loading lex data\", dataset_name, feature_set_name)\n",
    "    inputs = []\n",
    "    num_topics = num_topics_dict[dataset_name]\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    targets = dataset[\"a\"]\n",
    "    dataset = dataset.astype(str).values.tolist() \n",
    "    dic = gs.corpora.Dictionary.load(\"../models/dictionary/\" + dataset_name + \"_dictionary\")\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")   \n",
    "    print(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    for index, sample in enumerate(dataset): \n",
    "        dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    for i, sample in enumerate(dataset):\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        inputs.append(topic_vec)\n",
    "    train_loader, val_loader, test_loader = make_loader(inputs, targets, split_factor)\n",
    "    topics_num = len(lda_model.get_topics())\n",
    "    return topics_num, train_loader, val_loader, test_loader\n",
    "\n",
    "def make_loader(inputs, targets, test_size):\n",
    "    # make train and test sets\n",
    "    train_x, val_x, train_y, val_y = train_test_split(inputs, targets, test_size=test_size)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=test_size)\n",
    "    train_data = MyDataset(np.asarray(train_x), np.asarray(train_y))\n",
    "    val_data = MyDataset(np.asarray(val_x), np.asarray(val_y))\n",
    "    test_data = MyDataset(np.asarray(test_x), np.asarray(test_y))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=round(batch_size*test_size))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda(cuda, inputs, targets, net):\n",
    "    if cuda: return inputs.cuda(), targets.cuda(), net.cuda()\n",
    "    else: return inputs, targets, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every, batch_size, clip):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    error_curve = []\n",
    "    net.train()\n",
    "    hidden = net.init_hidden(batch_size, cuda)\n",
    "    for epoch in range(epochs): \n",
    "        for index, (inputs, targets) in enumerate(train_loader): \n",
    "            hidden = tuple([item.data for item in hidden])\n",
    "            net.zero_grad()\n",
    "            output, hidden = net(inputs, hidden)\n",
    "            if(cuda):\n",
    "                ouput = output.to(\"cuda\")\n",
    "                targets = targets.to(\"cuda\")\n",
    "            loss = criterion(output.float(), targets)\n",
    "            loss.backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            gc.collect()\n",
    "\n",
    "def run(dataset_name, feature_set_name):\n",
    "    file_name = \"net_lin_{}({})\".format(dataset_name, feature_set_name)\n",
    "    print(\"running \", file_name)\n",
    "    open(\"{}{}_{}\".format(\"../logs/\", file_name, \".txt\"), \"w\").close()\n",
    "    if feature_set_name == \"topics\":\n",
    "        num_topics, train_loader, val_loader, test_loader = load_topic_data(dataset_name)\n",
    "        net = Lstm(num_topics, output_dim, hidden_dim, n_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every, batch_size, clip)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-unigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name)\n",
    "        net = Lstm(input_dim, output_dim, hidden_dim, n_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every, batch_size, clip)\n",
    "        test(test_loader, net, file_name)\n",
    "    elif feature_set_name == \"vec-bigram\":\n",
    "        input_dim, train_loader, val_loader, test_loader = load_vector_data(dataset_name, True)\n",
    "        net = Lstm(input_dim, output_dim, hidden_dim, n_layers)\n",
    "        train(train_loader, val_loader, net, epochs, cuda, lr, file_name, print_every, batch_size, clip)\n",
    "        test(test_loader, net, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"norm_tweet\", \"norm_emotion\"]\n",
    "feature_set_names = [\"vec-unigram\", \"vec-bigram\", \"topic\"]\n",
    "dataset_names = [\"norm_test\"]\n",
    "feature_set_names = [\"vec-unigram\"]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "cuda = torch.cuda.is_available()\n",
    "batch_size = 256\n",
    "epochs = 10 + 1\n",
    "print_every = 2\n",
    "split_factor = 0.2\n",
    "output_dim = 4\n",
    "hidden_dim = 8\n",
    "n_layers = 2\n",
    "lr = 0.01\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  net_lin_norm_test(vec-unigram)\n",
      "loading vector data for norm_test\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names: \n",
    "    for feature_set_name in feature_set_names: \n",
    "        run(dataset_name, feature_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
