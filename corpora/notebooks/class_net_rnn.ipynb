{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc\n",
    "import torch.utils.data as D\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self, act_function, vocab_size, output_dim, embedding_dim, hidden_dim, n_layers, cuda, batch_size, drop_prob=0.5):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "        self.act_function = act_function\n",
    "        self.cuda = cuda\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # initiate layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers)#, batch_first=True\n",
    "        self.lin1 = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #if self.cuda:\n",
    "        #    return (torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda(), torch.randn(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
    "        #else:\n",
    "        #    return (torch.randn(self.n_layers, batch_size, self.hidden_dim), torch.randn(self.n_layers, batch_size, self.hidden_dim))\n",
    "        return (torch.randn(self.n_layers, self.embedding_dim, self.hidden_dim), torch.randn(self.n_layers, self.embedding_dim, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.long() \n",
    "        x = self.embedding(x) \n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.act_function(self.lin1(x[:,-1,:]))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        return x, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
