{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, act_function):\n",
    "        super(Lin_Net, self).__init__()\n",
    "        self.act_function = act_function\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # act_funtion = F.sigmoid oder F.relu\n",
    "        x = self.act_function(self.lin1(x))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, features, batch_size, debug=False):\n",
    "    datasets = []\n",
    "    for file in dataset:\n",
    "        datasets.append(pd.read_csv(\"../\" + file))\n",
    "    dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "    \n",
    "    target = dataset[\"affect\"]\n",
    "    dataset_full = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"h_count\", \"s_count\", \"a_count\", \"f_count\", \"cons_punct_count\"]]\n",
    "    dataset_nolex = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"cons_punct_count\"]]\n",
    "    dataset_lex = dataset[[\"h_count\", \"s_count\", \"a_count\", \"f_count\"]]\n",
    "    \n",
    "    # make train and test sets\n",
    "    if features == \"full\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.2)\n",
    "    elif features == \"nolex\":\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_nolex, target, test_size=0.2)\n",
    "    elif features == \"lex\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_lex, target, test_size=0.2)\n",
    "\n",
    "    # make data loaders\n",
    "    train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "    test_data = MyDataset(test_x.to_numpy(), test_y.to_numpy())\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    \n",
    "    if debug: \n",
    "        dataset_full = dataset_full.iloc[:10]\n",
    "        target = target[:10]\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.8)\n",
    "        train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "        test_loader = DataLoader(dataset=train_data, batch_size=1)\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(summary, file):\n",
    "    log = open(file, \"a\")\n",
    "    log.write(summary)\n",
    "    log.close()\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epochs, criterion, print_every, save_name, cuda, lr):\n",
    "    open(save_name + \"_train\", \"w\").close()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.5)\n",
    "    error_curve = []\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        for index, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            if cuda: \n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                net = net.cuda()\n",
    "            pred = net(inputs)    \n",
    "            loss = criterion(pred, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((index) % print_every == 0):\n",
    "                log(\"batch: {}/{} in epoch {}/{} \\n... loss: {}\\n\".\n",
    "                    format((index+1), len(train_loader), (epoch+1), epochs, loss.item()), \n",
    "                    save_name + \"_train\")\n",
    "        # save network after every epoch\n",
    "        torch.save(net.state_dict(), save_name + \".pt\")  \n",
    "        # after every epoch save the error\n",
    "        error_curve.append([epoch, loss.item()])\n",
    "    log(\"\\n\" + str(error_curve), save_name + \"_train\")\n",
    "    plt.plot([item[0] for item in error_curve], [item[1] for item in error_curve])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.savefig(save_name+\"_train_error.png\")\n",
    "\n",
    "def test(test_loader, net, criterion, print_every, save_name, cuda):\n",
    "    open(save_name + \"_test\", \"w\").close()\n",
    "    confusion = []\n",
    "    net.eval()\n",
    "    loss_sum, correct, correct2 = 0, 0, 0\n",
    "    for index, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        if cuda: \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            net = net.cuda()\n",
    "        pred = net(inputs)\n",
    "        pred_class = round(pred.item())\n",
    "        loss_sum += criterion(pred, targets).item()\n",
    "        confusion.append([targets.item(), pred_class])\n",
    "        \n",
    "        # correct? \n",
    "        if pred_class == targets.item():\n",
    "            correct += 1\n",
    "        \n",
    "        if ((index) % print_every == 0):\n",
    "            log(\"batch: {}/{}\\n... correct: {}\\n\".\n",
    "                format((index+1), len(test_loader), correct), \n",
    "                save_name + \"_test\")\n",
    "           \n",
    "    # give end report\n",
    "    log(\"average test loss: {}, relative correct: {}\\n\\nconfusion:\\n{}\".\n",
    "        format((loss_sum / len(test_loader)), (correct / len(test_loader)),str(confusion)), \n",
    "        save_name + \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables\n"
     ]
    }
   ],
   "source": [
    "# create variables \n",
    "print(\"creating variables\")\n",
    "emotion_dataset = [\"emotion_classification_1_clean.csv\", \"emotion_classification_2_clean.csv\", \"emotion_classification_3_clean.csv\", \"emotion_classification_4_clean.csv\", \"emotion_classification_5_clean.csv\", \"emotion_classification_6_clean.csv\", \"emotion_classification_7_clean.csv\", \"emotion_classification_8_clean.csv\"]\n",
    "tweet_dataset = [\"crowdflower_clean.csv\", \"emoint_clean.csv\", \"tec_clean.csv\"]\n",
    "act_function = torch.sigmoid\n",
    "criterion = nn.MSELoss()\n",
    "cuda = torch.cuda.is_available()\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 1/1000 \n",
      "... loss: 0.2837662994861603\n",
      "\n",
      "batch: 1/1 in epoch 2/1000 \n",
      "... loss: 0.5006978511810303\n",
      "\n",
      "batch: 1/1 in epoch 3/1000 \n",
      "... loss: 1.2827690839767456\n",
      "\n",
      "batch: 1/1 in epoch 4/1000 \n",
      "... loss: 4.073252201080322\n",
      "\n",
      "batch: 1/1 in epoch 5/1000 \n",
      "... loss: 10.667582511901855\n",
      "\n",
      "batch: 1/1 in epoch 6/1000 \n",
      "... loss: 11.398224830627441\n",
      "\n",
      "batch: 1/1 in epoch 7/1000 \n",
      "... loss: 0.32193297147750854\n",
      "\n",
      "batch: 1/1 in epoch 8/1000 \n",
      "... loss: 0.6663562655448914\n",
      "\n",
      "batch: 1/1 in epoch 9/1000 \n",
      "... loss: 0.6582034826278687\n",
      "\n",
      "batch: 1/1 in epoch 10/1000 \n",
      "... loss: 0.3842248320579529\n",
      "\n",
      "batch: 1/1 in epoch 11/1000 \n",
      "... loss: 0.26299160718917847\n",
      "\n",
      "batch: 1/1 in epoch 12/1000 \n",
      "... loss: 0.25382012128829956\n",
      "\n",
      "batch: 1/1 in epoch 13/1000 \n",
      "... loss: 0.2635621428489685\n",
      "\n",
      "batch: 1/1 in epoch 14/1000 \n",
      "... loss: 0.2625691592693329\n",
      "\n",
      "batch: 1/1 in epoch 15/1000 \n",
      "... loss: 0.2563020884990692\n",
      "\n",
      "batch: 1/1 in epoch 16/1000 \n",
      "... loss: 0.2522393763065338\n",
      "\n",
      "batch: 1/1 in epoch 17/1000 \n",
      "... loss: 0.25127288699150085\n",
      "\n",
      "batch: 1/1 in epoch 18/1000 \n",
      "... loss: 0.2514573335647583\n",
      "\n",
      "batch: 1/1 in epoch 19/1000 \n",
      "... loss: 0.2515496611595154\n",
      "\n",
      "batch: 1/1 in epoch 20/1000 \n",
      "... loss: 0.2514254152774811\n",
      "\n",
      "batch: 1/1 in epoch 21/1000 \n",
      "... loss: 0.251291424036026\n",
      "\n",
      "batch: 1/1 in epoch 22/1000 \n",
      "... loss: 0.251234769821167\n",
      "\n",
      "batch: 1/1 in epoch 23/1000 \n",
      "... loss: 0.25122538208961487\n",
      "\n",
      "batch: 1/1 in epoch 24/1000 \n",
      "... loss: 0.2512226402759552\n",
      "\n",
      "batch: 1/1 in epoch 25/1000 \n",
      "... loss: 0.25121423602104187\n",
      "\n",
      "batch: 1/1 in epoch 26/1000 \n",
      "... loss: 0.25120362639427185\n",
      "\n",
      "batch: 1/1 in epoch 27/1000 \n",
      "... loss: 0.25119441747665405\n",
      "\n",
      "batch: 1/1 in epoch 28/1000 \n",
      "... loss: 0.2511868476867676\n",
      "\n",
      "batch: 1/1 in epoch 29/1000 \n",
      "... loss: 0.25117984414100647\n",
      "\n",
      "batch: 1/1 in epoch 30/1000 \n",
      "... loss: 0.251172810792923\n",
      "\n",
      "batch: 1/1 in epoch 31/1000 \n",
      "... loss: 0.2511657476425171\n",
      "\n",
      "batch: 1/1 in epoch 32/1000 \n",
      "... loss: 0.251158744096756\n",
      "\n",
      "batch: 1/1 in epoch 33/1000 \n",
      "... loss: 0.2511518597602844\n",
      "\n",
      "batch: 1/1 in epoch 34/1000 \n",
      "... loss: 0.25114503502845764\n",
      "\n",
      "batch: 1/1 in epoch 35/1000 \n",
      "... loss: 0.251138299703598\n",
      "\n",
      "batch: 1/1 in epoch 36/1000 \n",
      "... loss: 0.2511315941810608\n",
      "\n",
      "batch: 1/1 in epoch 37/1000 \n",
      "... loss: 0.2511249780654907\n",
      "\n",
      "batch: 1/1 in epoch 38/1000 \n",
      "... loss: 0.25111839175224304\n",
      "\n",
      "batch: 1/1 in epoch 39/1000 \n",
      "... loss: 0.2511118948459625\n",
      "\n",
      "batch: 1/1 in epoch 40/1000 \n",
      "... loss: 0.2511054575443268\n",
      "\n",
      "batch: 1/1 in epoch 41/1000 \n",
      "... loss: 0.2510990500450134\n",
      "\n",
      "batch: 1/1 in epoch 42/1000 \n",
      "... loss: 0.25109270215034485\n",
      "\n",
      "batch: 1/1 in epoch 43/1000 \n",
      "... loss: 0.25108641386032104\n",
      "\n",
      "batch: 1/1 in epoch 44/1000 \n",
      "... loss: 0.2510802149772644\n",
      "\n",
      "batch: 1/1 in epoch 45/1000 \n",
      "... loss: 0.25107401609420776\n",
      "\n",
      "batch: 1/1 in epoch 46/1000 \n",
      "... loss: 0.2510679066181183\n",
      "\n",
      "batch: 1/1 in epoch 47/1000 \n",
      "... loss: 0.2510618269443512\n",
      "\n",
      "batch: 1/1 in epoch 48/1000 \n",
      "... loss: 0.2510558068752289\n",
      "\n",
      "batch: 1/1 in epoch 49/1000 \n",
      "... loss: 0.25104984641075134\n",
      "\n",
      "batch: 1/1 in epoch 50/1000 \n",
      "... loss: 0.2510439455509186\n",
      "\n",
      "batch: 1/1 in epoch 51/1000 \n",
      "... loss: 0.2510380744934082\n",
      "\n",
      "batch: 1/1 in epoch 52/1000 \n",
      "... loss: 0.2510322630405426\n",
      "\n",
      "batch: 1/1 in epoch 53/1000 \n",
      "... loss: 0.2510264813899994\n",
      "\n",
      "batch: 1/1 in epoch 54/1000 \n",
      "... loss: 0.25102078914642334\n",
      "\n",
      "batch: 1/1 in epoch 55/1000 \n",
      "... loss: 0.2510151267051697\n",
      "\n",
      "batch: 1/1 in epoch 56/1000 \n",
      "... loss: 0.2510094940662384\n",
      "\n",
      "batch: 1/1 in epoch 57/1000 \n",
      "... loss: 0.2510039210319519\n",
      "\n",
      "batch: 1/1 in epoch 58/1000 \n",
      "... loss: 0.2509984076023102\n",
      "\n",
      "batch: 1/1 in epoch 59/1000 \n",
      "... loss: 0.25099292397499084\n",
      "\n",
      "batch: 1/1 in epoch 60/1000 \n",
      "... loss: 0.2509874999523163\n",
      "\n",
      "batch: 1/1 in epoch 61/1000 \n",
      "... loss: 0.2509821057319641\n",
      "\n",
      "batch: 1/1 in epoch 62/1000 \n",
      "... loss: 0.2509767711162567\n",
      "\n",
      "batch: 1/1 in epoch 63/1000 \n",
      "... loss: 0.2509714663028717\n",
      "\n",
      "batch: 1/1 in epoch 64/1000 \n",
      "... loss: 0.25096622109413147\n",
      "\n",
      "batch: 1/1 in epoch 65/1000 \n",
      "... loss: 0.2509610056877136\n",
      "\n",
      "batch: 1/1 in epoch 66/1000 \n",
      "... loss: 0.25095584988594055\n",
      "\n",
      "batch: 1/1 in epoch 67/1000 \n",
      "... loss: 0.25095072388648987\n",
      "\n",
      "batch: 1/1 in epoch 68/1000 \n",
      "... loss: 0.2509456276893616\n",
      "\n",
      "batch: 1/1 in epoch 69/1000 \n",
      "... loss: 0.25094059109687805\n",
      "\n",
      "batch: 1/1 in epoch 70/1000 \n",
      "... loss: 0.2509355843067169\n",
      "\n",
      "batch: 1/1 in epoch 71/1000 \n",
      "... loss: 0.25093063712120056\n",
      "\n",
      "batch: 1/1 in epoch 72/1000 \n",
      "... loss: 0.2509257197380066\n",
      "\n",
      "batch: 1/1 in epoch 73/1000 \n",
      "... loss: 0.250920832157135\n",
      "\n",
      "batch: 1/1 in epoch 74/1000 \n",
      "... loss: 0.2509160041809082\n",
      "\n",
      "batch: 1/1 in epoch 75/1000 \n",
      "... loss: 0.2509111762046814\n",
      "\n",
      "batch: 1/1 in epoch 76/1000 \n",
      "... loss: 0.25090643763542175\n",
      "\n",
      "batch: 1/1 in epoch 77/1000 \n",
      "... loss: 0.2509016990661621\n",
      "\n",
      "batch: 1/1 in epoch 78/1000 \n",
      "... loss: 0.25089702010154724\n",
      "\n",
      "batch: 1/1 in epoch 79/1000 \n",
      "... loss: 0.25089237093925476\n",
      "\n",
      "batch: 1/1 in epoch 80/1000 \n",
      "... loss: 0.25088775157928467\n",
      "\n",
      "batch: 1/1 in epoch 81/1000 \n",
      "... loss: 0.25088316202163696\n",
      "\n",
      "batch: 1/1 in epoch 82/1000 \n",
      "... loss: 0.25087863206863403\n",
      "\n",
      "batch: 1/1 in epoch 83/1000 \n",
      "... loss: 0.2508741319179535\n",
      "\n",
      "batch: 1/1 in epoch 84/1000 \n",
      "... loss: 0.25086966156959534\n",
      "\n",
      "batch: 1/1 in epoch 85/1000 \n",
      "... loss: 0.2508651912212372\n",
      "\n",
      "batch: 1/1 in epoch 86/1000 \n",
      "... loss: 0.2508608102798462\n",
      "\n",
      "batch: 1/1 in epoch 87/1000 \n",
      "... loss: 0.2508564293384552\n",
      "\n",
      "batch: 1/1 in epoch 88/1000 \n",
      "... loss: 0.250852108001709\n",
      "\n",
      "batch: 1/1 in epoch 89/1000 \n",
      "... loss: 0.25084778666496277\n",
      "\n",
      "batch: 1/1 in epoch 90/1000 \n",
      "... loss: 0.25084352493286133\n",
      "\n",
      "batch: 1/1 in epoch 91/1000 \n",
      "... loss: 0.2508392930030823\n",
      "\n",
      "batch: 1/1 in epoch 92/1000 \n",
      "... loss: 0.2508350908756256\n",
      "\n",
      "batch: 1/1 in epoch 93/1000 \n",
      "... loss: 0.25083091855049133\n",
      "\n",
      "batch: 1/1 in epoch 94/1000 \n",
      "... loss: 0.25082677602767944\n",
      "\n",
      "batch: 1/1 in epoch 95/1000 \n",
      "... loss: 0.25082266330718994\n",
      "\n",
      "batch: 1/1 in epoch 96/1000 \n",
      "... loss: 0.2508186101913452\n",
      "\n",
      "batch: 1/1 in epoch 97/1000 \n",
      "... loss: 0.2508145570755005\n",
      "\n",
      "batch: 1/1 in epoch 98/1000 \n",
      "... loss: 0.25081053376197815\n",
      "\n",
      "batch: 1/1 in epoch 99/1000 \n",
      "... loss: 0.2508065700531006\n",
      "\n",
      "batch: 1/1 in epoch 100/1000 \n",
      "... loss: 0.250802606344223\n",
      "\n",
      "batch: 1/1 in epoch 101/1000 \n",
      "... loss: 0.25079867243766785\n",
      "\n",
      "batch: 1/1 in epoch 102/1000 \n",
      "... loss: 0.25079476833343506\n",
      "\n",
      "batch: 1/1 in epoch 103/1000 \n",
      "... loss: 0.25079092383384705\n",
      "\n",
      "batch: 1/1 in epoch 104/1000 \n",
      "... loss: 0.25078707933425903\n",
      "\n",
      "batch: 1/1 in epoch 105/1000 \n",
      "... loss: 0.2507832646369934\n",
      "\n",
      "batch: 1/1 in epoch 106/1000 \n",
      "... loss: 0.25077947974205017\n",
      "\n",
      "batch: 1/1 in epoch 107/1000 \n",
      "... loss: 0.2507757544517517\n",
      "\n",
      "batch: 1/1 in epoch 108/1000 \n",
      "... loss: 0.25077199935913086\n",
      "\n",
      "batch: 1/1 in epoch 109/1000 \n",
      "... loss: 0.2507683038711548\n",
      "\n",
      "batch: 1/1 in epoch 110/1000 \n",
      "... loss: 0.2507646381855011\n",
      "\n",
      "batch: 1/1 in epoch 111/1000 \n",
      "... loss: 0.2507610023021698\n",
      "\n",
      "batch: 1/1 in epoch 112/1000 \n",
      "... loss: 0.2507573664188385\n",
      "\n",
      "batch: 1/1 in epoch 113/1000 \n",
      "... loss: 0.250753790140152\n",
      "\n",
      "batch: 1/1 in epoch 114/1000 \n",
      "... loss: 0.25075021386146545\n",
      "\n",
      "batch: 1/1 in epoch 115/1000 \n",
      "... loss: 0.2507466673851013\n",
      "\n",
      "batch: 1/1 in epoch 116/1000 \n",
      "... loss: 0.25074315071105957\n",
      "\n",
      "batch: 1/1 in epoch 117/1000 \n",
      "... loss: 0.2507396638393402\n",
      "\n",
      "batch: 1/1 in epoch 118/1000 \n",
      "... loss: 0.25073620676994324\n",
      "\n",
      "batch: 1/1 in epoch 119/1000 \n",
      "... loss: 0.25073277950286865\n",
      "\n",
      "batch: 1/1 in epoch 120/1000 \n",
      "... loss: 0.25072935223579407\n",
      "\n",
      "batch: 1/1 in epoch 121/1000 \n",
      "... loss: 0.25072595477104187\n",
      "\n",
      "batch: 1/1 in epoch 122/1000 \n",
      "... loss: 0.25072258710861206\n",
      "\n",
      "batch: 1/1 in epoch 123/1000 \n",
      "... loss: 0.25071924924850464\n",
      "\n",
      "batch: 1/1 in epoch 124/1000 \n",
      "... loss: 0.2507159411907196\n",
      "\n",
      "batch: 1/1 in epoch 125/1000 \n",
      "... loss: 0.25071263313293457\n",
      "\n",
      "batch: 1/1 in epoch 126/1000 \n",
      "... loss: 0.2507093548774719\n",
      "\n",
      "batch: 1/1 in epoch 127/1000 \n",
      "... loss: 0.25070610642433167\n",
      "\n",
      "batch: 1/1 in epoch 128/1000 \n",
      "... loss: 0.2507028877735138\n",
      "\n",
      "batch: 1/1 in epoch 129/1000 \n",
      "... loss: 0.2506996691226959\n",
      "\n",
      "batch: 1/1 in epoch 130/1000 \n",
      "... loss: 0.2506965100765228\n",
      "\n",
      "batch: 1/1 in epoch 131/1000 \n",
      "... loss: 0.25069332122802734\n",
      "\n",
      "batch: 1/1 in epoch 132/1000 \n",
      "... loss: 0.25069019198417664\n",
      "\n",
      "batch: 1/1 in epoch 133/1000 \n",
      "... loss: 0.2506870627403259\n",
      "\n",
      "batch: 1/1 in epoch 134/1000 \n",
      "... loss: 0.25068399310112\n",
      "\n",
      "batch: 1/1 in epoch 135/1000 \n",
      "... loss: 0.25068092346191406\n",
      "\n",
      "batch: 1/1 in epoch 136/1000 \n",
      "... loss: 0.25067785382270813\n",
      "\n",
      "batch: 1/1 in epoch 137/1000 \n",
      "... loss: 0.250674843788147\n",
      "\n",
      "batch: 1/1 in epoch 138/1000 \n",
      "... loss: 0.2506718337535858\n",
      "\n",
      "batch: 1/1 in epoch 139/1000 \n",
      "... loss: 0.25066882371902466\n",
      "\n",
      "batch: 1/1 in epoch 140/1000 \n",
      "... loss: 0.2506658732891083\n",
      "\n",
      "batch: 1/1 in epoch 141/1000 \n",
      "... loss: 0.2506629228591919\n",
      "\n",
      "batch: 1/1 in epoch 142/1000 \n",
      "... loss: 0.2506599724292755\n",
      "\n",
      "batch: 1/1 in epoch 143/1000 \n",
      "... loss: 0.2506570816040039\n",
      "\n",
      "batch: 1/1 in epoch 144/1000 \n",
      "... loss: 0.2506541907787323\n",
      "\n",
      "batch: 1/1 in epoch 145/1000 \n",
      "... loss: 0.2506512999534607\n",
      "\n",
      "batch: 1/1 in epoch 146/1000 \n",
      "... loss: 0.25064846873283386\n",
      "\n",
      "batch: 1/1 in epoch 147/1000 \n",
      "... loss: 0.25064563751220703\n",
      "\n",
      "batch: 1/1 in epoch 148/1000 \n",
      "... loss: 0.2506428062915802\n",
      "\n",
      "batch: 1/1 in epoch 149/1000 \n",
      "... loss: 0.25064000487327576\n",
      "\n",
      "batch: 1/1 in epoch 150/1000 \n",
      "... loss: 0.2506372332572937\n",
      "\n",
      "batch: 1/1 in epoch 151/1000 \n",
      "... loss: 0.25063449144363403\n",
      "\n",
      "batch: 1/1 in epoch 152/1000 \n",
      "... loss: 0.25063174962997437\n",
      "\n",
      "batch: 1/1 in epoch 153/1000 \n",
      "... loss: 0.2506290376186371\n",
      "\n",
      "batch: 1/1 in epoch 154/1000 \n",
      "... loss: 0.2506263256072998\n",
      "\n",
      "batch: 1/1 in epoch 155/1000 \n",
      "... loss: 0.2506236433982849\n",
      "\n",
      "batch: 1/1 in epoch 156/1000 \n",
      "... loss: 0.25062096118927\n",
      "\n",
      "batch: 1/1 in epoch 157/1000 \n",
      "... loss: 0.2506183087825775\n",
      "\n",
      "batch: 1/1 in epoch 158/1000 \n",
      "... loss: 0.2506156861782074\n",
      "\n",
      "batch: 1/1 in epoch 159/1000 \n",
      "... loss: 0.2506130635738373\n",
      "\n",
      "batch: 1/1 in epoch 160/1000 \n",
      "... loss: 0.25061047077178955\n",
      "\n",
      "batch: 1/1 in epoch 161/1000 \n",
      "... loss: 0.2506078779697418\n",
      "\n",
      "batch: 1/1 in epoch 162/1000 \n",
      "... loss: 0.2506053149700165\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 163/1000 \n",
      "... loss: 0.25060275197029114\n",
      "\n",
      "batch: 1/1 in epoch 164/1000 \n",
      "... loss: 0.2506002187728882\n",
      "\n",
      "batch: 1/1 in epoch 165/1000 \n",
      "... loss: 0.2505977153778076\n",
      "\n",
      "batch: 1/1 in epoch 166/1000 \n",
      "... loss: 0.25059521198272705\n",
      "\n",
      "batch: 1/1 in epoch 167/1000 \n",
      "... loss: 0.25059273838996887\n",
      "\n",
      "batch: 1/1 in epoch 168/1000 \n",
      "... loss: 0.2505902647972107\n",
      "\n",
      "batch: 1/1 in epoch 169/1000 \n",
      "... loss: 0.2505878210067749\n",
      "\n",
      "batch: 1/1 in epoch 170/1000 \n",
      "... loss: 0.2505853772163391\n",
      "\n",
      "batch: 1/1 in epoch 171/1000 \n",
      "... loss: 0.2505829334259033\n",
      "\n",
      "batch: 1/1 in epoch 172/1000 \n",
      "... loss: 0.2505805492401123\n",
      "\n",
      "batch: 1/1 in epoch 173/1000 \n",
      "... loss: 0.2505781352519989\n",
      "\n",
      "batch: 1/1 in epoch 174/1000 \n",
      "... loss: 0.2505757808685303\n",
      "\n",
      "batch: 1/1 in epoch 175/1000 \n",
      "... loss: 0.25057339668273926\n",
      "\n",
      "batch: 1/1 in epoch 176/1000 \n",
      "... loss: 0.250571072101593\n",
      "\n",
      "batch: 1/1 in epoch 177/1000 \n",
      "... loss: 0.2505687177181244\n",
      "\n",
      "batch: 1/1 in epoch 178/1000 \n",
      "... loss: 0.25056639313697815\n",
      "\n",
      "batch: 1/1 in epoch 179/1000 \n",
      "... loss: 0.2505641281604767\n",
      "\n",
      "batch: 1/1 in epoch 180/1000 \n",
      "... loss: 0.25056180357933044\n",
      "\n",
      "batch: 1/1 in epoch 181/1000 \n",
      "... loss: 0.250559538602829\n",
      "\n",
      "batch: 1/1 in epoch 182/1000 \n",
      "... loss: 0.2505572736263275\n",
      "\n",
      "batch: 1/1 in epoch 183/1000 \n",
      "... loss: 0.25055503845214844\n",
      "\n",
      "batch: 1/1 in epoch 184/1000 \n",
      "... loss: 0.25055280327796936\n",
      "\n",
      "batch: 1/1 in epoch 185/1000 \n",
      "... loss: 0.2505505681037903\n",
      "\n",
      "batch: 1/1 in epoch 186/1000 \n",
      "... loss: 0.2505483627319336\n",
      "\n",
      "batch: 1/1 in epoch 187/1000 \n",
      "... loss: 0.2505461871623993\n",
      "\n",
      "batch: 1/1 in epoch 188/1000 \n",
      "... loss: 0.250544011592865\n",
      "\n",
      "batch: 1/1 in epoch 189/1000 \n",
      "... loss: 0.2505418360233307\n",
      "\n",
      "batch: 1/1 in epoch 190/1000 \n",
      "... loss: 0.2505396902561188\n",
      "\n",
      "batch: 1/1 in epoch 191/1000 \n",
      "... loss: 0.25053754448890686\n",
      "\n",
      "batch: 1/1 in epoch 192/1000 \n",
      "... loss: 0.25053542852401733\n",
      "\n",
      "batch: 1/1 in epoch 193/1000 \n",
      "... loss: 0.2505333125591278\n",
      "\n",
      "batch: 1/1 in epoch 194/1000 \n",
      "... loss: 0.2505311965942383\n",
      "\n",
      "batch: 1/1 in epoch 195/1000 \n",
      "... loss: 0.25052911043167114\n",
      "\n",
      "batch: 1/1 in epoch 196/1000 \n",
      "... loss: 0.250527024269104\n",
      "\n",
      "batch: 1/1 in epoch 197/1000 \n",
      "... loss: 0.25052496790885925\n",
      "\n",
      "batch: 1/1 in epoch 198/1000 \n",
      "... loss: 0.2505229115486145\n",
      "\n",
      "batch: 1/1 in epoch 199/1000 \n",
      "... loss: 0.25052088499069214\n",
      "\n",
      "batch: 1/1 in epoch 200/1000 \n",
      "... loss: 0.2505188286304474\n",
      "\n",
      "batch: 1/1 in epoch 201/1000 \n",
      "... loss: 0.2505168318748474\n",
      "\n",
      "batch: 1/1 in epoch 202/1000 \n",
      "... loss: 0.25051483511924744\n",
      "\n",
      "batch: 1/1 in epoch 203/1000 \n",
      "... loss: 0.25051283836364746\n",
      "\n",
      "batch: 1/1 in epoch 204/1000 \n",
      "... loss: 0.2505108714103699\n",
      "\n",
      "batch: 1/1 in epoch 205/1000 \n",
      "... loss: 0.2505089044570923\n",
      "\n",
      "batch: 1/1 in epoch 206/1000 \n",
      "... loss: 0.2505069375038147\n",
      "\n",
      "batch: 1/1 in epoch 207/1000 \n",
      "... loss: 0.2505050003528595\n",
      "\n",
      "batch: 1/1 in epoch 208/1000 \n",
      "... loss: 0.2505030632019043\n",
      "\n",
      "batch: 1/1 in epoch 209/1000 \n",
      "... loss: 0.2505011260509491\n",
      "\n",
      "batch: 1/1 in epoch 210/1000 \n",
      "... loss: 0.2504992187023163\n",
      "\n",
      "batch: 1/1 in epoch 211/1000 \n",
      "... loss: 0.25049731135368347\n",
      "\n",
      "batch: 1/1 in epoch 212/1000 \n",
      "... loss: 0.25049543380737305\n",
      "\n",
      "batch: 1/1 in epoch 213/1000 \n",
      "... loss: 0.25049352645874023\n",
      "\n",
      "batch: 1/1 in epoch 214/1000 \n",
      "... loss: 0.2504916787147522\n",
      "\n",
      "batch: 1/1 in epoch 215/1000 \n",
      "... loss: 0.25048983097076416\n",
      "\n",
      "batch: 1/1 in epoch 216/1000 \n",
      "... loss: 0.25048795342445374\n",
      "\n",
      "batch: 1/1 in epoch 217/1000 \n",
      "... loss: 0.2504861354827881\n",
      "\n",
      "batch: 1/1 in epoch 218/1000 \n",
      "... loss: 0.25048431754112244\n",
      "\n",
      "batch: 1/1 in epoch 219/1000 \n",
      "... loss: 0.2504824995994568\n",
      "\n",
      "batch: 1/1 in epoch 220/1000 \n",
      "... loss: 0.25048068165779114\n",
      "\n",
      "batch: 1/1 in epoch 221/1000 \n",
      "... loss: 0.2504788935184479\n",
      "\n",
      "batch: 1/1 in epoch 222/1000 \n",
      "... loss: 0.2504771053791046\n",
      "\n",
      "batch: 1/1 in epoch 223/1000 \n",
      "... loss: 0.25047534704208374\n",
      "\n",
      "batch: 1/1 in epoch 224/1000 \n",
      "... loss: 0.2504735589027405\n",
      "\n",
      "batch: 1/1 in epoch 225/1000 \n",
      "... loss: 0.2504718005657196\n",
      "\n",
      "batch: 1/1 in epoch 226/1000 \n",
      "... loss: 0.2504700720310211\n",
      "\n",
      "batch: 1/1 in epoch 227/1000 \n",
      "... loss: 0.25046834349632263\n",
      "\n",
      "batch: 1/1 in epoch 228/1000 \n",
      "... loss: 0.25046661496162415\n",
      "\n",
      "batch: 1/1 in epoch 229/1000 \n",
      "... loss: 0.25046488642692566\n",
      "\n",
      "batch: 1/1 in epoch 230/1000 \n",
      "... loss: 0.25046318769454956\n",
      "\n",
      "batch: 1/1 in epoch 231/1000 \n",
      "... loss: 0.25046148896217346\n",
      "\n",
      "batch: 1/1 in epoch 232/1000 \n",
      "... loss: 0.25045979022979736\n",
      "\n",
      "batch: 1/1 in epoch 233/1000 \n",
      "... loss: 0.25045812129974365\n",
      "\n",
      "batch: 1/1 in epoch 234/1000 \n",
      "... loss: 0.25045645236968994\n",
      "\n",
      "batch: 1/1 in epoch 235/1000 \n",
      "... loss: 0.25045478343963623\n",
      "\n",
      "batch: 1/1 in epoch 236/1000 \n",
      "... loss: 0.2504531443119049\n",
      "\n",
      "batch: 1/1 in epoch 237/1000 \n",
      "... loss: 0.2504515051841736\n",
      "\n",
      "batch: 1/1 in epoch 238/1000 \n",
      "... loss: 0.25044986605644226\n",
      "\n",
      "batch: 1/1 in epoch 239/1000 \n",
      "... loss: 0.25044822692871094\n",
      "\n",
      "batch: 1/1 in epoch 240/1000 \n",
      "... loss: 0.250446617603302\n",
      "\n",
      "batch: 1/1 in epoch 241/1000 \n",
      "... loss: 0.25044500827789307\n",
      "\n",
      "batch: 1/1 in epoch 242/1000 \n",
      "... loss: 0.2504434287548065\n",
      "\n",
      "batch: 1/1 in epoch 243/1000 \n",
      "... loss: 0.2504418194293976\n",
      "\n",
      "batch: 1/1 in epoch 244/1000 \n",
      "... loss: 0.25044023990631104\n",
      "\n",
      "batch: 1/1 in epoch 245/1000 \n",
      "... loss: 0.2504386603832245\n",
      "\n",
      "batch: 1/1 in epoch 246/1000 \n",
      "... loss: 0.2504371106624603\n",
      "\n",
      "batch: 1/1 in epoch 247/1000 \n",
      "... loss: 0.25043556094169617\n",
      "\n",
      "batch: 1/1 in epoch 248/1000 \n",
      "... loss: 0.250434011220932\n",
      "\n",
      "batch: 1/1 in epoch 249/1000 \n",
      "... loss: 0.25043246150016785\n",
      "\n",
      "batch: 1/1 in epoch 250/1000 \n",
      "... loss: 0.2504309415817261\n",
      "\n",
      "batch: 1/1 in epoch 251/1000 \n",
      "... loss: 0.2504294216632843\n",
      "\n",
      "batch: 1/1 in epoch 252/1000 \n",
      "... loss: 0.25042790174484253\n",
      "\n",
      "batch: 1/1 in epoch 253/1000 \n",
      "... loss: 0.25042638182640076\n",
      "\n",
      "batch: 1/1 in epoch 254/1000 \n",
      "... loss: 0.25042489171028137\n",
      "\n",
      "batch: 1/1 in epoch 255/1000 \n",
      "... loss: 0.250423401594162\n",
      "\n",
      "batch: 1/1 in epoch 256/1000 \n",
      "... loss: 0.250421941280365\n",
      "\n",
      "batch: 1/1 in epoch 257/1000 \n",
      "... loss: 0.2504204511642456\n",
      "\n",
      "batch: 1/1 in epoch 258/1000 \n",
      "... loss: 0.2504189908504486\n",
      "\n",
      "batch: 1/1 in epoch 259/1000 \n",
      "... loss: 0.2504175305366516\n",
      "\n",
      "batch: 1/1 in epoch 260/1000 \n",
      "... loss: 0.2504160702228546\n",
      "\n",
      "batch: 1/1 in epoch 261/1000 \n",
      "... loss: 0.2504146099090576\n",
      "\n",
      "batch: 1/1 in epoch 262/1000 \n",
      "... loss: 0.250413179397583\n",
      "\n",
      "batch: 1/1 in epoch 263/1000 \n",
      "... loss: 0.2504117488861084\n",
      "\n",
      "batch: 1/1 in epoch 264/1000 \n",
      "... loss: 0.2504103183746338\n",
      "\n",
      "batch: 1/1 in epoch 265/1000 \n",
      "... loss: 0.25040891766548157\n",
      "\n",
      "batch: 1/1 in epoch 266/1000 \n",
      "... loss: 0.25040751695632935\n",
      "\n",
      "batch: 1/1 in epoch 267/1000 \n",
      "... loss: 0.2504061162471771\n",
      "\n",
      "batch: 1/1 in epoch 268/1000 \n",
      "... loss: 0.2504047155380249\n",
      "\n",
      "batch: 1/1 in epoch 269/1000 \n",
      "... loss: 0.2504033148288727\n",
      "\n",
      "batch: 1/1 in epoch 270/1000 \n",
      "... loss: 0.25040194392204285\n",
      "\n",
      "batch: 1/1 in epoch 271/1000 \n",
      "... loss: 0.250400573015213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# debug set\n",
    "net_full = Lin_Net(8, 1, 64, act_function)\n",
    "train_loader_debug, test_loader_debug = make_data(emotion_dataset, \"full\", batch_size, True)\n",
    "train(train_loader_debug, net_full, 1000, criterion, 100, \"../logs/mse_debug\", cuda, 0.1)\n",
    "test(test_loader_debug, net_full, criterion, 100, \"../logs/mse_debug\", cuda)\n",
    "\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------- net_lin_emotion_full\")\n",
    "#net_full = Lin_Net(8, 1, 64, act_function)\n",
    "#train_loader_emotion_full, test_loader_emotion_full = make_data(emotion_dataset, \"full\", batch_size)\n",
    "#train(train_loader_emotion_full, net_full, 100, criterion, 5000, \"../logs/mse_emotion_full\", cuda, 0.1)\n",
    "#test(test_loader_emotion_full, net_full, criterion, 1000, \"../logs/mse_emotion_full\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_nolex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_emotion_nolex, test_loader_emotion_nolex = make_data(emotion_dataset, \"nolex\", batch_size)\n",
    "#train(train_loader_emotion_nolex, net_half, 100, criterion, 5000, \"../logs/mse_emotion_nolex\", cuda, 0.1)\n",
    "#test(test_loader_emotion_nolex, net_half, criterion, 1000, \"../logs/mse_emotion_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_lex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_emotion_lex, test_loader_emotion_lex = make_data(emotion_dataset, \"lex\", batch_size)\n",
    "#train(train_loader_emotion_lex, net_half, 100, criterion, 5000, \"../logs/mse_emotion_lex\", cuda, 0.1)\n",
    "#test(test_loader_emotion_lex, net_half, criterion, 1000, \"../logs/mse_emotion_lex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_full\")\n",
    "#net_full = Lin_Net(8, 1, 64, act_function)\n",
    "#train_loader_tweet_full, test_loader_tweet_full = make_data(tweet_dataset, \"full\", batch_size)\n",
    "#train(train_loader_tweet_full, net_full, 100, criterion, 5000, \"../logs/mse_tweet_full\", cuda, 0.1)\n",
    "#test(test_loader_tweet_full, net_full, criterion, 1000, \"../logs/mse_tweet_full\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_nolex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_tweet_nolex, test_loader_tweet_nolex = make_data(tweet_dataset, \"nolex\", batch_size)\n",
    "#train(train_loader_tweet_nolex, net_half, 100, criterion, 5000, \"../logs/mse_tweet_nolex\", cuda, 0.1)\n",
    "#test(test_loader_tweet_nolex, net_half, criterion, 1000, \"../logs/mse_tweet_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_lex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_tweet_lex, test_loader_tweet_lex = make_data(tweet_dataset, \"lex\", batch_size)\n",
    "#train(train_loader_tweet_lex, net_half, 100, criterion, 5000, \"../logs/mse_tweet_lex\", cuda, 0.1)\n",
    "#test(test_loader_tweet_lex, net_half, criterion, 1000, \"../logs/mse_tweet_lex\")\n",
    "\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
