{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, act_function):\n",
    "        super(Lin_Net, self).__init__()\n",
    "        self.act_function = act_function\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # act_funtion = F.sigmoid oder F.relu\n",
    "        x = self.act_function(self.lin1(x))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, features, batch_size, debug=False):\n",
    "    datasets = []\n",
    "    for file in dataset:\n",
    "        datasets.append(pd.read_csv(\"../\" + file))\n",
    "    dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "    \n",
    "    target = dataset[\"affect\"]\n",
    "    dataset_full = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"h_count\", \"s_count\", \"a_count\", \"f_count\", \"cons_punct_count\"]]\n",
    "    dataset_nolex = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"cons_punct_count\"]]\n",
    "    dataset_lex = dataset[[\"h_count\", \"s_count\", \"a_count\", \"f_count\"]]\n",
    "    \n",
    "    # make train and test sets\n",
    "    if features == \"full\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.2)\n",
    "    elif features == \"nolex\":\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_nolex, target, test_size=0.2)\n",
    "    elif features == \"lex\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_lex, target, test_size=0.2)\n",
    "\n",
    "    # make data loaders\n",
    "    train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "    test_data = MyDataset(test_x.to_numpy(), test_y.to_numpy())\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    \n",
    "    if debug: \n",
    "        dataset_full = dataset_full.iloc[:10]\n",
    "        target = target[:10]\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.8)\n",
    "        train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "        test_loader = DataLoader(dataset=train_data, batch_size=1)\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(summary, file):\n",
    "    log = open(file, \"a\")\n",
    "    log.write(summary)\n",
    "    log.close()\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epochs, criterion, print_every, save_name, cuda, lr):\n",
    "    open(save_name + \"_train\", \"w\").close()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.5)\n",
    "    error_curve = []\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        for index, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            if cuda: \n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                net = net.cuda()\n",
    "            pred = net(inputs)    \n",
    "            loss = criterion(pred, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((index) % print_every == 0):\n",
    "                log(\"batch: {}/{} in epoch {}/{} \\n... loss: {}\\n\".\n",
    "                    format((index+1), len(train_loader), (epoch+1), epochs, loss.item()), \n",
    "                    save_name + \"_train\")\n",
    "        # save network after every epoch\n",
    "        torch.save(net.state_dict(), save_name + \".pt\")  \n",
    "        # after every epoch save the error\n",
    "        error_curve.append([epoch, loss.item()])\n",
    "    log(\"\\n\" + str(error_curve), save_name + \"_train\")\n",
    "    plt.plot([item[0] for item in error_curve], [item[1] for item in error_curve])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.savefig(save_name+\"_train_error.png\")\n",
    "\n",
    "def test(test_loader, net, criterion, print_every, save_name, cuda):\n",
    "    open(save_name + \"_test\", \"w\").close()\n",
    "    confusion = []\n",
    "    net.eval()\n",
    "    loss_sum, correct, correct2 = 0, 0, 0\n",
    "    for index, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        if cuda: \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            net = net.cuda()\n",
    "        pred = net(inputs)\n",
    "        pred_class = round(pred.item())\n",
    "        loss_sum += criterion(pred, targets).item()\n",
    "        confusion.append([targets.item(), pred_class])\n",
    "        \n",
    "        # correct? \n",
    "        if pred_class == targets.item():\n",
    "            correct += 1\n",
    "        \n",
    "        if ((index) % print_every == 0):\n",
    "            log(\"batch: {}/{}\\n... correct: {}\\n\".\n",
    "                format((index+1), len(test_loader), correct), \n",
    "                save_name + \"_test\")\n",
    "           \n",
    "    # give end report\n",
    "    log(\"average test loss: {}, relative correct: {}\\n\\nconfusion:\\n{}\".\n",
    "        format((loss_sum / len(test_loader)), (correct / len(test_loader)),str(confusion)), \n",
    "        save_name + \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables\n"
     ]
    }
   ],
   "source": [
    "# create variables \n",
    "print(\"creating variables\")\n",
    "emotion_dataset = [\"emotion_classification_1_clean.csv\", \"emotion_classification_2_clean.csv\", \"emotion_classification_3_clean.csv\", \"emotion_classification_4_clean.csv\", \"emotion_classification_5_clean.csv\", \"emotion_classification_6_clean.csv\", \"emotion_classification_7_clean.csv\", \"emotion_classification_8_clean.csv\"]\n",
    "tweet_dataset = [\"crowdflower_clean.csv\", \"emoint_clean.csv\", \"tec_clean.csv\"]\n",
    "act_function = torch.sigmoid\n",
    "criterion = nn.MSELoss()\n",
    "cuda = torch.cuda.is_available()\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 1/1000 \n",
      "... loss: 0.2837662994861603\n",
      "\n",
      "batch: 1/1 in epoch 2/1000 \n",
      "... loss: 0.5006978511810303\n",
      "\n",
      "batch: 1/1 in epoch 3/1000 \n",
      "... loss: 1.2827690839767456\n",
      "\n",
      "batch: 1/1 in epoch 4/1000 \n",
      "... loss: 4.073252201080322\n",
      "\n",
      "batch: 1/1 in epoch 5/1000 \n",
      "... loss: 10.667582511901855\n",
      "\n",
      "batch: 1/1 in epoch 6/1000 \n",
      "... loss: 11.398224830627441\n",
      "\n",
      "batch: 1/1 in epoch 7/1000 \n",
      "... loss: 0.32193297147750854\n",
      "\n",
      "batch: 1/1 in epoch 8/1000 \n",
      "... loss: 0.6663562655448914\n",
      "\n",
      "batch: 1/1 in epoch 9/1000 \n",
      "... loss: 0.6582034826278687\n",
      "\n",
      "batch: 1/1 in epoch 10/1000 \n",
      "... loss: 0.3842248320579529\n",
      "\n",
      "batch: 1/1 in epoch 11/1000 \n",
      "... loss: 0.26299160718917847\n",
      "\n",
      "batch: 1/1 in epoch 12/1000 \n",
      "... loss: 0.25382012128829956\n",
      "\n",
      "batch: 1/1 in epoch 13/1000 \n",
      "... loss: 0.2635621428489685\n",
      "\n",
      "batch: 1/1 in epoch 14/1000 \n",
      "... loss: 0.2625691592693329\n",
      "\n",
      "batch: 1/1 in epoch 15/1000 \n",
      "... loss: 0.2563020884990692\n",
      "\n",
      "batch: 1/1 in epoch 16/1000 \n",
      "... loss: 0.2522393763065338\n",
      "\n",
      "batch: 1/1 in epoch 17/1000 \n",
      "... loss: 0.25127288699150085\n",
      "\n",
      "batch: 1/1 in epoch 18/1000 \n",
      "... loss: 0.2514573335647583\n",
      "\n",
      "batch: 1/1 in epoch 19/1000 \n",
      "... loss: 0.2515496611595154\n",
      "\n",
      "batch: 1/1 in epoch 20/1000 \n",
      "... loss: 0.2514254152774811\n",
      "\n",
      "batch: 1/1 in epoch 21/1000 \n",
      "... loss: 0.251291424036026\n",
      "\n",
      "batch: 1/1 in epoch 22/1000 \n",
      "... loss: 0.251234769821167\n",
      "\n",
      "batch: 1/1 in epoch 23/1000 \n",
      "... loss: 0.25122538208961487\n",
      "\n",
      "batch: 1/1 in epoch 24/1000 \n",
      "... loss: 0.2512226402759552\n",
      "\n",
      "batch: 1/1 in epoch 25/1000 \n",
      "... loss: 0.25121423602104187\n",
      "\n",
      "batch: 1/1 in epoch 26/1000 \n",
      "... loss: 0.25120362639427185\n",
      "\n",
      "batch: 1/1 in epoch 27/1000 \n",
      "... loss: 0.25119441747665405\n",
      "\n",
      "batch: 1/1 in epoch 28/1000 \n",
      "... loss: 0.2511868476867676\n",
      "\n",
      "batch: 1/1 in epoch 29/1000 \n",
      "... loss: 0.25117984414100647\n",
      "\n",
      "batch: 1/1 in epoch 30/1000 \n",
      "... loss: 0.251172810792923\n",
      "\n",
      "batch: 1/1 in epoch 31/1000 \n",
      "... loss: 0.2511657476425171\n",
      "\n",
      "batch: 1/1 in epoch 32/1000 \n",
      "... loss: 0.251158744096756\n",
      "\n",
      "batch: 1/1 in epoch 33/1000 \n",
      "... loss: 0.2511518597602844\n",
      "\n",
      "batch: 1/1 in epoch 34/1000 \n",
      "... loss: 0.25114503502845764\n",
      "\n",
      "batch: 1/1 in epoch 35/1000 \n",
      "... loss: 0.251138299703598\n",
      "\n",
      "batch: 1/1 in epoch 36/1000 \n",
      "... loss: 0.2511315941810608\n",
      "\n",
      "batch: 1/1 in epoch 37/1000 \n",
      "... loss: 0.2511249780654907\n",
      "\n",
      "batch: 1/1 in epoch 38/1000 \n",
      "... loss: 0.25111839175224304\n",
      "\n",
      "batch: 1/1 in epoch 39/1000 \n",
      "... loss: 0.2511118948459625\n",
      "\n",
      "batch: 1/1 in epoch 40/1000 \n",
      "... loss: 0.2511054575443268\n",
      "\n",
      "batch: 1/1 in epoch 41/1000 \n",
      "... loss: 0.2510990500450134\n",
      "\n",
      "batch: 1/1 in epoch 42/1000 \n",
      "... loss: 0.25109270215034485\n",
      "\n",
      "batch: 1/1 in epoch 43/1000 \n",
      "... loss: 0.25108641386032104\n",
      "\n",
      "batch: 1/1 in epoch 44/1000 \n",
      "... loss: 0.2510802149772644\n",
      "\n",
      "batch: 1/1 in epoch 45/1000 \n",
      "... loss: 0.25107401609420776\n",
      "\n",
      "batch: 1/1 in epoch 46/1000 \n",
      "... loss: 0.2510679066181183\n",
      "\n",
      "batch: 1/1 in epoch 47/1000 \n",
      "... loss: 0.2510618269443512\n",
      "\n",
      "batch: 1/1 in epoch 48/1000 \n",
      "... loss: 0.2510558068752289\n",
      "\n",
      "batch: 1/1 in epoch 49/1000 \n",
      "... loss: 0.25104984641075134\n",
      "\n",
      "batch: 1/1 in epoch 50/1000 \n",
      "... loss: 0.2510439455509186\n",
      "\n",
      "batch: 1/1 in epoch 51/1000 \n",
      "... loss: 0.2510380744934082\n",
      "\n",
      "batch: 1/1 in epoch 52/1000 \n",
      "... loss: 0.2510322630405426\n",
      "\n",
      "batch: 1/1 in epoch 53/1000 \n",
      "... loss: 0.2510264813899994\n",
      "\n",
      "batch: 1/1 in epoch 54/1000 \n",
      "... loss: 0.25102078914642334\n",
      "\n",
      "batch: 1/1 in epoch 55/1000 \n",
      "... loss: 0.2510151267051697\n",
      "\n",
      "batch: 1/1 in epoch 56/1000 \n",
      "... loss: 0.2510094940662384\n",
      "\n",
      "batch: 1/1 in epoch 57/1000 \n",
      "... loss: 0.2510039210319519\n",
      "\n",
      "batch: 1/1 in epoch 58/1000 \n",
      "... loss: 0.2509984076023102\n",
      "\n",
      "batch: 1/1 in epoch 59/1000 \n",
      "... loss: 0.25099292397499084\n",
      "\n",
      "batch: 1/1 in epoch 60/1000 \n",
      "... loss: 0.2509874999523163\n",
      "\n",
      "batch: 1/1 in epoch 61/1000 \n",
      "... loss: 0.2509821057319641\n",
      "\n",
      "batch: 1/1 in epoch 62/1000 \n",
      "... loss: 0.2509767711162567\n",
      "\n",
      "batch: 1/1 in epoch 63/1000 \n",
      "... loss: 0.2509714663028717\n",
      "\n",
      "batch: 1/1 in epoch 64/1000 \n",
      "... loss: 0.25096622109413147\n",
      "\n",
      "batch: 1/1 in epoch 65/1000 \n",
      "... loss: 0.2509610056877136\n",
      "\n",
      "batch: 1/1 in epoch 66/1000 \n",
      "... loss: 0.25095584988594055\n",
      "\n",
      "batch: 1/1 in epoch 67/1000 \n",
      "... loss: 0.25095072388648987\n",
      "\n",
      "batch: 1/1 in epoch 68/1000 \n",
      "... loss: 0.2509456276893616\n",
      "\n",
      "batch: 1/1 in epoch 69/1000 \n",
      "... loss: 0.25094059109687805\n",
      "\n",
      "batch: 1/1 in epoch 70/1000 \n",
      "... loss: 0.2509355843067169\n",
      "\n",
      "batch: 1/1 in epoch 71/1000 \n",
      "... loss: 0.25093063712120056\n",
      "\n",
      "batch: 1/1 in epoch 72/1000 \n",
      "... loss: 0.2509257197380066\n",
      "\n",
      "batch: 1/1 in epoch 73/1000 \n",
      "... loss: 0.250920832157135\n",
      "\n",
      "batch: 1/1 in epoch 74/1000 \n",
      "... loss: 0.2509160041809082\n",
      "\n",
      "batch: 1/1 in epoch 75/1000 \n",
      "... loss: 0.2509111762046814\n",
      "\n",
      "batch: 1/1 in epoch 76/1000 \n",
      "... loss: 0.25090643763542175\n",
      "\n",
      "batch: 1/1 in epoch 77/1000 \n",
      "... loss: 0.2509016990661621\n",
      "\n",
      "batch: 1/1 in epoch 78/1000 \n",
      "... loss: 0.25089702010154724\n",
      "\n",
      "batch: 1/1 in epoch 79/1000 \n",
      "... loss: 0.25089237093925476\n",
      "\n",
      "batch: 1/1 in epoch 80/1000 \n",
      "... loss: 0.25088775157928467\n",
      "\n",
      "batch: 1/1 in epoch 81/1000 \n",
      "... loss: 0.25088316202163696\n",
      "\n",
      "batch: 1/1 in epoch 82/1000 \n",
      "... loss: 0.25087863206863403\n",
      "\n",
      "batch: 1/1 in epoch 83/1000 \n",
      "... loss: 0.2508741319179535\n",
      "\n",
      "batch: 1/1 in epoch 84/1000 \n",
      "... loss: 0.25086966156959534\n",
      "\n",
      "batch: 1/1 in epoch 85/1000 \n",
      "... loss: 0.2508651912212372\n",
      "\n",
      "batch: 1/1 in epoch 86/1000 \n",
      "... loss: 0.2508608102798462\n",
      "\n",
      "batch: 1/1 in epoch 87/1000 \n",
      "... loss: 0.2508564293384552\n",
      "\n",
      "batch: 1/1 in epoch 88/1000 \n",
      "... loss: 0.250852108001709\n",
      "\n",
      "batch: 1/1 in epoch 89/1000 \n",
      "... loss: 0.25084778666496277\n",
      "\n",
      "batch: 1/1 in epoch 90/1000 \n",
      "... loss: 0.25084352493286133\n",
      "\n",
      "batch: 1/1 in epoch 91/1000 \n",
      "... loss: 0.2508392930030823\n",
      "\n",
      "batch: 1/1 in epoch 92/1000 \n",
      "... loss: 0.2508350908756256\n",
      "\n",
      "batch: 1/1 in epoch 93/1000 \n",
      "... loss: 0.25083091855049133\n",
      "\n",
      "batch: 1/1 in epoch 94/1000 \n",
      "... loss: 0.25082677602767944\n",
      "\n",
      "batch: 1/1 in epoch 95/1000 \n",
      "... loss: 0.25082266330718994\n",
      "\n",
      "batch: 1/1 in epoch 96/1000 \n",
      "... loss: 0.2508186101913452\n",
      "\n",
      "batch: 1/1 in epoch 97/1000 \n",
      "... loss: 0.2508145570755005\n",
      "\n",
      "batch: 1/1 in epoch 98/1000 \n",
      "... loss: 0.25081053376197815\n",
      "\n",
      "batch: 1/1 in epoch 99/1000 \n",
      "... loss: 0.2508065700531006\n",
      "\n",
      "batch: 1/1 in epoch 100/1000 \n",
      "... loss: 0.250802606344223\n",
      "\n",
      "batch: 1/1 in epoch 101/1000 \n",
      "... loss: 0.25079867243766785\n",
      "\n",
      "batch: 1/1 in epoch 102/1000 \n",
      "... loss: 0.25079476833343506\n",
      "\n",
      "batch: 1/1 in epoch 103/1000 \n",
      "... loss: 0.25079092383384705\n",
      "\n",
      "batch: 1/1 in epoch 104/1000 \n",
      "... loss: 0.25078707933425903\n",
      "\n",
      "batch: 1/1 in epoch 105/1000 \n",
      "... loss: 0.2507832646369934\n",
      "\n",
      "batch: 1/1 in epoch 106/1000 \n",
      "... loss: 0.25077947974205017\n",
      "\n",
      "batch: 1/1 in epoch 107/1000 \n",
      "... loss: 0.2507757544517517\n",
      "\n",
      "batch: 1/1 in epoch 108/1000 \n",
      "... loss: 0.25077199935913086\n",
      "\n",
      "batch: 1/1 in epoch 109/1000 \n",
      "... loss: 0.2507683038711548\n",
      "\n",
      "batch: 1/1 in epoch 110/1000 \n",
      "... loss: 0.2507646381855011\n",
      "\n",
      "batch: 1/1 in epoch 111/1000 \n",
      "... loss: 0.2507610023021698\n",
      "\n",
      "batch: 1/1 in epoch 112/1000 \n",
      "... loss: 0.2507573664188385\n",
      "\n",
      "batch: 1/1 in epoch 113/1000 \n",
      "... loss: 0.250753790140152\n",
      "\n",
      "batch: 1/1 in epoch 114/1000 \n",
      "... loss: 0.25075021386146545\n",
      "\n",
      "batch: 1/1 in epoch 115/1000 \n",
      "... loss: 0.2507466673851013\n",
      "\n",
      "batch: 1/1 in epoch 116/1000 \n",
      "... loss: 0.25074315071105957\n",
      "\n",
      "batch: 1/1 in epoch 117/1000 \n",
      "... loss: 0.2507396638393402\n",
      "\n",
      "batch: 1/1 in epoch 118/1000 \n",
      "... loss: 0.25073620676994324\n",
      "\n",
      "batch: 1/1 in epoch 119/1000 \n",
      "... loss: 0.25073277950286865\n",
      "\n",
      "batch: 1/1 in epoch 120/1000 \n",
      "... loss: 0.25072935223579407\n",
      "\n",
      "batch: 1/1 in epoch 121/1000 \n",
      "... loss: 0.25072595477104187\n",
      "\n",
      "batch: 1/1 in epoch 122/1000 \n",
      "... loss: 0.25072258710861206\n",
      "\n",
      "batch: 1/1 in epoch 123/1000 \n",
      "... loss: 0.25071924924850464\n",
      "\n",
      "batch: 1/1 in epoch 124/1000 \n",
      "... loss: 0.2507159411907196\n",
      "\n",
      "batch: 1/1 in epoch 125/1000 \n",
      "... loss: 0.25071263313293457\n",
      "\n",
      "batch: 1/1 in epoch 126/1000 \n",
      "... loss: 0.2507093548774719\n",
      "\n",
      "batch: 1/1 in epoch 127/1000 \n",
      "... loss: 0.25070610642433167\n",
      "\n",
      "batch: 1/1 in epoch 128/1000 \n",
      "... loss: 0.2507028877735138\n",
      "\n",
      "batch: 1/1 in epoch 129/1000 \n",
      "... loss: 0.2506996691226959\n",
      "\n",
      "batch: 1/1 in epoch 130/1000 \n",
      "... loss: 0.2506965100765228\n",
      "\n",
      "batch: 1/1 in epoch 131/1000 \n",
      "... loss: 0.25069332122802734\n",
      "\n",
      "batch: 1/1 in epoch 132/1000 \n",
      "... loss: 0.25069019198417664\n",
      "\n",
      "batch: 1/1 in epoch 133/1000 \n",
      "... loss: 0.2506870627403259\n",
      "\n",
      "batch: 1/1 in epoch 134/1000 \n",
      "... loss: 0.25068399310112\n",
      "\n",
      "batch: 1/1 in epoch 135/1000 \n",
      "... loss: 0.25068092346191406\n",
      "\n",
      "batch: 1/1 in epoch 136/1000 \n",
      "... loss: 0.25067785382270813\n",
      "\n",
      "batch: 1/1 in epoch 137/1000 \n",
      "... loss: 0.250674843788147\n",
      "\n",
      "batch: 1/1 in epoch 138/1000 \n",
      "... loss: 0.2506718337535858\n",
      "\n",
      "batch: 1/1 in epoch 139/1000 \n",
      "... loss: 0.25066882371902466\n",
      "\n",
      "batch: 1/1 in epoch 140/1000 \n",
      "... loss: 0.2506658732891083\n",
      "\n",
      "batch: 1/1 in epoch 141/1000 \n",
      "... loss: 0.2506629228591919\n",
      "\n",
      "batch: 1/1 in epoch 142/1000 \n",
      "... loss: 0.2506599724292755\n",
      "\n",
      "batch: 1/1 in epoch 143/1000 \n",
      "... loss: 0.2506570816040039\n",
      "\n",
      "batch: 1/1 in epoch 144/1000 \n",
      "... loss: 0.2506541907787323\n",
      "\n",
      "batch: 1/1 in epoch 145/1000 \n",
      "... loss: 0.2506512999534607\n",
      "\n",
      "batch: 1/1 in epoch 146/1000 \n",
      "... loss: 0.25064846873283386\n",
      "\n",
      "batch: 1/1 in epoch 147/1000 \n",
      "... loss: 0.25064563751220703\n",
      "\n",
      "batch: 1/1 in epoch 148/1000 \n",
      "... loss: 0.2506428062915802\n",
      "\n",
      "batch: 1/1 in epoch 149/1000 \n",
      "... loss: 0.25064000487327576\n",
      "\n",
      "batch: 1/1 in epoch 150/1000 \n",
      "... loss: 0.2506372332572937\n",
      "\n",
      "batch: 1/1 in epoch 151/1000 \n",
      "... loss: 0.25063449144363403\n",
      "\n",
      "batch: 1/1 in epoch 152/1000 \n",
      "... loss: 0.25063174962997437\n",
      "\n",
      "batch: 1/1 in epoch 153/1000 \n",
      "... loss: 0.2506290376186371\n",
      "\n",
      "batch: 1/1 in epoch 154/1000 \n",
      "... loss: 0.2506263256072998\n",
      "\n",
      "batch: 1/1 in epoch 155/1000 \n",
      "... loss: 0.2506236433982849\n",
      "\n",
      "batch: 1/1 in epoch 156/1000 \n",
      "... loss: 0.25062096118927\n",
      "\n",
      "batch: 1/1 in epoch 157/1000 \n",
      "... loss: 0.2506183087825775\n",
      "\n",
      "batch: 1/1 in epoch 158/1000 \n",
      "... loss: 0.2506156861782074\n",
      "\n",
      "batch: 1/1 in epoch 159/1000 \n",
      "... loss: 0.2506130635738373\n",
      "\n",
      "batch: 1/1 in epoch 160/1000 \n",
      "... loss: 0.25061047077178955\n",
      "\n",
      "batch: 1/1 in epoch 161/1000 \n",
      "... loss: 0.2506078779697418\n",
      "\n",
      "batch: 1/1 in epoch 162/1000 \n",
      "... loss: 0.2506053149700165\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 163/1000 \n",
      "... loss: 0.25060275197029114\n",
      "\n",
      "batch: 1/1 in epoch 164/1000 \n",
      "... loss: 0.2506002187728882\n",
      "\n",
      "batch: 1/1 in epoch 165/1000 \n",
      "... loss: 0.2505977153778076\n",
      "\n",
      "batch: 1/1 in epoch 166/1000 \n",
      "... loss: 0.25059521198272705\n",
      "\n",
      "batch: 1/1 in epoch 167/1000 \n",
      "... loss: 0.25059273838996887\n",
      "\n",
      "batch: 1/1 in epoch 168/1000 \n",
      "... loss: 0.2505902647972107\n",
      "\n",
      "batch: 1/1 in epoch 169/1000 \n",
      "... loss: 0.2505878210067749\n",
      "\n",
      "batch: 1/1 in epoch 170/1000 \n",
      "... loss: 0.2505853772163391\n",
      "\n",
      "batch: 1/1 in epoch 171/1000 \n",
      "... loss: 0.2505829334259033\n",
      "\n",
      "batch: 1/1 in epoch 172/1000 \n",
      "... loss: 0.2505805492401123\n",
      "\n",
      "batch: 1/1 in epoch 173/1000 \n",
      "... loss: 0.2505781352519989\n",
      "\n",
      "batch: 1/1 in epoch 174/1000 \n",
      "... loss: 0.2505757808685303\n",
      "\n",
      "batch: 1/1 in epoch 175/1000 \n",
      "... loss: 0.25057339668273926\n",
      "\n",
      "batch: 1/1 in epoch 176/1000 \n",
      "... loss: 0.250571072101593\n",
      "\n",
      "batch: 1/1 in epoch 177/1000 \n",
      "... loss: 0.2505687177181244\n",
      "\n",
      "batch: 1/1 in epoch 178/1000 \n",
      "... loss: 0.25056639313697815\n",
      "\n",
      "batch: 1/1 in epoch 179/1000 \n",
      "... loss: 0.2505641281604767\n",
      "\n",
      "batch: 1/1 in epoch 180/1000 \n",
      "... loss: 0.25056180357933044\n",
      "\n",
      "batch: 1/1 in epoch 181/1000 \n",
      "... loss: 0.250559538602829\n",
      "\n",
      "batch: 1/1 in epoch 182/1000 \n",
      "... loss: 0.2505572736263275\n",
      "\n",
      "batch: 1/1 in epoch 183/1000 \n",
      "... loss: 0.25055503845214844\n",
      "\n",
      "batch: 1/1 in epoch 184/1000 \n",
      "... loss: 0.25055280327796936\n",
      "\n",
      "batch: 1/1 in epoch 185/1000 \n",
      "... loss: 0.2505505681037903\n",
      "\n",
      "batch: 1/1 in epoch 186/1000 \n",
      "... loss: 0.2505483627319336\n",
      "\n",
      "batch: 1/1 in epoch 187/1000 \n",
      "... loss: 0.2505461871623993\n",
      "\n",
      "batch: 1/1 in epoch 188/1000 \n",
      "... loss: 0.250544011592865\n",
      "\n",
      "batch: 1/1 in epoch 189/1000 \n",
      "... loss: 0.2505418360233307\n",
      "\n",
      "batch: 1/1 in epoch 190/1000 \n",
      "... loss: 0.2505396902561188\n",
      "\n",
      "batch: 1/1 in epoch 191/1000 \n",
      "... loss: 0.25053754448890686\n",
      "\n",
      "batch: 1/1 in epoch 192/1000 \n",
      "... loss: 0.25053542852401733\n",
      "\n",
      "batch: 1/1 in epoch 193/1000 \n",
      "... loss: 0.2505333125591278\n",
      "\n",
      "batch: 1/1 in epoch 194/1000 \n",
      "... loss: 0.2505311965942383\n",
      "\n",
      "batch: 1/1 in epoch 195/1000 \n",
      "... loss: 0.25052911043167114\n",
      "\n",
      "batch: 1/1 in epoch 196/1000 \n",
      "... loss: 0.250527024269104\n",
      "\n",
      "batch: 1/1 in epoch 197/1000 \n",
      "... loss: 0.25052496790885925\n",
      "\n",
      "batch: 1/1 in epoch 198/1000 \n",
      "... loss: 0.2505229115486145\n",
      "\n",
      "batch: 1/1 in epoch 199/1000 \n",
      "... loss: 0.25052088499069214\n",
      "\n",
      "batch: 1/1 in epoch 200/1000 \n",
      "... loss: 0.2505188286304474\n",
      "\n",
      "batch: 1/1 in epoch 201/1000 \n",
      "... loss: 0.2505168318748474\n",
      "\n",
      "batch: 1/1 in epoch 202/1000 \n",
      "... loss: 0.25051483511924744\n",
      "\n",
      "batch: 1/1 in epoch 203/1000 \n",
      "... loss: 0.25051283836364746\n",
      "\n",
      "batch: 1/1 in epoch 204/1000 \n",
      "... loss: 0.2505108714103699\n",
      "\n",
      "batch: 1/1 in epoch 205/1000 \n",
      "... loss: 0.2505089044570923\n",
      "\n",
      "batch: 1/1 in epoch 206/1000 \n",
      "... loss: 0.2505069375038147\n",
      "\n",
      "batch: 1/1 in epoch 207/1000 \n",
      "... loss: 0.2505050003528595\n",
      "\n",
      "batch: 1/1 in epoch 208/1000 \n",
      "... loss: 0.2505030632019043\n",
      "\n",
      "batch: 1/1 in epoch 209/1000 \n",
      "... loss: 0.2505011260509491\n",
      "\n",
      "batch: 1/1 in epoch 210/1000 \n",
      "... loss: 0.2504992187023163\n",
      "\n",
      "batch: 1/1 in epoch 211/1000 \n",
      "... loss: 0.25049731135368347\n",
      "\n",
      "batch: 1/1 in epoch 212/1000 \n",
      "... loss: 0.25049543380737305\n",
      "\n",
      "batch: 1/1 in epoch 213/1000 \n",
      "... loss: 0.25049352645874023\n",
      "\n",
      "batch: 1/1 in epoch 214/1000 \n",
      "... loss: 0.2504916787147522\n",
      "\n",
      "batch: 1/1 in epoch 215/1000 \n",
      "... loss: 0.25048983097076416\n",
      "\n",
      "batch: 1/1 in epoch 216/1000 \n",
      "... loss: 0.25048795342445374\n",
      "\n",
      "batch: 1/1 in epoch 217/1000 \n",
      "... loss: 0.2504861354827881\n",
      "\n",
      "batch: 1/1 in epoch 218/1000 \n",
      "... loss: 0.25048431754112244\n",
      "\n",
      "batch: 1/1 in epoch 219/1000 \n",
      "... loss: 0.2504824995994568\n",
      "\n",
      "batch: 1/1 in epoch 220/1000 \n",
      "... loss: 0.25048068165779114\n",
      "\n",
      "batch: 1/1 in epoch 221/1000 \n",
      "... loss: 0.2504788935184479\n",
      "\n",
      "batch: 1/1 in epoch 222/1000 \n",
      "... loss: 0.2504771053791046\n",
      "\n",
      "batch: 1/1 in epoch 223/1000 \n",
      "... loss: 0.25047534704208374\n",
      "\n",
      "batch: 1/1 in epoch 224/1000 \n",
      "... loss: 0.2504735589027405\n",
      "\n",
      "batch: 1/1 in epoch 225/1000 \n",
      "... loss: 0.2504718005657196\n",
      "\n",
      "batch: 1/1 in epoch 226/1000 \n",
      "... loss: 0.2504700720310211\n",
      "\n",
      "batch: 1/1 in epoch 227/1000 \n",
      "... loss: 0.25046834349632263\n",
      "\n",
      "batch: 1/1 in epoch 228/1000 \n",
      "... loss: 0.25046661496162415\n",
      "\n",
      "batch: 1/1 in epoch 229/1000 \n",
      "... loss: 0.25046488642692566\n",
      "\n",
      "batch: 1/1 in epoch 230/1000 \n",
      "... loss: 0.25046318769454956\n",
      "\n",
      "batch: 1/1 in epoch 231/1000 \n",
      "... loss: 0.25046148896217346\n",
      "\n",
      "batch: 1/1 in epoch 232/1000 \n",
      "... loss: 0.25045979022979736\n",
      "\n",
      "batch: 1/1 in epoch 233/1000 \n",
      "... loss: 0.25045812129974365\n",
      "\n",
      "batch: 1/1 in epoch 234/1000 \n",
      "... loss: 0.25045645236968994\n",
      "\n",
      "batch: 1/1 in epoch 235/1000 \n",
      "... loss: 0.25045478343963623\n",
      "\n",
      "batch: 1/1 in epoch 236/1000 \n",
      "... loss: 0.2504531443119049\n",
      "\n",
      "batch: 1/1 in epoch 237/1000 \n",
      "... loss: 0.2504515051841736\n",
      "\n",
      "batch: 1/1 in epoch 238/1000 \n",
      "... loss: 0.25044986605644226\n",
      "\n",
      "batch: 1/1 in epoch 239/1000 \n",
      "... loss: 0.25044822692871094\n",
      "\n",
      "batch: 1/1 in epoch 240/1000 \n",
      "... loss: 0.250446617603302\n",
      "\n",
      "batch: 1/1 in epoch 241/1000 \n",
      "... loss: 0.25044500827789307\n",
      "\n",
      "batch: 1/1 in epoch 242/1000 \n",
      "... loss: 0.2504434287548065\n",
      "\n",
      "batch: 1/1 in epoch 243/1000 \n",
      "... loss: 0.2504418194293976\n",
      "\n",
      "batch: 1/1 in epoch 244/1000 \n",
      "... loss: 0.25044023990631104\n",
      "\n",
      "batch: 1/1 in epoch 245/1000 \n",
      "... loss: 0.2504386603832245\n",
      "\n",
      "batch: 1/1 in epoch 246/1000 \n",
      "... loss: 0.2504371106624603\n",
      "\n",
      "batch: 1/1 in epoch 247/1000 \n",
      "... loss: 0.25043556094169617\n",
      "\n",
      "batch: 1/1 in epoch 248/1000 \n",
      "... loss: 0.250434011220932\n",
      "\n",
      "batch: 1/1 in epoch 249/1000 \n",
      "... loss: 0.25043246150016785\n",
      "\n",
      "batch: 1/1 in epoch 250/1000 \n",
      "... loss: 0.2504309415817261\n",
      "\n",
      "batch: 1/1 in epoch 251/1000 \n",
      "... loss: 0.2504294216632843\n",
      "\n",
      "batch: 1/1 in epoch 252/1000 \n",
      "... loss: 0.25042790174484253\n",
      "\n",
      "batch: 1/1 in epoch 253/1000 \n",
      "... loss: 0.25042638182640076\n",
      "\n",
      "batch: 1/1 in epoch 254/1000 \n",
      "... loss: 0.25042489171028137\n",
      "\n",
      "batch: 1/1 in epoch 255/1000 \n",
      "... loss: 0.250423401594162\n",
      "\n",
      "batch: 1/1 in epoch 256/1000 \n",
      "... loss: 0.250421941280365\n",
      "\n",
      "batch: 1/1 in epoch 257/1000 \n",
      "... loss: 0.2504204511642456\n",
      "\n",
      "batch: 1/1 in epoch 258/1000 \n",
      "... loss: 0.2504189908504486\n",
      "\n",
      "batch: 1/1 in epoch 259/1000 \n",
      "... loss: 0.2504175305366516\n",
      "\n",
      "batch: 1/1 in epoch 260/1000 \n",
      "... loss: 0.2504160702228546\n",
      "\n",
      "batch: 1/1 in epoch 261/1000 \n",
      "... loss: 0.2504146099090576\n",
      "\n",
      "batch: 1/1 in epoch 262/1000 \n",
      "... loss: 0.250413179397583\n",
      "\n",
      "batch: 1/1 in epoch 263/1000 \n",
      "... loss: 0.2504117488861084\n",
      "\n",
      "batch: 1/1 in epoch 264/1000 \n",
      "... loss: 0.2504103183746338\n",
      "\n",
      "batch: 1/1 in epoch 265/1000 \n",
      "... loss: 0.25040891766548157\n",
      "\n",
      "batch: 1/1 in epoch 266/1000 \n",
      "... loss: 0.25040751695632935\n",
      "\n",
      "batch: 1/1 in epoch 267/1000 \n",
      "... loss: 0.2504061162471771\n",
      "\n",
      "batch: 1/1 in epoch 268/1000 \n",
      "... loss: 0.2504047155380249\n",
      "\n",
      "batch: 1/1 in epoch 269/1000 \n",
      "... loss: 0.2504033148288727\n",
      "\n",
      "batch: 1/1 in epoch 270/1000 \n",
      "... loss: 0.25040194392204285\n",
      "\n",
      "batch: 1/1 in epoch 271/1000 \n",
      "... loss: 0.250400573015213\n",
      "\n",
      "batch: 1/1 in epoch 272/1000 \n",
      "... loss: 0.2503992021083832\n",
      "\n",
      "batch: 1/1 in epoch 273/1000 \n",
      "... loss: 0.25039783120155334\n",
      "\n",
      "batch: 1/1 in epoch 274/1000 \n",
      "... loss: 0.2503964900970459\n",
      "\n",
      "batch: 1/1 in epoch 275/1000 \n",
      "... loss: 0.25039514899253845\n",
      "\n",
      "batch: 1/1 in epoch 276/1000 \n",
      "... loss: 0.250393807888031\n",
      "\n",
      "batch: 1/1 in epoch 277/1000 \n",
      "... loss: 0.25039249658584595\n",
      "\n",
      "batch: 1/1 in epoch 278/1000 \n",
      "... loss: 0.2503911554813385\n",
      "\n",
      "batch: 1/1 in epoch 279/1000 \n",
      "... loss: 0.25038984417915344\n",
      "\n",
      "batch: 1/1 in epoch 280/1000 \n",
      "... loss: 0.2503885328769684\n",
      "\n",
      "batch: 1/1 in epoch 281/1000 \n",
      "... loss: 0.2503872215747833\n",
      "\n",
      "batch: 1/1 in epoch 282/1000 \n",
      "... loss: 0.25038594007492065\n",
      "\n",
      "batch: 1/1 in epoch 283/1000 \n",
      "... loss: 0.2503846287727356\n",
      "\n",
      "batch: 1/1 in epoch 284/1000 \n",
      "... loss: 0.2503833472728729\n",
      "\n",
      "batch: 1/1 in epoch 285/1000 \n",
      "... loss: 0.25038206577301025\n",
      "\n",
      "batch: 1/1 in epoch 286/1000 \n",
      "... loss: 0.2503807842731476\n",
      "\n",
      "batch: 1/1 in epoch 287/1000 \n",
      "... loss: 0.2503795325756073\n",
      "\n",
      "batch: 1/1 in epoch 288/1000 \n",
      "... loss: 0.250378280878067\n",
      "\n",
      "batch: 1/1 in epoch 289/1000 \n",
      "... loss: 0.25037702918052673\n",
      "\n",
      "batch: 1/1 in epoch 290/1000 \n",
      "... loss: 0.25037577748298645\n",
      "\n",
      "batch: 1/1 in epoch 291/1000 \n",
      "... loss: 0.25037452578544617\n",
      "\n",
      "batch: 1/1 in epoch 292/1000 \n",
      "... loss: 0.25037330389022827\n",
      "\n",
      "batch: 1/1 in epoch 293/1000 \n",
      "... loss: 0.250372052192688\n",
      "\n",
      "batch: 1/1 in epoch 294/1000 \n",
      "... loss: 0.2503708302974701\n",
      "\n",
      "batch: 1/1 in epoch 295/1000 \n",
      "... loss: 0.2503696084022522\n",
      "\n",
      "batch: 1/1 in epoch 296/1000 \n",
      "... loss: 0.2503684163093567\n",
      "\n",
      "batch: 1/1 in epoch 297/1000 \n",
      "... loss: 0.2503671944141388\n",
      "\n",
      "batch: 1/1 in epoch 298/1000 \n",
      "... loss: 0.2503660023212433\n",
      "\n",
      "batch: 1/1 in epoch 299/1000 \n",
      "... loss: 0.2503648102283478\n",
      "\n",
      "batch: 1/1 in epoch 300/1000 \n",
      "... loss: 0.25036361813545227\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 301/1000 \n",
      "... loss: 0.25036242604255676\n",
      "\n",
      "batch: 1/1 in epoch 302/1000 \n",
      "... loss: 0.25036126375198364\n",
      "\n",
      "batch: 1/1 in epoch 303/1000 \n",
      "... loss: 0.25036007165908813\n",
      "\n",
      "batch: 1/1 in epoch 304/1000 \n",
      "... loss: 0.250358909368515\n",
      "\n",
      "batch: 1/1 in epoch 305/1000 \n",
      "... loss: 0.2503577470779419\n",
      "\n",
      "batch: 1/1 in epoch 306/1000 \n",
      "... loss: 0.25035661458969116\n",
      "\n",
      "batch: 1/1 in epoch 307/1000 \n",
      "... loss: 0.25035545229911804\n",
      "\n",
      "batch: 1/1 in epoch 308/1000 \n",
      "... loss: 0.2503543198108673\n",
      "\n",
      "batch: 1/1 in epoch 309/1000 \n",
      "... loss: 0.2503531873226166\n",
      "\n",
      "batch: 1/1 in epoch 310/1000 \n",
      "... loss: 0.25035202503204346\n",
      "\n",
      "batch: 1/1 in epoch 311/1000 \n",
      "... loss: 0.2503509223461151\n",
      "\n",
      "batch: 1/1 in epoch 312/1000 \n",
      "... loss: 0.2503497898578644\n",
      "\n",
      "batch: 1/1 in epoch 313/1000 \n",
      "... loss: 0.25034865736961365\n",
      "\n",
      "batch: 1/1 in epoch 314/1000 \n",
      "... loss: 0.2503475546836853\n",
      "\n",
      "batch: 1/1 in epoch 315/1000 \n",
      "... loss: 0.25034645199775696\n",
      "\n",
      "batch: 1/1 in epoch 316/1000 \n",
      "... loss: 0.2503453493118286\n",
      "\n",
      "batch: 1/1 in epoch 317/1000 \n",
      "... loss: 0.25034424662590027\n",
      "\n",
      "batch: 1/1 in epoch 318/1000 \n",
      "... loss: 0.2503431737422943\n",
      "\n",
      "batch: 1/1 in epoch 319/1000 \n",
      "... loss: 0.25034207105636597\n",
      "\n",
      "batch: 1/1 in epoch 320/1000 \n",
      "... loss: 0.25034099817276\n",
      "\n",
      "batch: 1/1 in epoch 321/1000 \n",
      "... loss: 0.25033992528915405\n",
      "\n",
      "batch: 1/1 in epoch 322/1000 \n",
      "... loss: 0.2503388524055481\n",
      "\n",
      "batch: 1/1 in epoch 323/1000 \n",
      "... loss: 0.25033777952194214\n",
      "\n",
      "batch: 1/1 in epoch 324/1000 \n",
      "... loss: 0.2503367066383362\n",
      "\n",
      "batch: 1/1 in epoch 325/1000 \n",
      "... loss: 0.2503356635570526\n",
      "\n",
      "batch: 1/1 in epoch 326/1000 \n",
      "... loss: 0.25033462047576904\n",
      "\n",
      "batch: 1/1 in epoch 327/1000 \n",
      "... loss: 0.2503335475921631\n",
      "\n",
      "batch: 1/1 in epoch 328/1000 \n",
      "... loss: 0.2503325045108795\n",
      "\n",
      "batch: 1/1 in epoch 329/1000 \n",
      "... loss: 0.25033149123191833\n",
      "\n",
      "batch: 1/1 in epoch 330/1000 \n",
      "... loss: 0.25033044815063477\n",
      "\n",
      "batch: 1/1 in epoch 331/1000 \n",
      "... loss: 0.2503294050693512\n",
      "\n",
      "batch: 1/1 in epoch 332/1000 \n",
      "... loss: 0.25032839179039\n",
      "\n",
      "batch: 1/1 in epoch 333/1000 \n",
      "... loss: 0.25032737851142883\n",
      "\n",
      "batch: 1/1 in epoch 334/1000 \n",
      "... loss: 0.25032636523246765\n",
      "\n",
      "batch: 1/1 in epoch 335/1000 \n",
      "... loss: 0.25032535195350647\n",
      "\n",
      "batch: 1/1 in epoch 336/1000 \n",
      "... loss: 0.2503243684768677\n",
      "\n",
      "batch: 1/1 in epoch 337/1000 \n",
      "... loss: 0.2503233551979065\n",
      "\n",
      "batch: 1/1 in epoch 338/1000 \n",
      "... loss: 0.2503223419189453\n",
      "\n",
      "batch: 1/1 in epoch 339/1000 \n",
      "... loss: 0.2503213584423065\n",
      "\n",
      "batch: 1/1 in epoch 340/1000 \n",
      "... loss: 0.2503203749656677\n",
      "\n",
      "batch: 1/1 in epoch 341/1000 \n",
      "... loss: 0.25031939148902893\n",
      "\n",
      "batch: 1/1 in epoch 342/1000 \n",
      "... loss: 0.25031840801239014\n",
      "\n",
      "batch: 1/1 in epoch 343/1000 \n",
      "... loss: 0.25031742453575134\n",
      "\n",
      "batch: 1/1 in epoch 344/1000 \n",
      "... loss: 0.25031647086143494\n",
      "\n",
      "batch: 1/1 in epoch 345/1000 \n",
      "... loss: 0.25031548738479614\n",
      "\n",
      "batch: 1/1 in epoch 346/1000 \n",
      "... loss: 0.25031453371047974\n",
      "\n",
      "batch: 1/1 in epoch 347/1000 \n",
      "... loss: 0.25031358003616333\n",
      "\n",
      "batch: 1/1 in epoch 348/1000 \n",
      "... loss: 0.2503126263618469\n",
      "\n",
      "batch: 1/1 in epoch 349/1000 \n",
      "... loss: 0.2503116726875305\n",
      "\n",
      "batch: 1/1 in epoch 350/1000 \n",
      "... loss: 0.2503107488155365\n",
      "\n",
      "batch: 1/1 in epoch 351/1000 \n",
      "... loss: 0.2503097951412201\n",
      "\n",
      "batch: 1/1 in epoch 352/1000 \n",
      "... loss: 0.2503088414669037\n",
      "\n",
      "batch: 1/1 in epoch 353/1000 \n",
      "... loss: 0.25030791759490967\n",
      "\n",
      "batch: 1/1 in epoch 354/1000 \n",
      "... loss: 0.25030699372291565\n",
      "\n",
      "batch: 1/1 in epoch 355/1000 \n",
      "... loss: 0.25030606985092163\n",
      "\n",
      "batch: 1/1 in epoch 356/1000 \n",
      "... loss: 0.2503051459789276\n",
      "\n",
      "batch: 1/1 in epoch 357/1000 \n",
      "... loss: 0.2503042221069336\n",
      "\n",
      "batch: 1/1 in epoch 358/1000 \n",
      "... loss: 0.25030332803726196\n",
      "\n",
      "batch: 1/1 in epoch 359/1000 \n",
      "... loss: 0.25030240416526794\n",
      "\n",
      "batch: 1/1 in epoch 360/1000 \n",
      "... loss: 0.2503015100955963\n",
      "\n",
      "batch: 1/1 in epoch 361/1000 \n",
      "... loss: 0.2503006160259247\n",
      "\n",
      "batch: 1/1 in epoch 362/1000 \n",
      "... loss: 0.25029972195625305\n",
      "\n",
      "batch: 1/1 in epoch 363/1000 \n",
      "... loss: 0.2502988278865814\n",
      "\n",
      "batch: 1/1 in epoch 364/1000 \n",
      "... loss: 0.2502979338169098\n",
      "\n",
      "batch: 1/1 in epoch 365/1000 \n",
      "... loss: 0.25029703974723816\n",
      "\n",
      "batch: 1/1 in epoch 366/1000 \n",
      "... loss: 0.2502961754798889\n",
      "\n",
      "batch: 1/1 in epoch 367/1000 \n",
      "... loss: 0.2502952814102173\n",
      "\n",
      "batch: 1/1 in epoch 368/1000 \n",
      "... loss: 0.25029441714286804\n",
      "\n",
      "batch: 1/1 in epoch 369/1000 \n",
      "... loss: 0.2502935528755188\n",
      "\n",
      "batch: 1/1 in epoch 370/1000 \n",
      "... loss: 0.25029268860816956\n",
      "\n",
      "batch: 1/1 in epoch 371/1000 \n",
      "... loss: 0.2502918243408203\n",
      "\n",
      "batch: 1/1 in epoch 372/1000 \n",
      "... loss: 0.25029096007347107\n",
      "\n",
      "batch: 1/1 in epoch 373/1000 \n",
      "... loss: 0.2502900958061218\n",
      "\n",
      "batch: 1/1 in epoch 374/1000 \n",
      "... loss: 0.25028926134109497\n",
      "\n",
      "batch: 1/1 in epoch 375/1000 \n",
      "... loss: 0.2502883970737457\n",
      "\n",
      "batch: 1/1 in epoch 376/1000 \n",
      "... loss: 0.25028756260871887\n",
      "\n",
      "batch: 1/1 in epoch 377/1000 \n",
      "... loss: 0.250286728143692\n",
      "\n",
      "batch: 1/1 in epoch 378/1000 \n",
      "... loss: 0.2502858638763428\n",
      "\n",
      "batch: 1/1 in epoch 379/1000 \n",
      "... loss: 0.2502850592136383\n",
      "\n",
      "batch: 1/1 in epoch 380/1000 \n",
      "... loss: 0.25028419494628906\n",
      "\n",
      "batch: 1/1 in epoch 381/1000 \n",
      "... loss: 0.2502833902835846\n",
      "\n",
      "batch: 1/1 in epoch 382/1000 \n",
      "... loss: 0.25028255581855774\n",
      "\n",
      "batch: 1/1 in epoch 383/1000 \n",
      "... loss: 0.25028175115585327\n",
      "\n",
      "batch: 1/1 in epoch 384/1000 \n",
      "... loss: 0.2502809166908264\n",
      "\n",
      "batch: 1/1 in epoch 385/1000 \n",
      "... loss: 0.25028011202812195\n",
      "\n",
      "batch: 1/1 in epoch 386/1000 \n",
      "... loss: 0.2502793073654175\n",
      "\n",
      "batch: 1/1 in epoch 387/1000 \n",
      "... loss: 0.250278502702713\n",
      "\n",
      "batch: 1/1 in epoch 388/1000 \n",
      "... loss: 0.25027769804000854\n",
      "\n",
      "batch: 1/1 in epoch 389/1000 \n",
      "... loss: 0.2502768933773041\n",
      "\n",
      "batch: 1/1 in epoch 390/1000 \n",
      "... loss: 0.2502760887145996\n",
      "\n",
      "batch: 1/1 in epoch 391/1000 \n",
      "... loss: 0.25027531385421753\n",
      "\n",
      "batch: 1/1 in epoch 392/1000 \n",
      "... loss: 0.25027450919151306\n",
      "\n",
      "batch: 1/1 in epoch 393/1000 \n",
      "... loss: 0.250273734331131\n",
      "\n",
      "batch: 1/1 in epoch 394/1000 \n",
      "... loss: 0.2502729296684265\n",
      "\n",
      "batch: 1/1 in epoch 395/1000 \n",
      "... loss: 0.25027215480804443\n",
      "\n",
      "batch: 1/1 in epoch 396/1000 \n",
      "... loss: 0.25027137994766235\n",
      "\n",
      "batch: 1/1 in epoch 397/1000 \n",
      "... loss: 0.2502706050872803\n",
      "\n",
      "batch: 1/1 in epoch 398/1000 \n",
      "... loss: 0.2502698302268982\n",
      "\n",
      "batch: 1/1 in epoch 399/1000 \n",
      "... loss: 0.2502690553665161\n",
      "\n",
      "batch: 1/1 in epoch 400/1000 \n",
      "... loss: 0.2502683103084564\n",
      "\n",
      "batch: 1/1 in epoch 401/1000 \n",
      "... loss: 0.25026753544807434\n",
      "\n",
      "batch: 1/1 in epoch 402/1000 \n",
      "... loss: 0.25026679039001465\n",
      "\n",
      "batch: 1/1 in epoch 403/1000 \n",
      "... loss: 0.25026601552963257\n",
      "\n",
      "batch: 1/1 in epoch 404/1000 \n",
      "... loss: 0.2502652704715729\n",
      "\n",
      "batch: 1/1 in epoch 405/1000 \n",
      "... loss: 0.2502645254135132\n",
      "\n",
      "batch: 1/1 in epoch 406/1000 \n",
      "... loss: 0.2502637803554535\n",
      "\n",
      "batch: 1/1 in epoch 407/1000 \n",
      "... loss: 0.2502630352973938\n",
      "\n",
      "batch: 1/1 in epoch 408/1000 \n",
      "... loss: 0.2502622902393341\n",
      "\n",
      "batch: 1/1 in epoch 409/1000 \n",
      "... loss: 0.2502615451812744\n",
      "\n",
      "batch: 1/1 in epoch 410/1000 \n",
      "... loss: 0.2502608299255371\n",
      "\n",
      "batch: 1/1 in epoch 411/1000 \n",
      "... loss: 0.2502600848674774\n",
      "\n",
      "batch: 1/1 in epoch 412/1000 \n",
      "... loss: 0.2502593398094177\n",
      "\n",
      "batch: 1/1 in epoch 413/1000 \n",
      "... loss: 0.2502586245536804\n",
      "\n",
      "batch: 1/1 in epoch 414/1000 \n",
      "... loss: 0.2502579092979431\n",
      "\n",
      "batch: 1/1 in epoch 415/1000 \n",
      "... loss: 0.2502571940422058\n",
      "\n",
      "batch: 1/1 in epoch 416/1000 \n",
      "... loss: 0.2502564787864685\n",
      "\n",
      "batch: 1/1 in epoch 417/1000 \n",
      "... loss: 0.2502557337284088\n",
      "\n",
      "batch: 1/1 in epoch 418/1000 \n",
      "... loss: 0.2502550482749939\n",
      "\n",
      "batch: 1/1 in epoch 419/1000 \n",
      "... loss: 0.2502543330192566\n",
      "\n",
      "batch: 1/1 in epoch 420/1000 \n",
      "... loss: 0.2502536177635193\n",
      "\n",
      "batch: 1/1 in epoch 421/1000 \n",
      "... loss: 0.25025293231010437\n",
      "\n",
      "batch: 1/1 in epoch 422/1000 \n",
      "... loss: 0.25025221705436707\n",
      "\n",
      "batch: 1/1 in epoch 423/1000 \n",
      "... loss: 0.25025153160095215\n",
      "\n",
      "batch: 1/1 in epoch 424/1000 \n",
      "... loss: 0.25025081634521484\n",
      "\n",
      "batch: 1/1 in epoch 425/1000 \n",
      "... loss: 0.2502501308917999\n",
      "\n",
      "batch: 1/1 in epoch 426/1000 \n",
      "... loss: 0.250249445438385\n",
      "\n",
      "batch: 1/1 in epoch 427/1000 \n",
      "... loss: 0.2502487599849701\n",
      "\n",
      "batch: 1/1 in epoch 428/1000 \n",
      "... loss: 0.2502480745315552\n",
      "\n",
      "batch: 1/1 in epoch 429/1000 \n",
      "... loss: 0.25024738907814026\n",
      "\n",
      "batch: 1/1 in epoch 430/1000 \n",
      "... loss: 0.25024670362472534\n",
      "\n",
      "batch: 1/1 in epoch 431/1000 \n",
      "... loss: 0.2502460181713104\n",
      "\n",
      "batch: 1/1 in epoch 432/1000 \n",
      "... loss: 0.2502453625202179\n",
      "\n",
      "batch: 1/1 in epoch 433/1000 \n",
      "... loss: 0.250244677066803\n",
      "\n",
      "batch: 1/1 in epoch 434/1000 \n",
      "... loss: 0.25024402141571045\n",
      "\n",
      "batch: 1/1 in epoch 435/1000 \n",
      "... loss: 0.25024333596229553\n",
      "\n",
      "batch: 1/1 in epoch 436/1000 \n",
      "... loss: 0.250242680311203\n",
      "\n",
      "batch: 1/1 in epoch 437/1000 \n",
      "... loss: 0.2502420246601105\n",
      "\n",
      "batch: 1/1 in epoch 438/1000 \n",
      "... loss: 0.25024136900901794\n",
      "\n",
      "batch: 1/1 in epoch 439/1000 \n",
      "... loss: 0.2502407133579254\n",
      "\n",
      "batch: 1/1 in epoch 440/1000 \n",
      "... loss: 0.2502400577068329\n",
      "\n",
      "batch: 1/1 in epoch 441/1000 \n",
      "... loss: 0.25023940205574036\n",
      "\n",
      "batch: 1/1 in epoch 442/1000 \n",
      "... loss: 0.2502387464046478\n",
      "\n",
      "batch: 1/1 in epoch 443/1000 \n",
      "... loss: 0.2502380907535553\n",
      "\n",
      "batch: 1/1 in epoch 444/1000 \n",
      "... loss: 0.25023746490478516\n",
      "\n",
      "batch: 1/1 in epoch 445/1000 \n",
      "... loss: 0.2502368092536926\n",
      "\n",
      "batch: 1/1 in epoch 446/1000 \n",
      "... loss: 0.2502361834049225\n",
      "\n",
      "batch: 1/1 in epoch 447/1000 \n",
      "... loss: 0.25023555755615234\n",
      "\n",
      "batch: 1/1 in epoch 448/1000 \n",
      "... loss: 0.2502349019050598\n",
      "\n",
      "batch: 1/1 in epoch 449/1000 \n",
      "... loss: 0.2502342760562897\n",
      "\n",
      "batch: 1/1 in epoch 450/1000 \n",
      "... loss: 0.25023365020751953\n",
      "\n",
      "batch: 1/1 in epoch 451/1000 \n",
      "... loss: 0.2502330243587494\n",
      "\n",
      "batch: 1/1 in epoch 452/1000 \n",
      "... loss: 0.25023239850997925\n",
      "\n",
      "batch: 1/1 in epoch 453/1000 \n",
      "... loss: 0.2502317726612091\n",
      "\n",
      "batch: 1/1 in epoch 454/1000 \n",
      "... loss: 0.25023117661476135\n",
      "\n",
      "batch: 1/1 in epoch 455/1000 \n",
      "... loss: 0.2502305209636688\n",
      "\n",
      "batch: 1/1 in epoch 456/1000 \n",
      "... loss: 0.25022992491722107\n",
      "\n",
      "batch: 1/1 in epoch 457/1000 \n",
      "... loss: 0.2502292990684509\n",
      "\n",
      "batch: 1/1 in epoch 458/1000 \n",
      "... loss: 0.2502287030220032\n",
      "\n",
      "batch: 1/1 in epoch 459/1000 \n",
      "... loss: 0.25022807717323303\n",
      "\n",
      "batch: 1/1 in epoch 460/1000 \n",
      "... loss: 0.2502274811267853\n",
      "\n",
      "batch: 1/1 in epoch 461/1000 \n",
      "... loss: 0.2502268850803375\n",
      "\n",
      "batch: 1/1 in epoch 462/1000 \n",
      "... loss: 0.2502262592315674\n",
      "\n",
      "batch: 1/1 in epoch 463/1000 \n",
      "... loss: 0.25022566318511963\n",
      "\n",
      "batch: 1/1 in epoch 464/1000 \n",
      "... loss: 0.2502250671386719\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 465/1000 \n",
      "... loss: 0.2502244710922241\n",
      "\n",
      "batch: 1/1 in epoch 466/1000 \n",
      "... loss: 0.25022387504577637\n",
      "\n",
      "batch: 1/1 in epoch 467/1000 \n",
      "... loss: 0.2502232789993286\n",
      "\n",
      "batch: 1/1 in epoch 468/1000 \n",
      "... loss: 0.25022271275520325\n",
      "\n",
      "batch: 1/1 in epoch 469/1000 \n",
      "... loss: 0.2502221167087555\n",
      "\n",
      "batch: 1/1 in epoch 470/1000 \n",
      "... loss: 0.25022152066230774\n",
      "\n",
      "batch: 1/1 in epoch 471/1000 \n",
      "... loss: 0.2502209544181824\n",
      "\n",
      "batch: 1/1 in epoch 472/1000 \n",
      "... loss: 0.2502203583717346\n",
      "\n",
      "batch: 1/1 in epoch 473/1000 \n",
      "... loss: 0.25021979212760925\n",
      "\n",
      "batch: 1/1 in epoch 474/1000 \n",
      "... loss: 0.2502192258834839\n",
      "\n",
      "batch: 1/1 in epoch 475/1000 \n",
      "... loss: 0.25021862983703613\n",
      "\n",
      "batch: 1/1 in epoch 476/1000 \n",
      "... loss: 0.25021806359291077\n",
      "\n",
      "batch: 1/1 in epoch 477/1000 \n",
      "... loss: 0.2502174973487854\n",
      "\n",
      "batch: 1/1 in epoch 478/1000 \n",
      "... loss: 0.25021693110466003\n",
      "\n",
      "batch: 1/1 in epoch 479/1000 \n",
      "... loss: 0.25021636486053467\n",
      "\n",
      "batch: 1/1 in epoch 480/1000 \n",
      "... loss: 0.2502157986164093\n",
      "\n",
      "batch: 1/1 in epoch 481/1000 \n",
      "... loss: 0.25021523237228394\n",
      "\n",
      "batch: 1/1 in epoch 482/1000 \n",
      "... loss: 0.25021466612815857\n",
      "\n",
      "batch: 1/1 in epoch 483/1000 \n",
      "... loss: 0.2502141296863556\n",
      "\n",
      "batch: 1/1 in epoch 484/1000 \n",
      "... loss: 0.2502135634422302\n",
      "\n",
      "batch: 1/1 in epoch 485/1000 \n",
      "... loss: 0.25021299719810486\n",
      "\n",
      "batch: 1/1 in epoch 486/1000 \n",
      "... loss: 0.2502124607563019\n",
      "\n",
      "batch: 1/1 in epoch 487/1000 \n",
      "... loss: 0.2502118945121765\n",
      "\n",
      "batch: 1/1 in epoch 488/1000 \n",
      "... loss: 0.25021135807037354\n",
      "\n",
      "batch: 1/1 in epoch 489/1000 \n",
      "... loss: 0.25021082162857056\n",
      "\n",
      "batch: 1/1 in epoch 490/1000 \n",
      "... loss: 0.2502102553844452\n",
      "\n",
      "batch: 1/1 in epoch 491/1000 \n",
      "... loss: 0.2502097189426422\n",
      "\n",
      "batch: 1/1 in epoch 492/1000 \n",
      "... loss: 0.25020918250083923\n",
      "\n",
      "batch: 1/1 in epoch 493/1000 \n",
      "... loss: 0.25020864605903625\n",
      "\n",
      "batch: 1/1 in epoch 494/1000 \n",
      "... loss: 0.2502081096172333\n",
      "\n",
      "batch: 1/1 in epoch 495/1000 \n",
      "... loss: 0.2502075731754303\n",
      "\n",
      "batch: 1/1 in epoch 496/1000 \n",
      "... loss: 0.2502070367336273\n",
      "\n",
      "batch: 1/1 in epoch 497/1000 \n",
      "... loss: 0.25020650029182434\n",
      "\n",
      "batch: 1/1 in epoch 498/1000 \n",
      "... loss: 0.25020599365234375\n",
      "\n",
      "batch: 1/1 in epoch 499/1000 \n",
      "... loss: 0.25020545721054077\n",
      "\n",
      "batch: 1/1 in epoch 500/1000 \n",
      "... loss: 0.2502049207687378\n",
      "\n",
      "batch: 1/1 in epoch 501/1000 \n",
      "... loss: 0.2502044141292572\n",
      "\n",
      "batch: 1/1 in epoch 502/1000 \n",
      "... loss: 0.2502038776874542\n",
      "\n",
      "batch: 1/1 in epoch 503/1000 \n",
      "... loss: 0.25020337104797363\n",
      "\n",
      "batch: 1/1 in epoch 504/1000 \n",
      "... loss: 0.25020283460617065\n",
      "\n",
      "batch: 1/1 in epoch 505/1000 \n",
      "... loss: 0.25020232796669006\n",
      "\n",
      "batch: 1/1 in epoch 506/1000 \n",
      "... loss: 0.2502018213272095\n",
      "\n",
      "batch: 1/1 in epoch 507/1000 \n",
      "... loss: 0.2502013146877289\n",
      "\n",
      "batch: 1/1 in epoch 508/1000 \n",
      "... loss: 0.2502007782459259\n",
      "\n",
      "batch: 1/1 in epoch 509/1000 \n",
      "... loss: 0.2502002716064453\n",
      "\n",
      "batch: 1/1 in epoch 510/1000 \n",
      "... loss: 0.2501997649669647\n",
      "\n",
      "batch: 1/1 in epoch 511/1000 \n",
      "... loss: 0.25019925832748413\n",
      "\n",
      "batch: 1/1 in epoch 512/1000 \n",
      "... loss: 0.2501987814903259\n",
      "\n",
      "batch: 1/1 in epoch 513/1000 \n",
      "... loss: 0.25019827485084534\n",
      "\n",
      "batch: 1/1 in epoch 514/1000 \n",
      "... loss: 0.25019776821136475\n",
      "\n",
      "batch: 1/1 in epoch 515/1000 \n",
      "... loss: 0.25019726157188416\n",
      "\n",
      "batch: 1/1 in epoch 516/1000 \n",
      "... loss: 0.25019675493240356\n",
      "\n",
      "batch: 1/1 in epoch 517/1000 \n",
      "... loss: 0.25019627809524536\n",
      "\n",
      "batch: 1/1 in epoch 518/1000 \n",
      "... loss: 0.25019577145576477\n",
      "\n",
      "batch: 1/1 in epoch 519/1000 \n",
      "... loss: 0.25019529461860657\n",
      "\n",
      "batch: 1/1 in epoch 520/1000 \n",
      "... loss: 0.250194787979126\n",
      "\n",
      "batch: 1/1 in epoch 521/1000 \n",
      "... loss: 0.2501943111419678\n",
      "\n",
      "batch: 1/1 in epoch 522/1000 \n",
      "... loss: 0.2501938045024872\n",
      "\n",
      "batch: 1/1 in epoch 523/1000 \n",
      "... loss: 0.250193327665329\n",
      "\n",
      "batch: 1/1 in epoch 524/1000 \n",
      "... loss: 0.2501928508281708\n",
      "\n",
      "batch: 1/1 in epoch 525/1000 \n",
      "... loss: 0.2501923739910126\n",
      "\n",
      "batch: 1/1 in epoch 526/1000 \n",
      "... loss: 0.25019189715385437\n",
      "\n",
      "batch: 1/1 in epoch 527/1000 \n",
      "... loss: 0.25019142031669617\n",
      "\n",
      "batch: 1/1 in epoch 528/1000 \n",
      "... loss: 0.25019094347953796\n",
      "\n",
      "batch: 1/1 in epoch 529/1000 \n",
      "... loss: 0.25019046664237976\n",
      "\n",
      "batch: 1/1 in epoch 530/1000 \n",
      "... loss: 0.25018998980522156\n",
      "\n",
      "batch: 1/1 in epoch 531/1000 \n",
      "... loss: 0.25018951296806335\n",
      "\n",
      "batch: 1/1 in epoch 532/1000 \n",
      "... loss: 0.25018903613090515\n",
      "\n",
      "batch: 1/1 in epoch 533/1000 \n",
      "... loss: 0.25018855929374695\n",
      "\n",
      "batch: 1/1 in epoch 534/1000 \n",
      "... loss: 0.25018811225891113\n",
      "\n",
      "batch: 1/1 in epoch 535/1000 \n",
      "... loss: 0.25018763542175293\n",
      "\n",
      "batch: 1/1 in epoch 536/1000 \n",
      "... loss: 0.2501871585845947\n",
      "\n",
      "batch: 1/1 in epoch 537/1000 \n",
      "... loss: 0.2501867115497589\n",
      "\n",
      "batch: 1/1 in epoch 538/1000 \n",
      "... loss: 0.2501862347126007\n",
      "\n",
      "batch: 1/1 in epoch 539/1000 \n",
      "... loss: 0.2501857876777649\n",
      "\n",
      "batch: 1/1 in epoch 540/1000 \n",
      "... loss: 0.2501853406429291\n",
      "\n",
      "batch: 1/1 in epoch 541/1000 \n",
      "... loss: 0.2501848638057709\n",
      "\n",
      "batch: 1/1 in epoch 542/1000 \n",
      "... loss: 0.25018441677093506\n",
      "\n",
      "batch: 1/1 in epoch 543/1000 \n",
      "... loss: 0.25018396973609924\n",
      "\n",
      "batch: 1/1 in epoch 544/1000 \n",
      "... loss: 0.2501835227012634\n",
      "\n",
      "batch: 1/1 in epoch 545/1000 \n",
      "... loss: 0.2501830458641052\n",
      "\n",
      "batch: 1/1 in epoch 546/1000 \n",
      "... loss: 0.2501826286315918\n",
      "\n",
      "batch: 1/1 in epoch 547/1000 \n",
      "... loss: 0.250182181596756\n",
      "\n",
      "batch: 1/1 in epoch 548/1000 \n",
      "... loss: 0.2501817047595978\n",
      "\n",
      "batch: 1/1 in epoch 549/1000 \n",
      "... loss: 0.25018125772476196\n",
      "\n",
      "batch: 1/1 in epoch 550/1000 \n",
      "... loss: 0.25018084049224854\n",
      "\n",
      "batch: 1/1 in epoch 551/1000 \n",
      "... loss: 0.2501803934574127\n",
      "\n",
      "batch: 1/1 in epoch 552/1000 \n",
      "... loss: 0.2501799464225769\n",
      "\n",
      "batch: 1/1 in epoch 553/1000 \n",
      "... loss: 0.2501794993877411\n",
      "\n",
      "batch: 1/1 in epoch 554/1000 \n",
      "... loss: 0.25017908215522766\n",
      "\n",
      "batch: 1/1 in epoch 555/1000 \n",
      "... loss: 0.25017863512039185\n",
      "\n",
      "batch: 1/1 in epoch 556/1000 \n",
      "... loss: 0.25017818808555603\n",
      "\n",
      "batch: 1/1 in epoch 557/1000 \n",
      "... loss: 0.2501777708530426\n",
      "\n",
      "batch: 1/1 in epoch 558/1000 \n",
      "... loss: 0.2501773536205292\n",
      "\n",
      "batch: 1/1 in epoch 559/1000 \n",
      "... loss: 0.25017690658569336\n",
      "\n",
      "batch: 1/1 in epoch 560/1000 \n",
      "... loss: 0.25017645955085754\n",
      "\n",
      "batch: 1/1 in epoch 561/1000 \n",
      "... loss: 0.2501760423183441\n",
      "\n",
      "batch: 1/1 in epoch 562/1000 \n",
      "... loss: 0.2501756250858307\n",
      "\n",
      "batch: 1/1 in epoch 563/1000 \n",
      "... loss: 0.25017520785331726\n",
      "\n",
      "batch: 1/1 in epoch 564/1000 \n",
      "... loss: 0.25017476081848145\n",
      "\n",
      "batch: 1/1 in epoch 565/1000 \n",
      "... loss: 0.250174343585968\n",
      "\n",
      "batch: 1/1 in epoch 566/1000 \n",
      "... loss: 0.2501739263534546\n",
      "\n",
      "batch: 1/1 in epoch 567/1000 \n",
      "... loss: 0.25017350912094116\n",
      "\n",
      "batch: 1/1 in epoch 568/1000 \n",
      "... loss: 0.25017309188842773\n",
      "\n",
      "batch: 1/1 in epoch 569/1000 \n",
      "... loss: 0.2501726746559143\n",
      "\n",
      "batch: 1/1 in epoch 570/1000 \n",
      "... loss: 0.2501722574234009\n",
      "\n",
      "batch: 1/1 in epoch 571/1000 \n",
      "... loss: 0.25017184019088745\n",
      "\n",
      "batch: 1/1 in epoch 572/1000 \n",
      "... loss: 0.250171422958374\n",
      "\n",
      "batch: 1/1 in epoch 573/1000 \n",
      "... loss: 0.250171035528183\n",
      "\n",
      "batch: 1/1 in epoch 574/1000 \n",
      "... loss: 0.25017061829566956\n",
      "\n",
      "batch: 1/1 in epoch 575/1000 \n",
      "... loss: 0.25017020106315613\n",
      "\n",
      "batch: 1/1 in epoch 576/1000 \n",
      "... loss: 0.2501697838306427\n",
      "\n",
      "batch: 1/1 in epoch 577/1000 \n",
      "... loss: 0.25016939640045166\n",
      "\n",
      "batch: 1/1 in epoch 578/1000 \n",
      "... loss: 0.25016897916793823\n",
      "\n",
      "batch: 1/1 in epoch 579/1000 \n",
      "... loss: 0.2501685619354248\n",
      "\n",
      "batch: 1/1 in epoch 580/1000 \n",
      "... loss: 0.25016817450523376\n",
      "\n",
      "batch: 1/1 in epoch 581/1000 \n",
      "... loss: 0.2501677870750427\n",
      "\n",
      "batch: 1/1 in epoch 582/1000 \n",
      "... loss: 0.2501673698425293\n",
      "\n",
      "batch: 1/1 in epoch 583/1000 \n",
      "... loss: 0.25016698241233826\n",
      "\n",
      "batch: 1/1 in epoch 584/1000 \n",
      "... loss: 0.25016656517982483\n",
      "\n",
      "batch: 1/1 in epoch 585/1000 \n",
      "... loss: 0.2501661777496338\n",
      "\n",
      "batch: 1/1 in epoch 586/1000 \n",
      "... loss: 0.25016579031944275\n",
      "\n",
      "batch: 1/1 in epoch 587/1000 \n",
      "... loss: 0.2501654028892517\n",
      "\n",
      "batch: 1/1 in epoch 588/1000 \n",
      "... loss: 0.25016501545906067\n",
      "\n",
      "batch: 1/1 in epoch 589/1000 \n",
      "... loss: 0.25016459822654724\n",
      "\n",
      "batch: 1/1 in epoch 590/1000 \n",
      "... loss: 0.2501642107963562\n",
      "\n",
      "batch: 1/1 in epoch 591/1000 \n",
      "... loss: 0.25016382336616516\n",
      "\n",
      "batch: 1/1 in epoch 592/1000 \n",
      "... loss: 0.2501634359359741\n",
      "\n",
      "batch: 1/1 in epoch 593/1000 \n",
      "... loss: 0.2501630485057831\n",
      "\n",
      "batch: 1/1 in epoch 594/1000 \n",
      "... loss: 0.25016266107559204\n",
      "\n",
      "batch: 1/1 in epoch 595/1000 \n",
      "... loss: 0.250162273645401\n",
      "\n",
      "batch: 1/1 in epoch 596/1000 \n",
      "... loss: 0.25016191601753235\n",
      "\n",
      "batch: 1/1 in epoch 597/1000 \n",
      "... loss: 0.2501615285873413\n",
      "\n",
      "batch: 1/1 in epoch 598/1000 \n",
      "... loss: 0.25016114115715027\n",
      "\n",
      "batch: 1/1 in epoch 599/1000 \n",
      "... loss: 0.25016075372695923\n",
      "\n",
      "batch: 1/1 in epoch 600/1000 \n",
      "... loss: 0.2501603662967682\n",
      "\n",
      "batch: 1/1 in epoch 601/1000 \n",
      "... loss: 0.25016000866889954\n",
      "\n",
      "batch: 1/1 in epoch 602/1000 \n",
      "... loss: 0.2501596212387085\n",
      "\n",
      "batch: 1/1 in epoch 603/1000 \n",
      "... loss: 0.25015926361083984\n",
      "\n",
      "batch: 1/1 in epoch 604/1000 \n",
      "... loss: 0.2501589059829712\n",
      "\n",
      "batch: 1/1 in epoch 605/1000 \n",
      "... loss: 0.25015851855278015\n",
      "\n",
      "batch: 1/1 in epoch 606/1000 \n",
      "... loss: 0.2501581311225891\n",
      "\n",
      "batch: 1/1 in epoch 607/1000 \n",
      "... loss: 0.25015777349472046\n",
      "\n",
      "batch: 1/1 in epoch 608/1000 \n",
      "... loss: 0.2501574158668518\n",
      "\n",
      "batch: 1/1 in epoch 609/1000 \n",
      "... loss: 0.25015705823898315\n",
      "\n",
      "batch: 1/1 in epoch 610/1000 \n",
      "... loss: 0.2501566708087921\n",
      "\n",
      "batch: 1/1 in epoch 611/1000 \n",
      "... loss: 0.2501562833786011\n",
      "\n",
      "batch: 1/1 in epoch 612/1000 \n",
      "... loss: 0.2501559555530548\n",
      "\n",
      "batch: 1/1 in epoch 613/1000 \n",
      "... loss: 0.25015556812286377\n",
      "\n",
      "batch: 1/1 in epoch 614/1000 \n",
      "... loss: 0.2501552104949951\n",
      "\n",
      "batch: 1/1 in epoch 615/1000 \n",
      "... loss: 0.25015485286712646\n",
      "\n",
      "batch: 1/1 in epoch 616/1000 \n",
      "... loss: 0.2501544952392578\n",
      "\n",
      "batch: 1/1 in epoch 617/1000 \n",
      "... loss: 0.25015413761138916\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 618/1000 \n",
      "... loss: 0.2501537799835205\n",
      "\n",
      "batch: 1/1 in epoch 619/1000 \n",
      "... loss: 0.25015342235565186\n",
      "\n",
      "batch: 1/1 in epoch 620/1000 \n",
      "... loss: 0.2501530647277832\n",
      "\n",
      "batch: 1/1 in epoch 621/1000 \n",
      "... loss: 0.25015270709991455\n",
      "\n",
      "batch: 1/1 in epoch 622/1000 \n",
      "... loss: 0.2501523494720459\n",
      "\n",
      "batch: 1/1 in epoch 623/1000 \n",
      "... loss: 0.25015202164649963\n",
      "\n",
      "batch: 1/1 in epoch 624/1000 \n",
      "... loss: 0.250151664018631\n",
      "\n",
      "batch: 1/1 in epoch 625/1000 \n",
      "... loss: 0.25015130639076233\n",
      "\n",
      "batch: 1/1 in epoch 626/1000 \n",
      "... loss: 0.2501509487628937\n",
      "\n",
      "batch: 1/1 in epoch 627/1000 \n",
      "... loss: 0.2501506209373474\n",
      "\n",
      "batch: 1/1 in epoch 628/1000 \n",
      "... loss: 0.25015026330947876\n",
      "\n",
      "batch: 1/1 in epoch 629/1000 \n",
      "... loss: 0.2501499056816101\n",
      "\n",
      "batch: 1/1 in epoch 630/1000 \n",
      "... loss: 0.25014957785606384\n",
      "\n",
      "batch: 1/1 in epoch 631/1000 \n",
      "... loss: 0.2501492202281952\n",
      "\n",
      "batch: 1/1 in epoch 632/1000 \n",
      "... loss: 0.2501488924026489\n",
      "\n",
      "batch: 1/1 in epoch 633/1000 \n",
      "... loss: 0.2501485347747803\n",
      "\n",
      "batch: 1/1 in epoch 634/1000 \n",
      "... loss: 0.250148206949234\n",
      "\n",
      "batch: 1/1 in epoch 635/1000 \n",
      "... loss: 0.25014787912368774\n",
      "\n",
      "batch: 1/1 in epoch 636/1000 \n",
      "... loss: 0.2501475214958191\n",
      "\n",
      "batch: 1/1 in epoch 637/1000 \n",
      "... loss: 0.2501471936702728\n",
      "\n",
      "batch: 1/1 in epoch 638/1000 \n",
      "... loss: 0.2501468360424042\n",
      "\n",
      "batch: 1/1 in epoch 639/1000 \n",
      "... loss: 0.2501465082168579\n",
      "\n",
      "batch: 1/1 in epoch 640/1000 \n",
      "... loss: 0.25014618039131165\n",
      "\n",
      "batch: 1/1 in epoch 641/1000 \n",
      "... loss: 0.2501458525657654\n",
      "\n",
      "batch: 1/1 in epoch 642/1000 \n",
      "... loss: 0.25014549493789673\n",
      "\n",
      "batch: 1/1 in epoch 643/1000 \n",
      "... loss: 0.25014519691467285\n",
      "\n",
      "batch: 1/1 in epoch 644/1000 \n",
      "... loss: 0.2501448392868042\n",
      "\n",
      "batch: 1/1 in epoch 645/1000 \n",
      "... loss: 0.25014451146125793\n",
      "\n",
      "batch: 1/1 in epoch 646/1000 \n",
      "... loss: 0.25014418363571167\n",
      "\n",
      "batch: 1/1 in epoch 647/1000 \n",
      "... loss: 0.2501438558101654\n",
      "\n",
      "batch: 1/1 in epoch 648/1000 \n",
      "... loss: 0.25014352798461914\n",
      "\n",
      "batch: 1/1 in epoch 649/1000 \n",
      "... loss: 0.2501432001590729\n",
      "\n",
      "batch: 1/1 in epoch 650/1000 \n",
      "... loss: 0.2501428723335266\n",
      "\n",
      "batch: 1/1 in epoch 651/1000 \n",
      "... loss: 0.25014254450798035\n",
      "\n",
      "batch: 1/1 in epoch 652/1000 \n",
      "... loss: 0.25014224648475647\n",
      "\n",
      "batch: 1/1 in epoch 653/1000 \n",
      "... loss: 0.2501419186592102\n",
      "\n",
      "batch: 1/1 in epoch 654/1000 \n",
      "... loss: 0.25014159083366394\n",
      "\n",
      "batch: 1/1 in epoch 655/1000 \n",
      "... loss: 0.25014129281044006\n",
      "\n",
      "batch: 1/1 in epoch 656/1000 \n",
      "... loss: 0.2501409649848938\n",
      "\n",
      "batch: 1/1 in epoch 657/1000 \n",
      "... loss: 0.25014063715934753\n",
      "\n",
      "batch: 1/1 in epoch 658/1000 \n",
      "... loss: 0.25014033913612366\n",
      "\n",
      "batch: 1/1 in epoch 659/1000 \n",
      "... loss: 0.2501400113105774\n",
      "\n",
      "batch: 1/1 in epoch 660/1000 \n",
      "... loss: 0.25013968348503113\n",
      "\n",
      "batch: 1/1 in epoch 661/1000 \n",
      "... loss: 0.25013938546180725\n",
      "\n",
      "batch: 1/1 in epoch 662/1000 \n",
      "... loss: 0.250139057636261\n",
      "\n",
      "batch: 1/1 in epoch 663/1000 \n",
      "... loss: 0.2501387596130371\n",
      "\n",
      "batch: 1/1 in epoch 664/1000 \n",
      "... loss: 0.25013843178749084\n",
      "\n",
      "batch: 1/1 in epoch 665/1000 \n",
      "... loss: 0.25013813376426697\n",
      "\n",
      "batch: 1/1 in epoch 666/1000 \n",
      "... loss: 0.2501378059387207\n",
      "\n",
      "batch: 1/1 in epoch 667/1000 \n",
      "... loss: 0.2501375079154968\n",
      "\n",
      "batch: 1/1 in epoch 668/1000 \n",
      "... loss: 0.25013720989227295\n",
      "\n",
      "batch: 1/1 in epoch 669/1000 \n",
      "... loss: 0.2501368820667267\n",
      "\n",
      "batch: 1/1 in epoch 670/1000 \n",
      "... loss: 0.2501365840435028\n",
      "\n",
      "batch: 1/1 in epoch 671/1000 \n",
      "... loss: 0.25013628602027893\n",
      "\n",
      "batch: 1/1 in epoch 672/1000 \n",
      "... loss: 0.25013598799705505\n",
      "\n",
      "batch: 1/1 in epoch 673/1000 \n",
      "... loss: 0.2501356601715088\n",
      "\n",
      "batch: 1/1 in epoch 674/1000 \n",
      "... loss: 0.2501353621482849\n",
      "\n",
      "batch: 1/1 in epoch 675/1000 \n",
      "... loss: 0.25013506412506104\n",
      "\n",
      "batch: 1/1 in epoch 676/1000 \n",
      "... loss: 0.25013476610183716\n",
      "\n",
      "batch: 1/1 in epoch 677/1000 \n",
      "... loss: 0.2501344680786133\n",
      "\n",
      "batch: 1/1 in epoch 678/1000 \n",
      "... loss: 0.2501341700553894\n",
      "\n",
      "batch: 1/1 in epoch 679/1000 \n",
      "... loss: 0.2501338720321655\n",
      "\n",
      "batch: 1/1 in epoch 680/1000 \n",
      "... loss: 0.25013357400894165\n",
      "\n",
      "batch: 1/1 in epoch 681/1000 \n",
      "... loss: 0.2501332759857178\n",
      "\n",
      "batch: 1/1 in epoch 682/1000 \n",
      "... loss: 0.2501329779624939\n",
      "\n",
      "batch: 1/1 in epoch 683/1000 \n",
      "... loss: 0.25013267993927\n",
      "\n",
      "batch: 1/1 in epoch 684/1000 \n",
      "... loss: 0.25013238191604614\n",
      "\n",
      "batch: 1/1 in epoch 685/1000 \n",
      "... loss: 0.25013208389282227\n",
      "\n",
      "batch: 1/1 in epoch 686/1000 \n",
      "... loss: 0.2501317858695984\n",
      "\n",
      "batch: 1/1 in epoch 687/1000 \n",
      "... loss: 0.2501315176486969\n",
      "\n",
      "batch: 1/1 in epoch 688/1000 \n",
      "... loss: 0.250131219625473\n",
      "\n",
      "batch: 1/1 in epoch 689/1000 \n",
      "... loss: 0.25013092160224915\n",
      "\n",
      "batch: 1/1 in epoch 690/1000 \n",
      "... loss: 0.25013065338134766\n",
      "\n",
      "batch: 1/1 in epoch 691/1000 \n",
      "... loss: 0.2501303553581238\n",
      "\n",
      "batch: 1/1 in epoch 692/1000 \n",
      "... loss: 0.2501300573348999\n",
      "\n",
      "batch: 1/1 in epoch 693/1000 \n",
      "... loss: 0.2501297891139984\n",
      "\n",
      "batch: 1/1 in epoch 694/1000 \n",
      "... loss: 0.25012949109077454\n",
      "\n",
      "batch: 1/1 in epoch 695/1000 \n",
      "... loss: 0.25012919306755066\n",
      "\n",
      "batch: 1/1 in epoch 696/1000 \n",
      "... loss: 0.25012892484664917\n",
      "\n",
      "batch: 1/1 in epoch 697/1000 \n",
      "... loss: 0.2501286268234253\n",
      "\n",
      "batch: 1/1 in epoch 698/1000 \n",
      "... loss: 0.2501283586025238\n",
      "\n",
      "batch: 1/1 in epoch 699/1000 \n",
      "... loss: 0.2501280605792999\n",
      "\n",
      "batch: 1/1 in epoch 700/1000 \n",
      "... loss: 0.25012779235839844\n",
      "\n",
      "batch: 1/1 in epoch 701/1000 \n",
      "... loss: 0.25012749433517456\n",
      "\n",
      "batch: 1/1 in epoch 702/1000 \n",
      "... loss: 0.25012722611427307\n",
      "\n",
      "batch: 1/1 in epoch 703/1000 \n",
      "... loss: 0.2501269578933716\n",
      "\n",
      "batch: 1/1 in epoch 704/1000 \n",
      "... loss: 0.2501266896724701\n",
      "\n",
      "batch: 1/1 in epoch 705/1000 \n",
      "... loss: 0.2501263916492462\n",
      "\n",
      "batch: 1/1 in epoch 706/1000 \n",
      "... loss: 0.2501261234283447\n",
      "\n",
      "batch: 1/1 in epoch 707/1000 \n",
      "... loss: 0.25012585520744324\n",
      "\n",
      "batch: 1/1 in epoch 708/1000 \n",
      "... loss: 0.25012555718421936\n",
      "\n",
      "batch: 1/1 in epoch 709/1000 \n",
      "... loss: 0.25012528896331787\n",
      "\n",
      "batch: 1/1 in epoch 710/1000 \n",
      "... loss: 0.2501250207424164\n",
      "\n",
      "batch: 1/1 in epoch 711/1000 \n",
      "... loss: 0.2501247525215149\n",
      "\n",
      "batch: 1/1 in epoch 712/1000 \n",
      "... loss: 0.2501244843006134\n",
      "\n",
      "batch: 1/1 in epoch 713/1000 \n",
      "... loss: 0.2501241862773895\n",
      "\n",
      "batch: 1/1 in epoch 714/1000 \n",
      "... loss: 0.2501239478588104\n",
      "\n",
      "batch: 1/1 in epoch 715/1000 \n",
      "... loss: 0.25012367963790894\n",
      "\n",
      "batch: 1/1 in epoch 716/1000 \n",
      "... loss: 0.25012341141700745\n",
      "\n",
      "batch: 1/1 in epoch 717/1000 \n",
      "... loss: 0.25012314319610596\n",
      "\n",
      "batch: 1/1 in epoch 718/1000 \n",
      "... loss: 0.25012287497520447\n",
      "\n",
      "batch: 1/1 in epoch 719/1000 \n",
      "... loss: 0.250122606754303\n",
      "\n",
      "batch: 1/1 in epoch 720/1000 \n",
      "... loss: 0.2501223385334015\n",
      "\n",
      "batch: 1/1 in epoch 721/1000 \n",
      "... loss: 0.2501220703125\n",
      "\n",
      "batch: 1/1 in epoch 722/1000 \n",
      "... loss: 0.2501218020915985\n",
      "\n",
      "batch: 1/1 in epoch 723/1000 \n",
      "... loss: 0.250121533870697\n",
      "\n",
      "batch: 1/1 in epoch 724/1000 \n",
      "... loss: 0.25012126564979553\n",
      "\n",
      "batch: 1/1 in epoch 725/1000 \n",
      "... loss: 0.25012102723121643\n",
      "\n",
      "batch: 1/1 in epoch 726/1000 \n",
      "... loss: 0.25012075901031494\n",
      "\n",
      "batch: 1/1 in epoch 727/1000 \n",
      "... loss: 0.25012049078941345\n",
      "\n",
      "batch: 1/1 in epoch 728/1000 \n",
      "... loss: 0.25012022256851196\n",
      "\n",
      "batch: 1/1 in epoch 729/1000 \n",
      "... loss: 0.2501199543476105\n",
      "\n",
      "batch: 1/1 in epoch 730/1000 \n",
      "... loss: 0.25011971592903137\n",
      "\n",
      "batch: 1/1 in epoch 731/1000 \n",
      "... loss: 0.2501194477081299\n",
      "\n",
      "batch: 1/1 in epoch 732/1000 \n",
      "... loss: 0.2501191794872284\n",
      "\n",
      "batch: 1/1 in epoch 733/1000 \n",
      "... loss: 0.2501189410686493\n",
      "\n",
      "batch: 1/1 in epoch 734/1000 \n",
      "... loss: 0.2501186728477478\n",
      "\n",
      "batch: 1/1 in epoch 735/1000 \n",
      "... loss: 0.2501184344291687\n",
      "\n",
      "batch: 1/1 in epoch 736/1000 \n",
      "... loss: 0.2501181662082672\n",
      "\n",
      "batch: 1/1 in epoch 737/1000 \n",
      "... loss: 0.2501178979873657\n",
      "\n",
      "batch: 1/1 in epoch 738/1000 \n",
      "... loss: 0.2501176595687866\n",
      "\n",
      "batch: 1/1 in epoch 739/1000 \n",
      "... loss: 0.2501174211502075\n",
      "\n",
      "batch: 1/1 in epoch 740/1000 \n",
      "... loss: 0.25011715292930603\n",
      "\n",
      "batch: 1/1 in epoch 741/1000 \n",
      "... loss: 0.25011691451072693\n",
      "\n",
      "batch: 1/1 in epoch 742/1000 \n",
      "... loss: 0.25011664628982544\n",
      "\n",
      "batch: 1/1 in epoch 743/1000 \n",
      "... loss: 0.25011640787124634\n",
      "\n",
      "batch: 1/1 in epoch 744/1000 \n",
      "... loss: 0.25011616945266724\n",
      "\n",
      "batch: 1/1 in epoch 745/1000 \n",
      "... loss: 0.25011590123176575\n",
      "\n",
      "batch: 1/1 in epoch 746/1000 \n",
      "... loss: 0.25011566281318665\n",
      "\n",
      "batch: 1/1 in epoch 747/1000 \n",
      "... loss: 0.25011542439460754\n",
      "\n",
      "batch: 1/1 in epoch 748/1000 \n",
      "... loss: 0.25011515617370605\n",
      "\n",
      "batch: 1/1 in epoch 749/1000 \n",
      "... loss: 0.25011491775512695\n",
      "\n",
      "batch: 1/1 in epoch 750/1000 \n",
      "... loss: 0.25011467933654785\n",
      "\n",
      "batch: 1/1 in epoch 751/1000 \n",
      "... loss: 0.25011444091796875\n",
      "\n",
      "batch: 1/1 in epoch 752/1000 \n",
      "... loss: 0.25011420249938965\n",
      "\n",
      "batch: 1/1 in epoch 753/1000 \n",
      "... loss: 0.25011393427848816\n",
      "\n",
      "batch: 1/1 in epoch 754/1000 \n",
      "... loss: 0.25011372566223145\n",
      "\n",
      "batch: 1/1 in epoch 755/1000 \n",
      "... loss: 0.25011345744132996\n",
      "\n",
      "batch: 1/1 in epoch 756/1000 \n",
      "... loss: 0.25011321902275085\n",
      "\n",
      "batch: 1/1 in epoch 757/1000 \n",
      "... loss: 0.25011298060417175\n",
      "\n",
      "batch: 1/1 in epoch 758/1000 \n",
      "... loss: 0.25011274218559265\n",
      "\n",
      "batch: 1/1 in epoch 759/1000 \n",
      "... loss: 0.25011250376701355\n",
      "\n",
      "batch: 1/1 in epoch 760/1000 \n",
      "... loss: 0.25011226534843445\n",
      "\n",
      "batch: 1/1 in epoch 761/1000 \n",
      "... loss: 0.25011202692985535\n",
      "\n",
      "batch: 1/1 in epoch 762/1000 \n",
      "... loss: 0.25011178851127625\n",
      "\n",
      "batch: 1/1 in epoch 763/1000 \n",
      "... loss: 0.25011155009269714\n",
      "\n",
      "batch: 1/1 in epoch 764/1000 \n",
      "... loss: 0.25011131167411804\n",
      "\n",
      "batch: 1/1 in epoch 765/1000 \n",
      "... loss: 0.25011107325553894\n",
      "\n",
      "batch: 1/1 in epoch 766/1000 \n",
      "... loss: 0.25011083483695984\n",
      "\n",
      "batch: 1/1 in epoch 767/1000 \n",
      "... loss: 0.2501106262207031\n",
      "\n",
      "batch: 1/1 in epoch 768/1000 \n",
      "... loss: 0.250110387802124\n",
      "\n",
      "batch: 1/1 in epoch 769/1000 \n",
      "... loss: 0.2501101493835449\n",
      "\n",
      "batch: 1/1 in epoch 770/1000 \n",
      "... loss: 0.2501099109649658\n",
      "\n",
      "batch: 1/1 in epoch 771/1000 \n",
      "... loss: 0.2501096725463867\n",
      "\n",
      "batch: 1/1 in epoch 772/1000 \n",
      "... loss: 0.2501094341278076\n",
      "\n",
      "batch: 1/1 in epoch 773/1000 \n",
      "... loss: 0.2501092255115509\n",
      "\n",
      "batch: 1/1 in epoch 774/1000 \n",
      "... loss: 0.2501089870929718\n",
      "\n",
      "batch: 1/1 in epoch 775/1000 \n",
      "... loss: 0.2501087486743927\n",
      "\n",
      "batch: 1/1 in epoch 776/1000 \n",
      "... loss: 0.250108540058136\n",
      "\n",
      "batch: 1/1 in epoch 777/1000 \n",
      "... loss: 0.2501083016395569\n",
      "\n",
      "batch: 1/1 in epoch 778/1000 \n",
      "... loss: 0.2501080632209778\n",
      "\n",
      "batch: 1/1 in epoch 779/1000 \n",
      "... loss: 0.25010785460472107\n",
      "\n",
      "batch: 1/1 in epoch 780/1000 \n",
      "... loss: 0.25010761618614197\n",
      "\n",
      "batch: 1/1 in epoch 781/1000 \n",
      "... loss: 0.25010740756988525\n",
      "\n",
      "batch: 1/1 in epoch 782/1000 \n",
      "... loss: 0.25010716915130615\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 783/1000 \n",
      "... loss: 0.25010693073272705\n",
      "\n",
      "batch: 1/1 in epoch 784/1000 \n",
      "... loss: 0.25010672211647034\n",
      "\n",
      "batch: 1/1 in epoch 785/1000 \n",
      "... loss: 0.25010648369789124\n",
      "\n",
      "batch: 1/1 in epoch 786/1000 \n",
      "... loss: 0.2501062750816345\n",
      "\n",
      "batch: 1/1 in epoch 787/1000 \n",
      "... loss: 0.2501060366630554\n",
      "\n",
      "batch: 1/1 in epoch 788/1000 \n",
      "... loss: 0.2501058280467987\n",
      "\n",
      "batch: 1/1 in epoch 789/1000 \n",
      "... loss: 0.250105619430542\n",
      "\n",
      "batch: 1/1 in epoch 790/1000 \n",
      "... loss: 0.2501053810119629\n",
      "\n",
      "batch: 1/1 in epoch 791/1000 \n",
      "... loss: 0.2501051723957062\n",
      "\n",
      "batch: 1/1 in epoch 792/1000 \n",
      "... loss: 0.2501049339771271\n",
      "\n",
      "batch: 1/1 in epoch 793/1000 \n",
      "... loss: 0.25010472536087036\n",
      "\n",
      "batch: 1/1 in epoch 794/1000 \n",
      "... loss: 0.25010451674461365\n",
      "\n",
      "batch: 1/1 in epoch 795/1000 \n",
      "... loss: 0.25010430812835693\n",
      "\n",
      "batch: 1/1 in epoch 796/1000 \n",
      "... loss: 0.25010406970977783\n",
      "\n",
      "batch: 1/1 in epoch 797/1000 \n",
      "... loss: 0.2501038610935211\n",
      "\n",
      "batch: 1/1 in epoch 798/1000 \n",
      "... loss: 0.2501036524772644\n",
      "\n",
      "batch: 1/1 in epoch 799/1000 \n",
      "... loss: 0.2501034140586853\n",
      "\n",
      "batch: 1/1 in epoch 800/1000 \n",
      "... loss: 0.2501032054424286\n",
      "\n",
      "batch: 1/1 in epoch 801/1000 \n",
      "... loss: 0.2501029968261719\n",
      "\n",
      "batch: 1/1 in epoch 802/1000 \n",
      "... loss: 0.25010278820991516\n",
      "\n",
      "batch: 1/1 in epoch 803/1000 \n",
      "... loss: 0.25010257959365845\n",
      "\n",
      "batch: 1/1 in epoch 804/1000 \n",
      "... loss: 0.25010237097740173\n",
      "\n",
      "batch: 1/1 in epoch 805/1000 \n",
      "... loss: 0.250102162361145\n",
      "\n",
      "batch: 1/1 in epoch 806/1000 \n",
      "... loss: 0.2501019239425659\n",
      "\n",
      "batch: 1/1 in epoch 807/1000 \n",
      "... loss: 0.2501017153263092\n",
      "\n",
      "batch: 1/1 in epoch 808/1000 \n",
      "... loss: 0.2501015067100525\n",
      "\n",
      "batch: 1/1 in epoch 809/1000 \n",
      "... loss: 0.2501012980937958\n",
      "\n",
      "batch: 1/1 in epoch 810/1000 \n",
      "... loss: 0.25010108947753906\n",
      "\n",
      "batch: 1/1 in epoch 811/1000 \n",
      "... loss: 0.25010088086128235\n",
      "\n",
      "batch: 1/1 in epoch 812/1000 \n",
      "... loss: 0.25010067224502563\n",
      "\n",
      "batch: 1/1 in epoch 813/1000 \n",
      "... loss: 0.2501004636287689\n",
      "\n",
      "batch: 1/1 in epoch 814/1000 \n",
      "... loss: 0.2501002550125122\n",
      "\n",
      "batch: 1/1 in epoch 815/1000 \n",
      "... loss: 0.2501000463962555\n",
      "\n",
      "batch: 1/1 in epoch 816/1000 \n",
      "... loss: 0.2500998377799988\n",
      "\n",
      "batch: 1/1 in epoch 817/1000 \n",
      "... loss: 0.25009965896606445\n",
      "\n",
      "batch: 1/1 in epoch 818/1000 \n",
      "... loss: 0.25009945034980774\n",
      "\n",
      "batch: 1/1 in epoch 819/1000 \n",
      "... loss: 0.250099241733551\n",
      "\n",
      "batch: 1/1 in epoch 820/1000 \n",
      "... loss: 0.2500990331172943\n",
      "\n",
      "batch: 1/1 in epoch 821/1000 \n",
      "... loss: 0.2500988245010376\n",
      "\n",
      "batch: 1/1 in epoch 822/1000 \n",
      "... loss: 0.2500986158847809\n",
      "\n",
      "batch: 1/1 in epoch 823/1000 \n",
      "... loss: 0.25009840726852417\n",
      "\n",
      "batch: 1/1 in epoch 824/1000 \n",
      "... loss: 0.25009822845458984\n",
      "\n",
      "batch: 1/1 in epoch 825/1000 \n",
      "... loss: 0.25009801983833313\n",
      "\n",
      "batch: 1/1 in epoch 826/1000 \n",
      "... loss: 0.2500978112220764\n",
      "\n",
      "batch: 1/1 in epoch 827/1000 \n",
      "... loss: 0.2500976026058197\n",
      "\n",
      "batch: 1/1 in epoch 828/1000 \n",
      "... loss: 0.2500974237918854\n",
      "\n",
      "batch: 1/1 in epoch 829/1000 \n",
      "... loss: 0.25009721517562866\n",
      "\n",
      "batch: 1/1 in epoch 830/1000 \n",
      "... loss: 0.25009700655937195\n",
      "\n",
      "batch: 1/1 in epoch 831/1000 \n",
      "... loss: 0.2500968277454376\n",
      "\n",
      "batch: 1/1 in epoch 832/1000 \n",
      "... loss: 0.2500966191291809\n",
      "\n",
      "batch: 1/1 in epoch 833/1000 \n",
      "... loss: 0.2500964105129242\n",
      "\n",
      "batch: 1/1 in epoch 834/1000 \n",
      "... loss: 0.25009623169898987\n",
      "\n",
      "batch: 1/1 in epoch 835/1000 \n",
      "... loss: 0.25009602308273315\n",
      "\n",
      "batch: 1/1 in epoch 836/1000 \n",
      "... loss: 0.25009584426879883\n",
      "\n",
      "batch: 1/1 in epoch 837/1000 \n",
      "... loss: 0.2500956356525421\n",
      "\n",
      "batch: 1/1 in epoch 838/1000 \n",
      "... loss: 0.2500954568386078\n",
      "\n",
      "batch: 1/1 in epoch 839/1000 \n",
      "... loss: 0.2500952482223511\n",
      "\n",
      "batch: 1/1 in epoch 840/1000 \n",
      "... loss: 0.25009506940841675\n",
      "\n",
      "batch: 1/1 in epoch 841/1000 \n",
      "... loss: 0.25009486079216003\n",
      "\n",
      "batch: 1/1 in epoch 842/1000 \n",
      "... loss: 0.2500946819782257\n",
      "\n",
      "batch: 1/1 in epoch 843/1000 \n",
      "... loss: 0.250094473361969\n",
      "\n",
      "batch: 1/1 in epoch 844/1000 \n",
      "... loss: 0.25009429454803467\n",
      "\n",
      "batch: 1/1 in epoch 845/1000 \n",
      "... loss: 0.25009408593177795\n",
      "\n",
      "batch: 1/1 in epoch 846/1000 \n",
      "... loss: 0.25009390711784363\n",
      "\n",
      "batch: 1/1 in epoch 847/1000 \n",
      "... loss: 0.2500936985015869\n",
      "\n",
      "batch: 1/1 in epoch 848/1000 \n",
      "... loss: 0.2500935196876526\n",
      "\n",
      "batch: 1/1 in epoch 849/1000 \n",
      "... loss: 0.25009334087371826\n",
      "\n",
      "batch: 1/1 in epoch 850/1000 \n",
      "... loss: 0.25009313225746155\n",
      "\n",
      "batch: 1/1 in epoch 851/1000 \n",
      "... loss: 0.2500929534435272\n",
      "\n",
      "batch: 1/1 in epoch 852/1000 \n",
      "... loss: 0.2500927746295929\n",
      "\n",
      "batch: 1/1 in epoch 853/1000 \n",
      "... loss: 0.2500925660133362\n",
      "\n",
      "batch: 1/1 in epoch 854/1000 \n",
      "... loss: 0.25009238719940186\n",
      "\n",
      "batch: 1/1 in epoch 855/1000 \n",
      "... loss: 0.25009220838546753\n",
      "\n",
      "batch: 1/1 in epoch 856/1000 \n",
      "... loss: 0.2500919997692108\n",
      "\n",
      "batch: 1/1 in epoch 857/1000 \n",
      "... loss: 0.2500918209552765\n",
      "\n",
      "batch: 1/1 in epoch 858/1000 \n",
      "... loss: 0.25009164214134216\n",
      "\n",
      "batch: 1/1 in epoch 859/1000 \n",
      "... loss: 0.25009146332740784\n",
      "\n",
      "batch: 1/1 in epoch 860/1000 \n",
      "... loss: 0.2500912845134735\n",
      "\n",
      "batch: 1/1 in epoch 861/1000 \n",
      "... loss: 0.2500910758972168\n",
      "\n",
      "batch: 1/1 in epoch 862/1000 \n",
      "... loss: 0.25009089708328247\n",
      "\n",
      "batch: 1/1 in epoch 863/1000 \n",
      "... loss: 0.25009071826934814\n",
      "\n",
      "batch: 1/1 in epoch 864/1000 \n",
      "... loss: 0.2500905394554138\n",
      "\n",
      "batch: 1/1 in epoch 865/1000 \n",
      "... loss: 0.2500903606414795\n",
      "\n",
      "batch: 1/1 in epoch 866/1000 \n",
      "... loss: 0.25009018182754517\n",
      "\n",
      "batch: 1/1 in epoch 867/1000 \n",
      "... loss: 0.25009000301361084\n",
      "\n",
      "batch: 1/1 in epoch 868/1000 \n",
      "... loss: 0.2500898241996765\n",
      "\n",
      "batch: 1/1 in epoch 869/1000 \n",
      "... loss: 0.2500896453857422\n",
      "\n",
      "batch: 1/1 in epoch 870/1000 \n",
      "... loss: 0.2500894367694855\n",
      "\n",
      "batch: 1/1 in epoch 871/1000 \n",
      "... loss: 0.25008925795555115\n",
      "\n",
      "batch: 1/1 in epoch 872/1000 \n",
      "... loss: 0.2500890791416168\n",
      "\n",
      "batch: 1/1 in epoch 873/1000 \n",
      "... loss: 0.2500889003276825\n",
      "\n",
      "batch: 1/1 in epoch 874/1000 \n",
      "... loss: 0.25008872151374817\n",
      "\n",
      "batch: 1/1 in epoch 875/1000 \n",
      "... loss: 0.25008854269981384\n",
      "\n",
      "batch: 1/1 in epoch 876/1000 \n",
      "... loss: 0.2500883936882019\n",
      "\n",
      "batch: 1/1 in epoch 877/1000 \n",
      "... loss: 0.2500882148742676\n",
      "\n",
      "batch: 1/1 in epoch 878/1000 \n",
      "... loss: 0.25008803606033325\n",
      "\n",
      "batch: 1/1 in epoch 879/1000 \n",
      "... loss: 0.25008782744407654\n",
      "\n",
      "batch: 1/1 in epoch 880/1000 \n",
      "... loss: 0.2500876784324646\n",
      "\n",
      "batch: 1/1 in epoch 881/1000 \n",
      "... loss: 0.2500874996185303\n",
      "\n",
      "batch: 1/1 in epoch 882/1000 \n",
      "... loss: 0.25008732080459595\n",
      "\n",
      "batch: 1/1 in epoch 883/1000 \n",
      "... loss: 0.2500871419906616\n",
      "\n",
      "batch: 1/1 in epoch 884/1000 \n",
      "... loss: 0.2500869631767273\n",
      "\n",
      "batch: 1/1 in epoch 885/1000 \n",
      "... loss: 0.25008681416511536\n",
      "\n",
      "batch: 1/1 in epoch 886/1000 \n",
      "... loss: 0.25008663535118103\n",
      "\n",
      "batch: 1/1 in epoch 887/1000 \n",
      "... loss: 0.2500864565372467\n",
      "\n",
      "batch: 1/1 in epoch 888/1000 \n",
      "... loss: 0.2500862777233124\n",
      "\n",
      "batch: 1/1 in epoch 889/1000 \n",
      "... loss: 0.25008612871170044\n",
      "\n",
      "batch: 1/1 in epoch 890/1000 \n",
      "... loss: 0.2500859200954437\n",
      "\n",
      "batch: 1/1 in epoch 891/1000 \n",
      "... loss: 0.2500857710838318\n",
      "\n",
      "batch: 1/1 in epoch 892/1000 \n",
      "... loss: 0.25008559226989746\n",
      "\n",
      "batch: 1/1 in epoch 893/1000 \n",
      "... loss: 0.25008541345596313\n",
      "\n",
      "batch: 1/1 in epoch 894/1000 \n",
      "... loss: 0.2500852346420288\n",
      "\n",
      "batch: 1/1 in epoch 895/1000 \n",
      "... loss: 0.25008508563041687\n",
      "\n",
      "batch: 1/1 in epoch 896/1000 \n",
      "... loss: 0.25008493661880493\n",
      "\n",
      "batch: 1/1 in epoch 897/1000 \n",
      "... loss: 0.2500847578048706\n",
      "\n",
      "batch: 1/1 in epoch 898/1000 \n",
      "... loss: 0.2500845789909363\n",
      "\n",
      "batch: 1/1 in epoch 899/1000 \n",
      "... loss: 0.25008440017700195\n",
      "\n",
      "batch: 1/1 in epoch 900/1000 \n",
      "... loss: 0.25008425116539\n",
      "\n",
      "batch: 1/1 in epoch 901/1000 \n",
      "... loss: 0.2500840723514557\n",
      "\n",
      "batch: 1/1 in epoch 902/1000 \n",
      "... loss: 0.25008392333984375\n",
      "\n",
      "batch: 1/1 in epoch 903/1000 \n",
      "... loss: 0.2500837445259094\n",
      "\n",
      "batch: 1/1 in epoch 904/1000 \n",
      "... loss: 0.2500835657119751\n",
      "\n",
      "batch: 1/1 in epoch 905/1000 \n",
      "... loss: 0.25008341670036316\n",
      "\n",
      "batch: 1/1 in epoch 906/1000 \n",
      "... loss: 0.25008323788642883\n",
      "\n",
      "batch: 1/1 in epoch 907/1000 \n",
      "... loss: 0.2500830888748169\n",
      "\n",
      "batch: 1/1 in epoch 908/1000 \n",
      "... loss: 0.25008291006088257\n",
      "\n",
      "batch: 1/1 in epoch 909/1000 \n",
      "... loss: 0.25008276104927063\n",
      "\n",
      "batch: 1/1 in epoch 910/1000 \n",
      "... loss: 0.2500825822353363\n",
      "\n",
      "batch: 1/1 in epoch 911/1000 \n",
      "... loss: 0.25008243322372437\n",
      "\n",
      "batch: 1/1 in epoch 912/1000 \n",
      "... loss: 0.25008225440979004\n",
      "\n",
      "batch: 1/1 in epoch 913/1000 \n",
      "... loss: 0.2500821053981781\n",
      "\n",
      "batch: 1/1 in epoch 914/1000 \n",
      "... loss: 0.2500819265842438\n",
      "\n",
      "batch: 1/1 in epoch 915/1000 \n",
      "... loss: 0.25008177757263184\n",
      "\n",
      "batch: 1/1 in epoch 916/1000 \n",
      "... loss: 0.2500816285610199\n",
      "\n",
      "batch: 1/1 in epoch 917/1000 \n",
      "... loss: 0.25008144974708557\n",
      "\n",
      "batch: 1/1 in epoch 918/1000 \n",
      "... loss: 0.25008130073547363\n",
      "\n",
      "batch: 1/1 in epoch 919/1000 \n",
      "... loss: 0.2500811219215393\n",
      "\n",
      "batch: 1/1 in epoch 920/1000 \n",
      "... loss: 0.25008097290992737\n",
      "\n",
      "batch: 1/1 in epoch 921/1000 \n",
      "... loss: 0.25008082389831543\n",
      "\n",
      "batch: 1/1 in epoch 922/1000 \n",
      "... loss: 0.2500806450843811\n",
      "\n",
      "batch: 1/1 in epoch 923/1000 \n",
      "... loss: 0.25008049607276917\n",
      "\n",
      "batch: 1/1 in epoch 924/1000 \n",
      "... loss: 0.2500803470611572\n",
      "\n",
      "batch: 1/1 in epoch 925/1000 \n",
      "... loss: 0.2500801980495453\n",
      "\n",
      "batch: 1/1 in epoch 926/1000 \n",
      "... loss: 0.25008001923561096\n",
      "\n",
      "batch: 1/1 in epoch 927/1000 \n",
      "... loss: 0.250079870223999\n",
      "\n",
      "batch: 1/1 in epoch 928/1000 \n",
      "... loss: 0.2500796914100647\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 929/1000 \n",
      "... loss: 0.25007957220077515\n",
      "\n",
      "batch: 1/1 in epoch 930/1000 \n",
      "... loss: 0.2500793933868408\n",
      "\n",
      "batch: 1/1 in epoch 931/1000 \n",
      "... loss: 0.2500792443752289\n",
      "\n",
      "batch: 1/1 in epoch 932/1000 \n",
      "... loss: 0.25007909536361694\n",
      "\n",
      "batch: 1/1 in epoch 933/1000 \n",
      "... loss: 0.250078946352005\n",
      "\n",
      "batch: 1/1 in epoch 934/1000 \n",
      "... loss: 0.2500787675380707\n",
      "\n",
      "batch: 1/1 in epoch 935/1000 \n",
      "... loss: 0.25007861852645874\n",
      "\n",
      "batch: 1/1 in epoch 936/1000 \n",
      "... loss: 0.2500784695148468\n",
      "\n",
      "batch: 1/1 in epoch 937/1000 \n",
      "... loss: 0.25007832050323486\n",
      "\n",
      "batch: 1/1 in epoch 938/1000 \n",
      "... loss: 0.2500781714916229\n",
      "\n",
      "batch: 1/1 in epoch 939/1000 \n",
      "... loss: 0.2500779926776886\n",
      "\n",
      "batch: 1/1 in epoch 940/1000 \n",
      "... loss: 0.25007787346839905\n",
      "\n",
      "batch: 1/1 in epoch 941/1000 \n",
      "... loss: 0.2500776946544647\n",
      "\n",
      "batch: 1/1 in epoch 942/1000 \n",
      "... loss: 0.2500775456428528\n",
      "\n",
      "batch: 1/1 in epoch 943/1000 \n",
      "... loss: 0.25007742643356323\n",
      "\n",
      "batch: 1/1 in epoch 944/1000 \n",
      "... loss: 0.2500772476196289\n",
      "\n",
      "batch: 1/1 in epoch 945/1000 \n",
      "... loss: 0.25007709860801697\n",
      "\n",
      "batch: 1/1 in epoch 946/1000 \n",
      "... loss: 0.25007694959640503\n",
      "\n",
      "batch: 1/1 in epoch 947/1000 \n",
      "... loss: 0.2500768005847931\n",
      "\n",
      "batch: 1/1 in epoch 948/1000 \n",
      "... loss: 0.25007665157318115\n",
      "\n",
      "batch: 1/1 in epoch 949/1000 \n",
      "... loss: 0.2500765025615692\n",
      "\n",
      "batch: 1/1 in epoch 950/1000 \n",
      "... loss: 0.2500763535499573\n",
      "\n",
      "batch: 1/1 in epoch 951/1000 \n",
      "... loss: 0.25007620453834534\n",
      "\n",
      "batch: 1/1 in epoch 952/1000 \n",
      "... loss: 0.2500760555267334\n",
      "\n",
      "batch: 1/1 in epoch 953/1000 \n",
      "... loss: 0.25007590651512146\n",
      "\n",
      "batch: 1/1 in epoch 954/1000 \n",
      "... loss: 0.2500757575035095\n",
      "\n",
      "batch: 1/1 in epoch 955/1000 \n",
      "... loss: 0.2500756084918976\n",
      "\n",
      "batch: 1/1 in epoch 956/1000 \n",
      "... loss: 0.25007545948028564\n",
      "\n",
      "batch: 1/1 in epoch 957/1000 \n",
      "... loss: 0.2500753104686737\n",
      "\n",
      "batch: 1/1 in epoch 958/1000 \n",
      "... loss: 0.25007519125938416\n",
      "\n",
      "batch: 1/1 in epoch 959/1000 \n",
      "... loss: 0.2500750422477722\n",
      "\n",
      "batch: 1/1 in epoch 960/1000 \n",
      "... loss: 0.2500748932361603\n",
      "\n",
      "batch: 1/1 in epoch 961/1000 \n",
      "... loss: 0.25007474422454834\n",
      "\n",
      "batch: 1/1 in epoch 962/1000 \n",
      "... loss: 0.2500745952129364\n",
      "\n",
      "batch: 1/1 in epoch 963/1000 \n",
      "... loss: 0.25007444620132446\n",
      "\n",
      "batch: 1/1 in epoch 964/1000 \n",
      "... loss: 0.2500742971897125\n",
      "\n",
      "batch: 1/1 in epoch 965/1000 \n",
      "... loss: 0.250074177980423\n",
      "\n",
      "batch: 1/1 in epoch 966/1000 \n",
      "... loss: 0.25007402896881104\n",
      "\n",
      "batch: 1/1 in epoch 967/1000 \n",
      "... loss: 0.2500738799571991\n",
      "\n",
      "batch: 1/1 in epoch 968/1000 \n",
      "... loss: 0.25007373094558716\n",
      "\n",
      "batch: 1/1 in epoch 969/1000 \n",
      "... loss: 0.2500735819339752\n",
      "\n",
      "batch: 1/1 in epoch 970/1000 \n",
      "... loss: 0.25007346272468567\n",
      "\n",
      "batch: 1/1 in epoch 971/1000 \n",
      "... loss: 0.25007331371307373\n",
      "\n",
      "batch: 1/1 in epoch 972/1000 \n",
      "... loss: 0.2500731647014618\n",
      "\n",
      "batch: 1/1 in epoch 973/1000 \n",
      "... loss: 0.25007301568984985\n",
      "\n",
      "batch: 1/1 in epoch 974/1000 \n",
      "... loss: 0.2500728964805603\n",
      "\n",
      "batch: 1/1 in epoch 975/1000 \n",
      "... loss: 0.25007274746894836\n",
      "\n",
      "batch: 1/1 in epoch 976/1000 \n",
      "... loss: 0.2500725984573364\n",
      "\n",
      "batch: 1/1 in epoch 977/1000 \n",
      "... loss: 0.2500724494457245\n",
      "\n",
      "batch: 1/1 in epoch 978/1000 \n",
      "... loss: 0.25007233023643494\n",
      "\n",
      "batch: 1/1 in epoch 979/1000 \n",
      "... loss: 0.250072181224823\n",
      "\n",
      "batch: 1/1 in epoch 980/1000 \n",
      "... loss: 0.25007206201553345\n",
      "\n",
      "batch: 1/1 in epoch 981/1000 \n",
      "... loss: 0.2500719130039215\n",
      "\n",
      "batch: 1/1 in epoch 982/1000 \n",
      "... loss: 0.25007176399230957\n",
      "\n",
      "batch: 1/1 in epoch 983/1000 \n",
      "... loss: 0.25007164478302\n",
      "\n",
      "batch: 1/1 in epoch 984/1000 \n",
      "... loss: 0.2500714957714081\n",
      "\n",
      "batch: 1/1 in epoch 985/1000 \n",
      "... loss: 0.25007134675979614\n",
      "\n",
      "batch: 1/1 in epoch 986/1000 \n",
      "... loss: 0.2500712275505066\n",
      "\n",
      "batch: 1/1 in epoch 987/1000 \n",
      "... loss: 0.25007107853889465\n",
      "\n",
      "batch: 1/1 in epoch 988/1000 \n",
      "... loss: 0.2500709593296051\n",
      "\n",
      "batch: 1/1 in epoch 989/1000 \n",
      "... loss: 0.25007081031799316\n",
      "\n",
      "batch: 1/1 in epoch 990/1000 \n",
      "... loss: 0.2500706911087036\n",
      "\n",
      "batch: 1/1 in epoch 991/1000 \n",
      "... loss: 0.2500705420970917\n",
      "\n",
      "batch: 1/1 in epoch 992/1000 \n",
      "... loss: 0.25007039308547974\n",
      "\n",
      "batch: 1/1 in epoch 993/1000 \n",
      "... loss: 0.2500702738761902\n",
      "\n",
      "batch: 1/1 in epoch 994/1000 \n",
      "... loss: 0.25007012486457825\n",
      "\n",
      "batch: 1/1 in epoch 995/1000 \n",
      "... loss: 0.2500700056552887\n",
      "\n",
      "batch: 1/1 in epoch 996/1000 \n",
      "... loss: 0.25006985664367676\n",
      "\n",
      "batch: 1/1 in epoch 997/1000 \n",
      "... loss: 0.2500697374343872\n",
      "\n",
      "batch: 1/1 in epoch 998/1000 \n",
      "... loss: 0.25006958842277527\n",
      "\n",
      "batch: 1/1 in epoch 999/1000 \n",
      "... loss: 0.2500694692134857\n",
      "\n",
      "batch: 1/1 in epoch 1000/1000 \n",
      "... loss: 0.2500693202018738\n",
      "\n",
      "\n",
      "[[0, 0.2837662994861603], [1, 0.5006978511810303], [2, 1.2827690839767456], [3, 4.073252201080322], [4, 10.667582511901855], [5, 11.398224830627441], [6, 0.32193297147750854], [7, 0.6663562655448914], [8, 0.6582034826278687], [9, 0.3842248320579529], [10, 0.26299160718917847], [11, 0.25382012128829956], [12, 0.2635621428489685], [13, 0.2625691592693329], [14, 0.2563020884990692], [15, 0.2522393763065338], [16, 0.25127288699150085], [17, 0.2514573335647583], [18, 0.2515496611595154], [19, 0.2514254152774811], [20, 0.251291424036026], [21, 0.251234769821167], [22, 0.25122538208961487], [23, 0.2512226402759552], [24, 0.25121423602104187], [25, 0.25120362639427185], [26, 0.25119441747665405], [27, 0.2511868476867676], [28, 0.25117984414100647], [29, 0.251172810792923], [30, 0.2511657476425171], [31, 0.251158744096756], [32, 0.2511518597602844], [33, 0.25114503502845764], [34, 0.251138299703598], [35, 0.2511315941810608], [36, 0.2511249780654907], [37, 0.25111839175224304], [38, 0.2511118948459625], [39, 0.2511054575443268], [40, 0.2510990500450134], [41, 0.25109270215034485], [42, 0.25108641386032104], [43, 0.2510802149772644], [44, 0.25107401609420776], [45, 0.2510679066181183], [46, 0.2510618269443512], [47, 0.2510558068752289], [48, 0.25104984641075134], [49, 0.2510439455509186], [50, 0.2510380744934082], [51, 0.2510322630405426], [52, 0.2510264813899994], [53, 0.25102078914642334], [54, 0.2510151267051697], [55, 0.2510094940662384], [56, 0.2510039210319519], [57, 0.2509984076023102], [58, 0.25099292397499084], [59, 0.2509874999523163], [60, 0.2509821057319641], [61, 0.2509767711162567], [62, 0.2509714663028717], [63, 0.25096622109413147], [64, 0.2509610056877136], [65, 0.25095584988594055], [66, 0.25095072388648987], [67, 0.2509456276893616], [68, 0.25094059109687805], [69, 0.2509355843067169], [70, 0.25093063712120056], [71, 0.2509257197380066], [72, 0.250920832157135], [73, 0.2509160041809082], [74, 0.2509111762046814], [75, 0.25090643763542175], [76, 0.2509016990661621], [77, 0.25089702010154724], [78, 0.25089237093925476], [79, 0.25088775157928467], [80, 0.25088316202163696], [81, 0.25087863206863403], [82, 0.2508741319179535], [83, 0.25086966156959534], [84, 0.2508651912212372], [85, 0.2508608102798462], [86, 0.2508564293384552], [87, 0.250852108001709], [88, 0.25084778666496277], [89, 0.25084352493286133], [90, 0.2508392930030823], [91, 0.2508350908756256], [92, 0.25083091855049133], [93, 0.25082677602767944], [94, 0.25082266330718994], [95, 0.2508186101913452], [96, 0.2508145570755005], [97, 0.25081053376197815], [98, 0.2508065700531006], [99, 0.250802606344223], [100, 0.25079867243766785], [101, 0.25079476833343506], [102, 0.25079092383384705], [103, 0.25078707933425903], [104, 0.2507832646369934], [105, 0.25077947974205017], [106, 0.2507757544517517], [107, 0.25077199935913086], [108, 0.2507683038711548], [109, 0.2507646381855011], [110, 0.2507610023021698], [111, 0.2507573664188385], [112, 0.250753790140152], [113, 0.25075021386146545], [114, 0.2507466673851013], [115, 0.25074315071105957], [116, 0.2507396638393402], [117, 0.25073620676994324], [118, 0.25073277950286865], [119, 0.25072935223579407], [120, 0.25072595477104187], [121, 0.25072258710861206], [122, 0.25071924924850464], [123, 0.2507159411907196], [124, 0.25071263313293457], [125, 0.2507093548774719], [126, 0.25070610642433167], [127, 0.2507028877735138], [128, 0.2506996691226959], [129, 0.2506965100765228], [130, 0.25069332122802734], [131, 0.25069019198417664], [132, 0.2506870627403259], [133, 0.25068399310112], [134, 0.25068092346191406], [135, 0.25067785382270813], [136, 0.250674843788147], [137, 0.2506718337535858], [138, 0.25066882371902466], [139, 0.2506658732891083], [140, 0.2506629228591919], [141, 0.2506599724292755], [142, 0.2506570816040039], [143, 0.2506541907787323], [144, 0.2506512999534607], [145, 0.25064846873283386], [146, 0.25064563751220703], [147, 0.2506428062915802], [148, 0.25064000487327576], [149, 0.2506372332572937], [150, 0.25063449144363403], [151, 0.25063174962997437], [152, 0.2506290376186371], [153, 0.2506263256072998], [154, 0.2506236433982849], [155, 0.25062096118927], [156, 0.2506183087825775], [157, 0.2506156861782074], [158, 0.2506130635738373], [159, 0.25061047077178955], [160, 0.2506078779697418], [161, 0.2506053149700165], [162, 0.25060275197029114], [163, 0.2506002187728882], [164, 0.2505977153778076], [165, 0.25059521198272705], [166, 0.25059273838996887], [167, 0.2505902647972107], [168, 0.2505878210067749], [169, 0.2505853772163391], [170, 0.2505829334259033], [171, 0.2505805492401123], [172, 0.2505781352519989], [173, 0.2505757808685303], [174, 0.25057339668273926], [175, 0.250571072101593], [176, 0.2505687177181244], [177, 0.25056639313697815], [178, 0.2505641281604767], [179, 0.25056180357933044], [180, 0.250559538602829], [181, 0.2505572736263275], [182, 0.25055503845214844], [183, 0.25055280327796936], [184, 0.2505505681037903], [185, 0.2505483627319336], [186, 0.2505461871623993], [187, 0.250544011592865], [188, 0.2505418360233307], [189, 0.2505396902561188], [190, 0.25053754448890686], [191, 0.25053542852401733], [192, 0.2505333125591278], [193, 0.2505311965942383], [194, 0.25052911043167114], [195, 0.250527024269104], [196, 0.25052496790885925], [197, 0.2505229115486145], [198, 0.25052088499069214], [199, 0.2505188286304474], [200, 0.2505168318748474], [201, 0.25051483511924744], [202, 0.25051283836364746], [203, 0.2505108714103699], [204, 0.2505089044570923], [205, 0.2505069375038147], [206, 0.2505050003528595], [207, 0.2505030632019043], [208, 0.2505011260509491], [209, 0.2504992187023163], [210, 0.25049731135368347], [211, 0.25049543380737305], [212, 0.25049352645874023], [213, 0.2504916787147522], [214, 0.25048983097076416], [215, 0.25048795342445374], [216, 0.2504861354827881], [217, 0.25048431754112244], [218, 0.2504824995994568], [219, 0.25048068165779114], [220, 0.2504788935184479], [221, 0.2504771053791046], [222, 0.25047534704208374], [223, 0.2504735589027405], [224, 0.2504718005657196], [225, 0.2504700720310211], [226, 0.25046834349632263], [227, 0.25046661496162415], [228, 0.25046488642692566], [229, 0.25046318769454956], [230, 0.25046148896217346], [231, 0.25045979022979736], [232, 0.25045812129974365], [233, 0.25045645236968994], [234, 0.25045478343963623], [235, 0.2504531443119049], [236, 0.2504515051841736], [237, 0.25044986605644226], [238, 0.25044822692871094], [239, 0.250446617603302], [240, 0.25044500827789307], [241, 0.2504434287548065], [242, 0.2504418194293976], [243, 0.25044023990631104], [244, 0.2504386603832245], [245, 0.2504371106624603], [246, 0.25043556094169617], [247, 0.250434011220932], [248, 0.25043246150016785], [249, 0.2504309415817261], [250, 0.2504294216632843], [251, 0.25042790174484253], [252, 0.25042638182640076], [253, 0.25042489171028137], [254, 0.250423401594162], [255, 0.250421941280365], [256, 0.2504204511642456], [257, 0.2504189908504486], [258, 0.2504175305366516], [259, 0.2504160702228546], [260, 0.2504146099090576], [261, 0.250413179397583], [262, 0.2504117488861084], [263, 0.2504103183746338], [264, 0.25040891766548157], [265, 0.25040751695632935], [266, 0.2504061162471771], [267, 0.2504047155380249], [268, 0.2504033148288727], [269, 0.25040194392204285], [270, 0.250400573015213], [271, 0.2503992021083832], [272, 0.25039783120155334], [273, 0.2503964900970459], [274, 0.25039514899253845], [275, 0.250393807888031], [276, 0.25039249658584595], [277, 0.2503911554813385], [278, 0.25038984417915344], [279, 0.2503885328769684], [280, 0.2503872215747833], [281, 0.25038594007492065], [282, 0.2503846287727356], [283, 0.2503833472728729], [284, 0.25038206577301025], [285, 0.2503807842731476], [286, 0.2503795325756073], [287, 0.250378280878067], [288, 0.25037702918052673], [289, 0.25037577748298645], [290, 0.25037452578544617], [291, 0.25037330389022827], [292, 0.250372052192688], [293, 0.2503708302974701], [294, 0.2503696084022522], [295, 0.2503684163093567], [296, 0.2503671944141388], [297, 0.2503660023212433], [298, 0.2503648102283478], [299, 0.25036361813545227], [300, 0.25036242604255676], [301, 0.25036126375198364], [302, 0.25036007165908813], [303, 0.250358909368515], [304, 0.2503577470779419], [305, 0.25035661458969116], [306, 0.25035545229911804], [307, 0.2503543198108673], [308, 0.2503531873226166], [309, 0.25035202503204346], [310, 0.2503509223461151], [311, 0.2503497898578644], [312, 0.25034865736961365], [313, 0.2503475546836853], [314, 0.25034645199775696], [315, 0.2503453493118286], [316, 0.25034424662590027], [317, 0.2503431737422943], [318, 0.25034207105636597], [319, 0.25034099817276], [320, 0.25033992528915405], [321, 0.2503388524055481], [322, 0.25033777952194214], [323, 0.2503367066383362], [324, 0.2503356635570526], [325, 0.25033462047576904], [326, 0.2503335475921631], [327, 0.2503325045108795], [328, 0.25033149123191833], [329, 0.25033044815063477], [330, 0.2503294050693512], [331, 0.25032839179039], [332, 0.25032737851142883], [333, 0.25032636523246765], [334, 0.25032535195350647], [335, 0.2503243684768677], [336, 0.2503233551979065], [337, 0.2503223419189453], [338, 0.2503213584423065], [339, 0.2503203749656677], [340, 0.25031939148902893], [341, 0.25031840801239014], [342, 0.25031742453575134], [343, 0.25031647086143494], [344, 0.25031548738479614], [345, 0.25031453371047974], [346, 0.25031358003616333], [347, 0.2503126263618469], [348, 0.2503116726875305], [349, 0.2503107488155365], [350, 0.2503097951412201], [351, 0.2503088414669037], [352, 0.25030791759490967], [353, 0.25030699372291565], [354, 0.25030606985092163], [355, 0.2503051459789276], [356, 0.2503042221069336], [357, 0.25030332803726196], [358, 0.25030240416526794], [359, 0.2503015100955963], [360, 0.2503006160259247], [361, 0.25029972195625305], [362, 0.2502988278865814], [363, 0.2502979338169098], [364, 0.25029703974723816], [365, 0.2502961754798889], [366, 0.2502952814102173], [367, 0.25029441714286804], [368, 0.2502935528755188], [369, 0.25029268860816956], [370, 0.2502918243408203], [371, 0.25029096007347107], [372, 0.2502900958061218], [373, 0.25028926134109497], [374, 0.2502883970737457], [375, 0.25028756260871887], [376, 0.250286728143692], [377, 0.2502858638763428], [378, 0.2502850592136383], [379, 0.25028419494628906], [380, 0.2502833902835846], [381, 0.25028255581855774], [382, 0.25028175115585327], [383, 0.2502809166908264], [384, 0.25028011202812195], [385, 0.2502793073654175], [386, 0.250278502702713], [387, 0.25027769804000854], [388, 0.2502768933773041], [389, 0.2502760887145996], [390, 0.25027531385421753], [391, 0.25027450919151306], [392, 0.250273734331131], [393, 0.2502729296684265], [394, 0.25027215480804443], [395, 0.25027137994766235], [396, 0.2502706050872803], [397, 0.2502698302268982], [398, 0.2502690553665161], [399, 0.2502683103084564], [400, 0.25026753544807434], [401, 0.25026679039001465], [402, 0.25026601552963257], [403, 0.2502652704715729], [404, 0.2502645254135132], [405, 0.2502637803554535], [406, 0.2502630352973938], [407, 0.2502622902393341], [408, 0.2502615451812744], [409, 0.2502608299255371], [410, 0.2502600848674774], [411, 0.2502593398094177], [412, 0.2502586245536804], [413, 0.2502579092979431], [414, 0.2502571940422058], [415, 0.2502564787864685], [416, 0.2502557337284088], [417, 0.2502550482749939], [418, 0.2502543330192566], [419, 0.2502536177635193], [420, 0.25025293231010437], [421, 0.25025221705436707], [422, 0.25025153160095215], [423, 0.25025081634521484], [424, 0.2502501308917999], [425, 0.250249445438385], [426, 0.2502487599849701], [427, 0.2502480745315552], [428, 0.25024738907814026], [429, 0.25024670362472534], [430, 0.2502460181713104], [431, 0.2502453625202179], [432, 0.250244677066803], [433, 0.25024402141571045], [434, 0.25024333596229553], [435, 0.250242680311203], [436, 0.2502420246601105], [437, 0.25024136900901794], [438, 0.2502407133579254], [439, 0.2502400577068329], [440, 0.25023940205574036], [441, 0.2502387464046478], [442, 0.2502380907535553], [443, 0.25023746490478516], [444, 0.2502368092536926], [445, 0.2502361834049225], [446, 0.25023555755615234], [447, 0.2502349019050598], [448, 0.2502342760562897], [449, 0.25023365020751953], [450, 0.2502330243587494], [451, 0.25023239850997925], [452, 0.2502317726612091], [453, 0.25023117661476135], [454, 0.2502305209636688], [455, 0.25022992491722107], [456, 0.2502292990684509], [457, 0.2502287030220032], [458, 0.25022807717323303], [459, 0.2502274811267853], [460, 0.2502268850803375], [461, 0.2502262592315674], [462, 0.25022566318511963], [463, 0.2502250671386719], [464, 0.2502244710922241], [465, 0.25022387504577637], [466, 0.2502232789993286], [467, 0.25022271275520325], [468, 0.2502221167087555], [469, 0.25022152066230774], [470, 0.2502209544181824], [471, 0.2502203583717346], [472, 0.25021979212760925], [473, 0.2502192258834839], [474, 0.25021862983703613], [475, 0.25021806359291077], [476, 0.2502174973487854], [477, 0.25021693110466003], [478, 0.25021636486053467], [479, 0.2502157986164093], [480, 0.25021523237228394], [481, 0.25021466612815857], [482, 0.2502141296863556], [483, 0.2502135634422302], [484, 0.25021299719810486], [485, 0.2502124607563019], [486, 0.2502118945121765], [487, 0.25021135807037354], [488, 0.25021082162857056], [489, 0.2502102553844452], [490, 0.2502097189426422], [491, 0.25020918250083923], [492, 0.25020864605903625], [493, 0.2502081096172333], [494, 0.2502075731754303], [495, 0.2502070367336273], [496, 0.25020650029182434], [497, 0.25020599365234375], [498, 0.25020545721054077], [499, 0.2502049207687378], [500, 0.2502044141292572], [501, 0.2502038776874542], [502, 0.25020337104797363], [503, 0.25020283460617065], [504, 0.25020232796669006], [505, 0.2502018213272095], [506, 0.2502013146877289], [507, 0.2502007782459259], [508, 0.2502002716064453], [509, 0.2501997649669647], [510, 0.25019925832748413], [511, 0.2501987814903259], [512, 0.25019827485084534], [513, 0.25019776821136475], [514, 0.25019726157188416], [515, 0.25019675493240356], [516, 0.25019627809524536], [517, 0.25019577145576477], [518, 0.25019529461860657], [519, 0.250194787979126], [520, 0.2501943111419678], [521, 0.2501938045024872], [522, 0.250193327665329], [523, 0.2501928508281708], [524, 0.2501923739910126], [525, 0.25019189715385437], [526, 0.25019142031669617], [527, 0.25019094347953796], [528, 0.25019046664237976], [529, 0.25018998980522156], [530, 0.25018951296806335], [531, 0.25018903613090515], [532, 0.25018855929374695], [533, 0.25018811225891113], [534, 0.25018763542175293], [535, 0.2501871585845947], [536, 0.2501867115497589], [537, 0.2501862347126007], [538, 0.2501857876777649], [539, 0.2501853406429291], [540, 0.2501848638057709], [541, 0.25018441677093506], [542, 0.25018396973609924], [543, 0.2501835227012634], [544, 0.2501830458641052], [545, 0.2501826286315918], [546, 0.250182181596756], [547, 0.2501817047595978], [548, 0.25018125772476196], [549, 0.25018084049224854], [550, 0.2501803934574127], [551, 0.2501799464225769], [552, 0.2501794993877411], [553, 0.25017908215522766], [554, 0.25017863512039185], [555, 0.25017818808555603], [556, 0.2501777708530426], [557, 0.2501773536205292], [558, 0.25017690658569336], [559, 0.25017645955085754], [560, 0.2501760423183441], [561, 0.2501756250858307], [562, 0.25017520785331726], [563, 0.25017476081848145], [564, 0.250174343585968], [565, 0.2501739263534546], [566, 0.25017350912094116], [567, 0.25017309188842773], [568, 0.2501726746559143], [569, 0.2501722574234009], [570, 0.25017184019088745], [571, 0.250171422958374], [572, 0.250171035528183], [573, 0.25017061829566956], [574, 0.25017020106315613], [575, 0.2501697838306427], [576, 0.25016939640045166], [577, 0.25016897916793823], [578, 0.2501685619354248], [579, 0.25016817450523376], [580, 0.2501677870750427], [581, 0.2501673698425293], [582, 0.25016698241233826], [583, 0.25016656517982483], [584, 0.2501661777496338], [585, 0.25016579031944275], [586, 0.2501654028892517], [587, 0.25016501545906067], [588, 0.25016459822654724], [589, 0.2501642107963562], [590, 0.25016382336616516], [591, 0.2501634359359741], [592, 0.2501630485057831], [593, 0.25016266107559204], [594, 0.250162273645401], [595, 0.25016191601753235], [596, 0.2501615285873413], [597, 0.25016114115715027], [598, 0.25016075372695923], [599, 0.2501603662967682], [600, 0.25016000866889954], [601, 0.2501596212387085], [602, 0.25015926361083984], [603, 0.2501589059829712], [604, 0.25015851855278015], [605, 0.2501581311225891], [606, 0.25015777349472046], [607, 0.2501574158668518], [608, 0.25015705823898315], [609, 0.2501566708087921], [610, 0.2501562833786011], [611, 0.2501559555530548], [612, 0.25015556812286377], [613, 0.2501552104949951], [614, 0.25015485286712646], [615, 0.2501544952392578], [616, 0.25015413761138916], [617, 0.2501537799835205], [618, 0.25015342235565186], [619, 0.2501530647277832], [620, 0.25015270709991455], [621, 0.2501523494720459], [622, 0.25015202164649963], [623, 0.250151664018631], [624, 0.25015130639076233], [625, 0.2501509487628937], [626, 0.2501506209373474], [627, 0.25015026330947876], [628, 0.2501499056816101], [629, 0.25014957785606384], [630, 0.2501492202281952], [631, 0.2501488924026489], [632, 0.2501485347747803], [633, 0.250148206949234], [634, 0.25014787912368774], [635, 0.2501475214958191], [636, 0.2501471936702728], [637, 0.2501468360424042], [638, 0.2501465082168579], [639, 0.25014618039131165], [640, 0.2501458525657654], [641, 0.25014549493789673], [642, 0.25014519691467285], [643, 0.2501448392868042], [644, 0.25014451146125793], [645, 0.25014418363571167], [646, 0.2501438558101654], [647, 0.25014352798461914], [648, 0.2501432001590729], [649, 0.2501428723335266], [650, 0.25014254450798035], [651, 0.25014224648475647], [652, 0.2501419186592102], [653, 0.25014159083366394], [654, 0.25014129281044006], [655, 0.2501409649848938], [656, 0.25014063715934753], [657, 0.25014033913612366], [658, 0.2501400113105774], [659, 0.25013968348503113], [660, 0.25013938546180725], [661, 0.250139057636261], [662, 0.2501387596130371], [663, 0.25013843178749084], [664, 0.25013813376426697], [665, 0.2501378059387207], [666, 0.2501375079154968], [667, 0.25013720989227295], [668, 0.2501368820667267], [669, 0.2501365840435028], [670, 0.25013628602027893], [671, 0.25013598799705505], [672, 0.2501356601715088], [673, 0.2501353621482849], [674, 0.25013506412506104], [675, 0.25013476610183716], [676, 0.2501344680786133], [677, 0.2501341700553894], [678, 0.2501338720321655], [679, 0.25013357400894165], [680, 0.2501332759857178], [681, 0.2501329779624939], [682, 0.25013267993927], [683, 0.25013238191604614], [684, 0.25013208389282227], [685, 0.2501317858695984], [686, 0.2501315176486969], [687, 0.250131219625473], [688, 0.25013092160224915], [689, 0.25013065338134766], [690, 0.2501303553581238], [691, 0.2501300573348999], [692, 0.2501297891139984], [693, 0.25012949109077454], [694, 0.25012919306755066], [695, 0.25012892484664917], [696, 0.2501286268234253], [697, 0.2501283586025238], [698, 0.2501280605792999], [699, 0.25012779235839844], [700, 0.25012749433517456], [701, 0.25012722611427307], [702, 0.2501269578933716], [703, 0.2501266896724701], [704, 0.2501263916492462], [705, 0.2501261234283447], [706, 0.25012585520744324], [707, 0.25012555718421936], [708, 0.25012528896331787], [709, 0.2501250207424164], [710, 0.2501247525215149], [711, 0.2501244843006134], [712, 0.2501241862773895], [713, 0.2501239478588104], [714, 0.25012367963790894], [715, 0.25012341141700745], [716, 0.25012314319610596], [717, 0.25012287497520447], [718, 0.250122606754303], [719, 0.2501223385334015], [720, 0.2501220703125], [721, 0.2501218020915985], [722, 0.250121533870697], [723, 0.25012126564979553], [724, 0.25012102723121643], [725, 0.25012075901031494], [726, 0.25012049078941345], [727, 0.25012022256851196], [728, 0.2501199543476105], [729, 0.25011971592903137], [730, 0.2501194477081299], [731, 0.2501191794872284], [732, 0.2501189410686493], [733, 0.2501186728477478], [734, 0.2501184344291687], [735, 0.2501181662082672], [736, 0.2501178979873657], [737, 0.2501176595687866], [738, 0.2501174211502075], [739, 0.25011715292930603], [740, 0.25011691451072693], [741, 0.25011664628982544], [742, 0.25011640787124634], [743, 0.25011616945266724], [744, 0.25011590123176575], [745, 0.25011566281318665], [746, 0.25011542439460754], [747, 0.25011515617370605], [748, 0.25011491775512695], [749, 0.25011467933654785], [750, 0.25011444091796875], [751, 0.25011420249938965], [752, 0.25011393427848816], [753, 0.25011372566223145], [754, 0.25011345744132996], [755, 0.25011321902275085], [756, 0.25011298060417175], [757, 0.25011274218559265], [758, 0.25011250376701355], [759, 0.25011226534843445], [760, 0.25011202692985535], [761, 0.25011178851127625], [762, 0.25011155009269714], [763, 0.25011131167411804], [764, 0.25011107325553894], [765, 0.25011083483695984], [766, 0.2501106262207031], [767, 0.250110387802124], [768, 0.2501101493835449], [769, 0.2501099109649658], [770, 0.2501096725463867], [771, 0.2501094341278076], [772, 0.2501092255115509], [773, 0.2501089870929718], [774, 0.2501087486743927], [775, 0.250108540058136], [776, 0.2501083016395569], [777, 0.2501080632209778], [778, 0.25010785460472107], [779, 0.25010761618614197], [780, 0.25010740756988525], [781, 0.25010716915130615], [782, 0.25010693073272705], [783, 0.25010672211647034], [784, 0.25010648369789124], [785, 0.2501062750816345], [786, 0.2501060366630554], [787, 0.2501058280467987], [788, 0.250105619430542], [789, 0.2501053810119629], [790, 0.2501051723957062], [791, 0.2501049339771271], [792, 0.25010472536087036], [793, 0.25010451674461365], [794, 0.25010430812835693], [795, 0.25010406970977783], [796, 0.2501038610935211], [797, 0.2501036524772644], [798, 0.2501034140586853], [799, 0.2501032054424286], [800, 0.2501029968261719], [801, 0.25010278820991516], [802, 0.25010257959365845], [803, 0.25010237097740173], [804, 0.250102162361145], [805, 0.2501019239425659], [806, 0.2501017153263092], [807, 0.2501015067100525], [808, 0.2501012980937958], [809, 0.25010108947753906], [810, 0.25010088086128235], [811, 0.25010067224502563], [812, 0.2501004636287689], [813, 0.2501002550125122], [814, 0.2501000463962555], [815, 0.2500998377799988], [816, 0.25009965896606445], [817, 0.25009945034980774], [818, 0.250099241733551], [819, 0.2500990331172943], [820, 0.2500988245010376], [821, 0.2500986158847809], [822, 0.25009840726852417], [823, 0.25009822845458984], [824, 0.25009801983833313], [825, 0.2500978112220764], [826, 0.2500976026058197], [827, 0.2500974237918854], [828, 0.25009721517562866], [829, 0.25009700655937195], [830, 0.2500968277454376], [831, 0.2500966191291809], [832, 0.2500964105129242], [833, 0.25009623169898987], [834, 0.25009602308273315], [835, 0.25009584426879883], [836, 0.2500956356525421], [837, 0.2500954568386078], [838, 0.2500952482223511], [839, 0.25009506940841675], [840, 0.25009486079216003], [841, 0.2500946819782257], [842, 0.250094473361969], [843, 0.25009429454803467], [844, 0.25009408593177795], [845, 0.25009390711784363], [846, 0.2500936985015869], [847, 0.2500935196876526], [848, 0.25009334087371826], [849, 0.25009313225746155], [850, 0.2500929534435272], [851, 0.2500927746295929], [852, 0.2500925660133362], [853, 0.25009238719940186], [854, 0.25009220838546753], [855, 0.2500919997692108], [856, 0.2500918209552765], [857, 0.25009164214134216], [858, 0.25009146332740784], [859, 0.2500912845134735], [860, 0.2500910758972168], [861, 0.25009089708328247], [862, 0.25009071826934814], [863, 0.2500905394554138], [864, 0.2500903606414795], [865, 0.25009018182754517], [866, 0.25009000301361084], [867, 0.2500898241996765], [868, 0.2500896453857422], [869, 0.2500894367694855], [870, 0.25008925795555115], [871, 0.2500890791416168], [872, 0.2500889003276825], [873, 0.25008872151374817], [874, 0.25008854269981384], [875, 0.2500883936882019], [876, 0.2500882148742676], [877, 0.25008803606033325], [878, 0.25008782744407654], [879, 0.2500876784324646], [880, 0.2500874996185303], [881, 0.25008732080459595], [882, 0.2500871419906616], [883, 0.2500869631767273], [884, 0.25008681416511536], [885, 0.25008663535118103], [886, 0.2500864565372467], [887, 0.2500862777233124], [888, 0.25008612871170044], [889, 0.2500859200954437], [890, 0.2500857710838318], [891, 0.25008559226989746], [892, 0.25008541345596313], [893, 0.2500852346420288], [894, 0.25008508563041687], [895, 0.25008493661880493], [896, 0.2500847578048706], [897, 0.2500845789909363], [898, 0.25008440017700195], [899, 0.25008425116539], [900, 0.2500840723514557], [901, 0.25008392333984375], [902, 0.2500837445259094], [903, 0.2500835657119751], [904, 0.25008341670036316], [905, 0.25008323788642883], [906, 0.2500830888748169], [907, 0.25008291006088257], [908, 0.25008276104927063], [909, 0.2500825822353363], [910, 0.25008243322372437], [911, 0.25008225440979004], [912, 0.2500821053981781], [913, 0.2500819265842438], [914, 0.25008177757263184], [915, 0.2500816285610199], [916, 0.25008144974708557], [917, 0.25008130073547363], [918, 0.2500811219215393], [919, 0.25008097290992737], [920, 0.25008082389831543], [921, 0.2500806450843811], [922, 0.25008049607276917], [923, 0.2500803470611572], [924, 0.2500801980495453], [925, 0.25008001923561096], [926, 0.250079870223999], [927, 0.2500796914100647], [928, 0.25007957220077515], [929, 0.2500793933868408], [930, 0.2500792443752289], [931, 0.25007909536361694], [932, 0.250078946352005], [933, 0.2500787675380707], [934, 0.25007861852645874], [935, 0.2500784695148468], [936, 0.25007832050323486], [937, 0.2500781714916229], [938, 0.2500779926776886], [939, 0.25007787346839905], [940, 0.2500776946544647], [941, 0.2500775456428528], [942, 0.25007742643356323], [943, 0.2500772476196289], [944, 0.25007709860801697], [945, 0.25007694959640503], [946, 0.2500768005847931], [947, 0.25007665157318115], [948, 0.2500765025615692], [949, 0.2500763535499573], [950, 0.25007620453834534], [951, 0.2500760555267334], [952, 0.25007590651512146], [953, 0.2500757575035095], [954, 0.2500756084918976], [955, 0.25007545948028564], [956, 0.2500753104686737], [957, 0.25007519125938416], [958, 0.2500750422477722], [959, 0.2500748932361603], [960, 0.25007474422454834], [961, 0.2500745952129364], [962, 0.25007444620132446], [963, 0.2500742971897125], [964, 0.250074177980423], [965, 0.25007402896881104], [966, 0.2500738799571991], [967, 0.25007373094558716], [968, 0.2500735819339752], [969, 0.25007346272468567], [970, 0.25007331371307373], [971, 0.2500731647014618], [972, 0.25007301568984985], [973, 0.2500728964805603], [974, 0.25007274746894836], [975, 0.2500725984573364], [976, 0.2500724494457245], [977, 0.25007233023643494], [978, 0.250072181224823], [979, 0.25007206201553345], [980, 0.2500719130039215], [981, 0.25007176399230957], [982, 0.25007164478302], [983, 0.2500714957714081], [984, 0.25007134675979614], [985, 0.2500712275505066], [986, 0.25007107853889465], [987, 0.2500709593296051], [988, 0.25007081031799316], [989, 0.2500706911087036], [990, 0.2500705420970917], [991, 0.25007039308547974], [992, 0.2500702738761902], [993, 0.25007012486457825], [994, 0.2500700056552887], [995, 0.25006985664367676], [996, 0.2500697374343872], [997, 0.25006958842277527], [998, 0.2500694692134857], [999, 0.2500693202018738]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/2\n",
      "... correct: 1\n",
      "\n",
      "average test loss: 0.2417515516281128, relative correct: 1.0\n",
      "\n",
      "confusion:\n",
      "[[0.0, 0], [1.0, 1]]\n",
      "... done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# debug set\n",
    "net_full = Lin_Net(8, 1, 64, act_function)\n",
    "train_loader_debug, test_loader_debug = make_data(emotion_dataset, \"full\", batch_size, True)\n",
    "train(train_loader_debug, net_full, 1000, criterion, 100, \"../logs/mse_debug\", cuda, 0.1)\n",
    "test(test_loader_debug, net_full, criterion, 100, \"../logs/mse_debug\", cuda)\n",
    "\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------- net_lin_emotion_full\")\n",
    "#net_full = Lin_Net(8, 1, 64, act_function)\n",
    "#train_loader_emotion_full, test_loader_emotion_full = make_data(emotion_dataset, \"full\", batch_size)\n",
    "#train(train_loader_emotion_full, net_full, 100, criterion, 5000, \"../logs/mse_emotion_full\", cuda, 0.1)\n",
    "#test(test_loader_emotion_full, net_full, criterion, 1000, \"../logs/mse_emotion_full\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_nolex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_emotion_nolex, test_loader_emotion_nolex = make_data(emotion_dataset, \"nolex\", batch_size)\n",
    "#train(train_loader_emotion_nolex, net_half, 100, criterion, 5000, \"../logs/mse_emotion_nolex\", cuda, 0.1)\n",
    "#test(test_loader_emotion_nolex, net_half, criterion, 1000, \"../logs/mse_emotion_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_lex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_emotion_lex, test_loader_emotion_lex = make_data(emotion_dataset, \"lex\", batch_size)\n",
    "#train(train_loader_emotion_lex, net_half, 100, criterion, 5000, \"../logs/mse_emotion_lex\", cuda, 0.1)\n",
    "#test(test_loader_emotion_lex, net_half, criterion, 1000, \"../logs/mse_emotion_lex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_full\")\n",
    "#net_full = Lin_Net(8, 1, 64, act_function)\n",
    "#train_loader_tweet_full, test_loader_tweet_full = make_data(tweet_dataset, \"full\", batch_size)\n",
    "#train(train_loader_tweet_full, net_full, 100, criterion, 5000, \"../logs/mse_tweet_full\", cuda, 0.1)\n",
    "#test(test_loader_tweet_full, net_full, criterion, 1000, \"../logs/mse_tweet_full\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_nolex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_tweet_nolex, test_loader_tweet_nolex = make_data(tweet_dataset, \"nolex\", batch_size)\n",
    "#train(train_loader_tweet_nolex, net_half, 100, criterion, 5000, \"../logs/mse_tweet_nolex\", cuda, 0.1)\n",
    "#test(test_loader_tweet_nolex, net_half, criterion, 1000, \"../logs/mse_tweet_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_lex\")\n",
    "#net_half = Lin_Net(4, 1, 64, act_function)\n",
    "#train_loader_tweet_lex, test_loader_tweet_lex = make_data(tweet_dataset, \"lex\", batch_size)\n",
    "#train(train_loader_tweet_lex, net_half, 100, criterion, 5000, \"../logs/mse_tweet_lex\", cuda, 0.1)\n",
    "#test(test_loader_tweet_lex, net_half, criterion, 1000, \"../logs/mse_tweet_lex\")\n",
    "\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
