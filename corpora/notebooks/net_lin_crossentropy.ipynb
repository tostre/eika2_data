{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim_1, hidden_dim_2, act_function):\n",
    "        super(Lin_Net, self).__init__()\n",
    "        self.act_function = act_function\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.lin2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.lin3 = nn.Linear(hidden_dim_2, hidden_dim_1)\n",
    "        self.lin4 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_function(self.lin1(x))\n",
    "        x = self.act_function(self.lin2(x))\n",
    "        x = self.act_function(self.lin3(x))\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, features, batch_size, debug=False):\n",
    "    datasets = []\n",
    "    for file in dataset:\n",
    "        datasets.append(pd.read_csv(\"../\" + file))\n",
    "    dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "    \n",
    "    target = dataset[\"affect\"]\n",
    "    dataset_full = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"h_count\", \"s_count\", \"a_count\", \"f_count\", \"cons_punct_count\"]]\n",
    "    dataset_nolex = dataset[[\"word_count\", \"upper_word_count\", \"ent_word_count\", \"cons_punct_count\"]]\n",
    "    dataset_lex = dataset[[\"h_count\", \"s_count\", \"a_count\", \"f_count\"]]\n",
    "    \n",
    "    # make train and test sets\n",
    "    if features == \"full\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.2)\n",
    "    elif features == \"nolex\":\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_nolex, target, test_size=0.2)\n",
    "    elif features == \"lex\": \n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_lex, target, test_size=0.2)\n",
    "\n",
    "    # make data loaders\n",
    "    train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "    test_data = MyDataset(test_x.to_numpy(), test_y.to_numpy())\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    \n",
    "    if debug: \n",
    "        dataset_full = dataset_full.iloc[:10]\n",
    "        target = target[:10]\n",
    "        train_x, test_x, train_y, test_y = train_test_split(dataset_full, target, test_size=0.8)\n",
    "        train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "        test_loader = DataLoader(dataset=train_data, batch_size=1)\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(summary, file):\n",
    "    log = open(file, \"a\")\n",
    "    log.write(summary)\n",
    "    log.close()\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epochs, criterion, print_every, save_name, cuda, lr):\n",
    "    open(save_name + \"_train_log\", \"w\").close()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.5)\n",
    "    error_curve = []\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        for index, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.float(), targets.long()\n",
    "            if cuda: \n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                net = net.cuda()\n",
    "            pred = net(inputs)\n",
    "            loss = criterion(pred.float(), targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((index) % print_every == 0):\n",
    "                log(\"batch: {}/{} in epoch {}/{} \\n... loss: {}\\n\".\n",
    "                    format((index+1), len(train_loader), (epoch+1), epochs, loss.item()), \n",
    "                    save_name + \"_train_log\")\n",
    "        # save network after every epoch\n",
    "        torch.save(net.state_dict(), save_name + \".pt\")  \n",
    "        # after every epoch save the error\n",
    "        error_curve.append([epoch, loss.item()])\n",
    "    log(\"\\n\" + str(error_curve), save_name + \"_train_log\")\n",
    "    return error_curve\n",
    "\n",
    "def test(test_loader, net, criterion, print_every, save_name, cuda):\n",
    "    open(save_name + \"_test_log\", \"w\").close()\n",
    "    confusion = []\n",
    "    net.eval()\n",
    "    loss_sum, correct, correct2 = 0, 0, 0\n",
    "    for index, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.float(), targets.long()\n",
    "        if cuda: \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            net = net.cuda()\n",
    "        pred = net(inputs)\n",
    "        pred_class = torch.max(pred.data, 1)[1]\n",
    "        loss_sum += criterion(pred, targets).item()\n",
    "        confusion.append([targets.item(), pred_class.item()])\n",
    "        if pred_class.item() == targets.item(): \n",
    "            correct += 1\n",
    "        if ((index) % print_every == 0):\n",
    "            log(\"batch: {}/{}\\n... correct: {}\\n\".\n",
    "                format((index+1), len(test_loader), correct), \n",
    "                save_name + \"_test_log\")\n",
    "           \n",
    "    # give end report\n",
    "    log(\"average test loss: {}, relative correct: {}\\n\\nconfusion:\\n{}\".\n",
    "        format((loss_sum / len(test_loader)), (correct / len(test_loader)),str(confusion)), \n",
    "        save_name + \"test_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables\n",
      "batch: 1/1 in epoch 1/1000 \n",
      "... loss: 1.633866548538208\n",
      "\n",
      "batch: 1/1 in epoch 2/1000 \n",
      "... loss: 1.147742748260498\n",
      "\n",
      "batch: 1/1 in epoch 3/1000 \n",
      "... loss: 0.8667626976966858\n",
      "\n",
      "batch: 1/1 in epoch 4/1000 \n",
      "... loss: 0.7740287780761719\n",
      "\n",
      "batch: 1/1 in epoch 5/1000 \n",
      "... loss: 0.7412734031677246\n",
      "\n",
      "batch: 1/1 in epoch 6/1000 \n",
      "... loss: 0.7261660099029541\n",
      "\n",
      "batch: 1/1 in epoch 7/1000 \n",
      "... loss: 0.7191129922866821\n",
      "\n",
      "batch: 1/1 in epoch 8/1000 \n",
      "... loss: 0.7149940729141235\n",
      "\n",
      "batch: 1/1 in epoch 9/1000 \n",
      "... loss: 0.7121402621269226\n",
      "\n",
      "batch: 1/1 in epoch 10/1000 \n",
      "... loss: 0.7101860642433167\n",
      "\n",
      "batch: 1/1 in epoch 11/1000 \n",
      "... loss: 0.7086505889892578\n",
      "\n",
      "batch: 1/1 in epoch 12/1000 \n",
      "... loss: 0.7073701620101929\n",
      "\n",
      "batch: 1/1 in epoch 13/1000 \n",
      "... loss: 0.706290602684021\n",
      "\n",
      "batch: 1/1 in epoch 14/1000 \n",
      "... loss: 0.7053389549255371\n",
      "\n",
      "batch: 1/1 in epoch 15/1000 \n",
      "... loss: 0.7044856548309326\n",
      "\n",
      "batch: 1/1 in epoch 16/1000 \n",
      "... loss: 0.7037140727043152\n",
      "\n",
      "batch: 1/1 in epoch 17/1000 \n",
      "... loss: 0.7030069828033447\n",
      "\n",
      "batch: 1/1 in epoch 18/1000 \n",
      "... loss: 0.702354371547699\n",
      "\n",
      "batch: 1/1 in epoch 19/1000 \n",
      "... loss: 0.7017486691474915\n",
      "\n",
      "batch: 1/1 in epoch 20/1000 \n",
      "... loss: 0.701183021068573\n",
      "\n",
      "batch: 1/1 in epoch 21/1000 \n",
      "... loss: 0.7006525993347168\n",
      "\n",
      "batch: 1/1 in epoch 22/1000 \n",
      "... loss: 0.7001528739929199\n",
      "\n",
      "batch: 1/1 in epoch 23/1000 \n",
      "... loss: 0.6996805667877197\n",
      "\n",
      "batch: 1/1 in epoch 24/1000 \n",
      "... loss: 0.6992322206497192\n",
      "\n",
      "batch: 1/1 in epoch 25/1000 \n",
      "... loss: 0.6988053321838379\n",
      "\n",
      "batch: 1/1 in epoch 26/1000 \n",
      "... loss: 0.6983977556228638\n",
      "\n",
      "batch: 1/1 in epoch 27/1000 \n",
      "... loss: 0.6980074644088745\n",
      "\n",
      "batch: 1/1 in epoch 28/1000 \n",
      "... loss: 0.6976325511932373\n",
      "\n",
      "batch: 1/1 in epoch 29/1000 \n",
      "... loss: 0.6972715854644775\n",
      "\n",
      "batch: 1/1 in epoch 30/1000 \n",
      "... loss: 0.6969231963157654\n",
      "\n",
      "batch: 1/1 in epoch 31/1000 \n",
      "... loss: 0.6965861320495605\n",
      "\n",
      "batch: 1/1 in epoch 32/1000 \n",
      "... loss: 0.6962596774101257\n",
      "\n",
      "batch: 1/1 in epoch 33/1000 \n",
      "... loss: 0.6959421038627625\n",
      "\n",
      "batch: 1/1 in epoch 34/1000 \n",
      "... loss: 0.695633053779602\n",
      "\n",
      "batch: 1/1 in epoch 35/1000 \n",
      "... loss: 0.6953316926956177\n",
      "\n",
      "batch: 1/1 in epoch 36/1000 \n",
      "... loss: 0.6950371265411377\n",
      "\n",
      "batch: 1/1 in epoch 37/1000 \n",
      "... loss: 0.6947488784790039\n",
      "\n",
      "batch: 1/1 in epoch 38/1000 \n",
      "... loss: 0.694466233253479\n",
      "\n",
      "batch: 1/1 in epoch 39/1000 \n",
      "... loss: 0.6941887140274048\n",
      "\n",
      "batch: 1/1 in epoch 40/1000 \n",
      "... loss: 0.693915843963623\n",
      "\n",
      "batch: 1/1 in epoch 41/1000 \n",
      "... loss: 0.693647027015686\n",
      "\n",
      "batch: 1/1 in epoch 42/1000 \n",
      "... loss: 0.693381667137146\n",
      "\n",
      "batch: 1/1 in epoch 43/1000 \n",
      "... loss: 0.6931198239326477\n",
      "\n",
      "batch: 1/1 in epoch 44/1000 \n",
      "... loss: 0.6928606033325195\n",
      "\n",
      "batch: 1/1 in epoch 45/1000 \n",
      "... loss: 0.6926040649414062\n",
      "\n",
      "batch: 1/1 in epoch 46/1000 \n",
      "... loss: 0.6923496723175049\n",
      "\n",
      "batch: 1/1 in epoch 47/1000 \n",
      "... loss: 0.6920971274375916\n",
      "\n",
      "batch: 1/1 in epoch 48/1000 \n",
      "... loss: 0.6918462514877319\n",
      "\n",
      "batch: 1/1 in epoch 49/1000 \n",
      "... loss: 0.6915966272354126\n",
      "\n",
      "batch: 1/1 in epoch 50/1000 \n",
      "... loss: 0.6913478374481201\n",
      "\n",
      "batch: 1/1 in epoch 51/1000 \n",
      "... loss: 0.6911001205444336\n",
      "\n",
      "batch: 1/1 in epoch 52/1000 \n",
      "... loss: 0.6908527612686157\n",
      "\n",
      "batch: 1/1 in epoch 53/1000 \n",
      "... loss: 0.690605640411377\n",
      "\n",
      "batch: 1/1 in epoch 54/1000 \n",
      "... loss: 0.6903586983680725\n",
      "\n",
      "batch: 1/1 in epoch 55/1000 \n",
      "... loss: 0.6901116371154785\n",
      "\n",
      "batch: 1/1 in epoch 56/1000 \n",
      "... loss: 0.6898640394210815\n",
      "\n",
      "batch: 1/1 in epoch 57/1000 \n",
      "... loss: 0.6896159648895264\n",
      "\n",
      "batch: 1/1 in epoch 58/1000 \n",
      "... loss: 0.6893671751022339\n",
      "\n",
      "batch: 1/1 in epoch 59/1000 \n",
      "... loss: 0.6891173124313354\n",
      "\n",
      "batch: 1/1 in epoch 60/1000 \n",
      "... loss: 0.6888662576675415\n",
      "\n",
      "batch: 1/1 in epoch 61/1000 \n",
      "... loss: 0.688614010810852\n",
      "\n",
      "batch: 1/1 in epoch 62/1000 \n",
      "... loss: 0.6883600950241089\n",
      "\n",
      "batch: 1/1 in epoch 63/1000 \n",
      "... loss: 0.688104510307312\n",
      "\n",
      "batch: 1/1 in epoch 64/1000 \n",
      "... loss: 0.6878470182418823\n",
      "\n",
      "batch: 1/1 in epoch 65/1000 \n",
      "... loss: 0.6875876188278198\n",
      "\n",
      "batch: 1/1 in epoch 66/1000 \n",
      "... loss: 0.6873258352279663\n",
      "\n",
      "batch: 1/1 in epoch 67/1000 \n",
      "... loss: 0.6870617866516113\n",
      "\n",
      "batch: 1/1 in epoch 68/1000 \n",
      "... loss: 0.6867948770523071\n",
      "\n",
      "batch: 1/1 in epoch 69/1000 \n",
      "... loss: 0.6865254640579224\n",
      "\n",
      "batch: 1/1 in epoch 70/1000 \n",
      "... loss: 0.6862529516220093\n",
      "\n",
      "batch: 1/1 in epoch 71/1000 \n",
      "... loss: 0.6859773397445679\n",
      "\n",
      "batch: 1/1 in epoch 72/1000 \n",
      "... loss: 0.6856984496116638\n",
      "\n",
      "batch: 1/1 in epoch 73/1000 \n",
      "... loss: 0.6854161024093628\n",
      "\n",
      "batch: 1/1 in epoch 74/1000 \n",
      "... loss: 0.6851301789283752\n",
      "\n",
      "batch: 1/1 in epoch 75/1000 \n",
      "... loss: 0.6848403215408325\n",
      "\n",
      "batch: 1/1 in epoch 76/1000 \n",
      "... loss: 0.6845464110374451\n",
      "\n",
      "batch: 1/1 in epoch 77/1000 \n",
      "... loss: 0.6842485070228577\n",
      "\n",
      "batch: 1/1 in epoch 78/1000 \n",
      "... loss: 0.6839459538459778\n",
      "\n",
      "batch: 1/1 in epoch 79/1000 \n",
      "... loss: 0.6836388111114502\n",
      "\n",
      "batch: 1/1 in epoch 80/1000 \n",
      "... loss: 0.6833270788192749\n",
      "\n",
      "batch: 1/1 in epoch 81/1000 \n",
      "... loss: 0.6830103397369385\n",
      "\n",
      "batch: 1/1 in epoch 82/1000 \n",
      "... loss: 0.6826883554458618\n",
      "\n",
      "batch: 1/1 in epoch 83/1000 \n",
      "... loss: 0.6823609471321106\n",
      "\n",
      "batch: 1/1 in epoch 84/1000 \n",
      "... loss: 0.6820279359817505\n",
      "\n",
      "batch: 1/1 in epoch 85/1000 \n",
      "... loss: 0.6816892623901367\n",
      "\n",
      "batch: 1/1 in epoch 86/1000 \n",
      "... loss: 0.6813441514968872\n",
      "\n",
      "batch: 1/1 in epoch 87/1000 \n",
      "... loss: 0.6809931993484497\n",
      "\n",
      "batch: 1/1 in epoch 88/1000 \n",
      "... loss: 0.6806354522705078\n",
      "\n",
      "batch: 1/1 in epoch 89/1000 \n",
      "... loss: 0.6802711486816406\n",
      "\n",
      "batch: 1/1 in epoch 90/1000 \n",
      "... loss: 0.6798998117446899\n",
      "\n",
      "batch: 1/1 in epoch 91/1000 \n",
      "... loss: 0.6795214414596558\n",
      "\n",
      "batch: 1/1 in epoch 92/1000 \n",
      "... loss: 0.6791352033615112\n",
      "\n",
      "batch: 1/1 in epoch 93/1000 \n",
      "... loss: 0.678741455078125\n",
      "\n",
      "batch: 1/1 in epoch 94/1000 \n",
      "... loss: 0.6783396005630493\n",
      "\n",
      "batch: 1/1 in epoch 95/1000 \n",
      "... loss: 0.6779294013977051\n",
      "\n",
      "batch: 1/1 in epoch 96/1000 \n",
      "... loss: 0.6775105595588684\n",
      "\n",
      "batch: 1/1 in epoch 97/1000 \n",
      "... loss: 0.677082896232605\n",
      "\n",
      "batch: 1/1 in epoch 98/1000 \n",
      "... loss: 0.6766461133956909\n",
      "\n",
      "batch: 1/1 in epoch 99/1000 \n",
      "... loss: 0.6761994361877441\n",
      "\n",
      "batch: 1/1 in epoch 100/1000 \n",
      "... loss: 0.6757432222366333\n",
      "\n",
      "batch: 1/1 in epoch 101/1000 \n",
      "... loss: 0.6752766370773315\n",
      "\n",
      "batch: 1/1 in epoch 102/1000 \n",
      "... loss: 0.6747997403144836\n",
      "\n",
      "batch: 1/1 in epoch 103/1000 \n",
      "... loss: 0.674311637878418\n",
      "\n",
      "batch: 1/1 in epoch 104/1000 \n",
      "... loss: 0.6738123893737793\n",
      "\n",
      "batch: 1/1 in epoch 105/1000 \n",
      "... loss: 0.6733013987541199\n",
      "\n",
      "batch: 1/1 in epoch 106/1000 \n",
      "... loss: 0.6727782487869263\n",
      "\n",
      "batch: 1/1 in epoch 107/1000 \n",
      "... loss: 0.6722428202629089\n",
      "\n",
      "batch: 1/1 in epoch 108/1000 \n",
      "... loss: 0.6716943979263306\n",
      "\n",
      "batch: 1/1 in epoch 109/1000 \n",
      "... loss: 0.6711325645446777\n",
      "\n",
      "batch: 1/1 in epoch 110/1000 \n",
      "... loss: 0.670556902885437\n",
      "\n",
      "batch: 1/1 in epoch 111/1000 \n",
      "... loss: 0.669966995716095\n",
      "\n",
      "batch: 1/1 in epoch 112/1000 \n",
      "... loss: 0.6693622469902039\n",
      "\n",
      "batch: 1/1 in epoch 113/1000 \n",
      "... loss: 0.6687420606613159\n",
      "\n",
      "batch: 1/1 in epoch 114/1000 \n",
      "... loss: 0.6681061387062073\n",
      "\n",
      "batch: 1/1 in epoch 115/1000 \n",
      "... loss: 0.6674540042877197\n",
      "\n",
      "batch: 1/1 in epoch 116/1000 \n",
      "... loss: 0.6667847037315369\n",
      "\n",
      "batch: 1/1 in epoch 117/1000 \n",
      "... loss: 0.66609787940979\n",
      "\n",
      "batch: 1/1 in epoch 118/1000 \n",
      "... loss: 0.6653929948806763\n",
      "\n",
      "batch: 1/1 in epoch 119/1000 \n",
      "... loss: 0.6646692156791687\n",
      "\n",
      "batch: 1/1 in epoch 120/1000 \n",
      "... loss: 0.6639260649681091\n",
      "\n",
      "batch: 1/1 in epoch 121/1000 \n",
      "... loss: 0.6631628274917603\n",
      "\n",
      "batch: 1/1 in epoch 122/1000 \n",
      "... loss: 0.6623785495758057\n",
      "\n",
      "batch: 1/1 in epoch 123/1000 \n",
      "... loss: 0.6615728139877319\n",
      "\n",
      "batch: 1/1 in epoch 124/1000 \n",
      "... loss: 0.6607446670532227\n",
      "\n",
      "batch: 1/1 in epoch 125/1000 \n",
      "... loss: 0.6598935723304749\n",
      "\n",
      "batch: 1/1 in epoch 126/1000 \n",
      "... loss: 0.6590183973312378\n",
      "\n",
      "batch: 1/1 in epoch 127/1000 \n",
      "... loss: 0.6581183075904846\n",
      "\n",
      "batch: 1/1 in epoch 128/1000 \n",
      "... loss: 0.6571928262710571\n",
      "\n",
      "batch: 1/1 in epoch 129/1000 \n",
      "... loss: 0.6562402844429016\n",
      "\n",
      "batch: 1/1 in epoch 130/1000 \n",
      "... loss: 0.655260443687439\n",
      "\n",
      "batch: 1/1 in epoch 131/1000 \n",
      "... loss: 0.6542519330978394\n",
      "\n",
      "batch: 1/1 in epoch 132/1000 \n",
      "... loss: 0.6532137989997864\n",
      "\n",
      "batch: 1/1 in epoch 133/1000 \n",
      "... loss: 0.6521451473236084\n",
      "\n",
      "batch: 1/1 in epoch 134/1000 \n",
      "... loss: 0.6510446071624756\n",
      "\n",
      "batch: 1/1 in epoch 135/1000 \n",
      "... loss: 0.6499111652374268\n",
      "\n",
      "batch: 1/1 in epoch 136/1000 \n",
      "... loss: 0.6487436294555664\n",
      "\n",
      "batch: 1/1 in epoch 137/1000 \n",
      "... loss: 0.6475406885147095\n",
      "\n",
      "batch: 1/1 in epoch 138/1000 \n",
      "... loss: 0.6463010311126709\n",
      "\n",
      "batch: 1/1 in epoch 139/1000 \n",
      "... loss: 0.6450233459472656\n",
      "\n",
      "batch: 1/1 in epoch 140/1000 \n",
      "... loss: 0.643706202507019\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 141/1000 \n",
      "... loss: 0.6423483490943909\n",
      "\n",
      "batch: 1/1 in epoch 142/1000 \n",
      "... loss: 0.6409481763839722\n",
      "\n",
      "batch: 1/1 in epoch 143/1000 \n",
      "... loss: 0.6395038962364197\n",
      "\n",
      "batch: 1/1 in epoch 144/1000 \n",
      "... loss: 0.6380140781402588\n",
      "\n",
      "batch: 1/1 in epoch 145/1000 \n",
      "... loss: 0.636476993560791\n",
      "\n",
      "batch: 1/1 in epoch 146/1000 \n",
      "... loss: 0.634891152381897\n",
      "\n",
      "batch: 1/1 in epoch 147/1000 \n",
      "... loss: 0.6332545280456543\n",
      "\n",
      "batch: 1/1 in epoch 148/1000 \n",
      "... loss: 0.6315650939941406\n",
      "\n",
      "batch: 1/1 in epoch 149/1000 \n",
      "... loss: 0.6298211812973022\n",
      "\n",
      "batch: 1/1 in epoch 150/1000 \n",
      "... loss: 0.6280206441879272\n",
      "\n",
      "batch: 1/1 in epoch 151/1000 \n",
      "... loss: 0.626161515712738\n",
      "\n",
      "batch: 1/1 in epoch 152/1000 \n",
      "... loss: 0.6242414116859436\n",
      "\n",
      "batch: 1/1 in epoch 153/1000 \n",
      "... loss: 0.622258186340332\n",
      "\n",
      "batch: 1/1 in epoch 154/1000 \n",
      "... loss: 0.6202096939086914\n",
      "\n",
      "batch: 1/1 in epoch 155/1000 \n",
      "... loss: 0.6180931925773621\n",
      "\n",
      "batch: 1/1 in epoch 156/1000 \n",
      "... loss: 0.6159063577651978\n",
      "\n",
      "batch: 1/1 in epoch 157/1000 \n",
      "... loss: 0.6136467456817627\n",
      "\n",
      "batch: 1/1 in epoch 158/1000 \n",
      "... loss: 0.6113117337226868\n",
      "\n",
      "batch: 1/1 in epoch 159/1000 \n",
      "... loss: 0.608898401260376\n",
      "\n",
      "batch: 1/1 in epoch 160/1000 \n",
      "... loss: 0.6064040064811707\n",
      "\n",
      "batch: 1/1 in epoch 161/1000 \n",
      "... loss: 0.6038255095481873\n",
      "\n",
      "batch: 1/1 in epoch 162/1000 \n",
      "... loss: 0.6011598706245422\n",
      "\n",
      "batch: 1/1 in epoch 163/1000 \n",
      "... loss: 0.5984044075012207\n",
      "\n",
      "batch: 1/1 in epoch 164/1000 \n",
      "... loss: 0.5955555438995361\n",
      "\n",
      "batch: 1/1 in epoch 165/1000 \n",
      "... loss: 0.592610239982605\n",
      "\n",
      "batch: 1/1 in epoch 166/1000 \n",
      "... loss: 0.5895649194717407\n",
      "\n",
      "batch: 1/1 in epoch 167/1000 \n",
      "... loss: 0.5864163637161255\n",
      "\n",
      "batch: 1/1 in epoch 168/1000 \n",
      "... loss: 0.5831610560417175\n",
      "\n",
      "batch: 1/1 in epoch 169/1000 \n",
      "... loss: 0.5797954797744751\n",
      "\n",
      "batch: 1/1 in epoch 170/1000 \n",
      "... loss: 0.576315701007843\n",
      "\n",
      "batch: 1/1 in epoch 171/1000 \n",
      "... loss: 0.5727185010910034\n",
      "\n",
      "batch: 1/1 in epoch 172/1000 \n",
      "... loss: 0.5690000057220459\n",
      "\n",
      "batch: 1/1 in epoch 173/1000 \n",
      "... loss: 0.5651562213897705\n",
      "\n",
      "batch: 1/1 in epoch 174/1000 \n",
      "... loss: 0.5611835718154907\n",
      "\n",
      "batch: 1/1 in epoch 175/1000 \n",
      "... loss: 0.5570782423019409\n",
      "\n",
      "batch: 1/1 in epoch 176/1000 \n",
      "... loss: 0.5528364777565002\n",
      "\n",
      "batch: 1/1 in epoch 177/1000 \n",
      "... loss: 0.548454761505127\n",
      "\n",
      "batch: 1/1 in epoch 178/1000 \n",
      "... loss: 0.5439288020133972\n",
      "\n",
      "batch: 1/1 in epoch 179/1000 \n",
      "... loss: 0.5392556190490723\n",
      "\n",
      "batch: 1/1 in epoch 180/1000 \n",
      "... loss: 0.5344318151473999\n",
      "\n",
      "batch: 1/1 in epoch 181/1000 \n",
      "... loss: 0.5294535756111145\n",
      "\n",
      "batch: 1/1 in epoch 182/1000 \n",
      "... loss: 0.5243179798126221\n",
      "\n",
      "batch: 1/1 in epoch 183/1000 \n",
      "... loss: 0.5190222263336182\n",
      "\n",
      "batch: 1/1 in epoch 184/1000 \n",
      "... loss: 0.5135635137557983\n",
      "\n",
      "batch: 1/1 in epoch 185/1000 \n",
      "... loss: 0.5079396963119507\n",
      "\n",
      "batch: 1/1 in epoch 186/1000 \n",
      "... loss: 0.5021488666534424\n",
      "\n",
      "batch: 1/1 in epoch 187/1000 \n",
      "... loss: 0.4961891174316406\n",
      "\n",
      "batch: 1/1 in epoch 188/1000 \n",
      "... loss: 0.4900597631931305\n",
      "\n",
      "batch: 1/1 in epoch 189/1000 \n",
      "... loss: 0.48375993967056274\n",
      "\n",
      "batch: 1/1 in epoch 190/1000 \n",
      "... loss: 0.47729015350341797\n",
      "\n",
      "batch: 1/1 in epoch 191/1000 \n",
      "... loss: 0.4706505239009857\n",
      "\n",
      "batch: 1/1 in epoch 192/1000 \n",
      "... loss: 0.4638429284095764\n",
      "\n",
      "batch: 1/1 in epoch 193/1000 \n",
      "... loss: 0.4568690359592438\n",
      "\n",
      "batch: 1/1 in epoch 194/1000 \n",
      "... loss: 0.44973188638687134\n",
      "\n",
      "batch: 1/1 in epoch 195/1000 \n",
      "... loss: 0.4424353241920471\n",
      "\n",
      "batch: 1/1 in epoch 196/1000 \n",
      "... loss: 0.4349837899208069\n",
      "\n",
      "batch: 1/1 in epoch 197/1000 \n",
      "... loss: 0.4273826479911804\n",
      "\n",
      "batch: 1/1 in epoch 198/1000 \n",
      "... loss: 0.41963857412338257\n",
      "\n",
      "batch: 1/1 in epoch 199/1000 \n",
      "... loss: 0.4117589592933655\n",
      "\n",
      "batch: 1/1 in epoch 200/1000 \n",
      "... loss: 0.4037514626979828\n",
      "\n",
      "batch: 1/1 in epoch 201/1000 \n",
      "... loss: 0.3956257700920105\n",
      "\n",
      "batch: 1/1 in epoch 202/1000 \n",
      "... loss: 0.3873916566371918\n",
      "\n",
      "batch: 1/1 in epoch 203/1000 \n",
      "... loss: 0.37906014919281006\n",
      "\n",
      "batch: 1/1 in epoch 204/1000 \n",
      "... loss: 0.37064313888549805\n",
      "\n",
      "batch: 1/1 in epoch 205/1000 \n",
      "... loss: 0.362153023481369\n",
      "\n",
      "batch: 1/1 in epoch 206/1000 \n",
      "... loss: 0.3536030054092407\n",
      "\n",
      "batch: 1/1 in epoch 207/1000 \n",
      "... loss: 0.3450074791908264\n",
      "\n",
      "batch: 1/1 in epoch 208/1000 \n",
      "... loss: 0.3363807499408722\n",
      "\n",
      "batch: 1/1 in epoch 209/1000 \n",
      "... loss: 0.3277379274368286\n",
      "\n",
      "batch: 1/1 in epoch 210/1000 \n",
      "... loss: 0.31909501552581787\n",
      "\n",
      "batch: 1/1 in epoch 211/1000 \n",
      "... loss: 0.31046754121780396\n",
      "\n",
      "batch: 1/1 in epoch 212/1000 \n",
      "... loss: 0.3018718957901001\n",
      "\n",
      "batch: 1/1 in epoch 213/1000 \n",
      "... loss: 0.2933241128921509\n",
      "\n",
      "batch: 1/1 in epoch 214/1000 \n",
      "... loss: 0.28484046459198\n",
      "\n",
      "batch: 1/1 in epoch 215/1000 \n",
      "... loss: 0.27643659710884094\n",
      "\n",
      "batch: 1/1 in epoch 216/1000 \n",
      "... loss: 0.26812803745269775\n",
      "\n",
      "batch: 1/1 in epoch 217/1000 \n",
      "... loss: 0.2599296271800995\n",
      "\n",
      "batch: 1/1 in epoch 218/1000 \n",
      "... loss: 0.25185561180114746\n",
      "\n",
      "batch: 1/1 in epoch 219/1000 \n",
      "... loss: 0.24391904473304749\n",
      "\n",
      "batch: 1/1 in epoch 220/1000 \n",
      "... loss: 0.2361319661140442\n",
      "\n",
      "batch: 1/1 in epoch 221/1000 \n",
      "... loss: 0.22850576043128967\n",
      "\n",
      "batch: 1/1 in epoch 222/1000 \n",
      "... loss: 0.22105026245117188\n",
      "\n",
      "batch: 1/1 in epoch 223/1000 \n",
      "... loss: 0.2137739062309265\n",
      "\n",
      "batch: 1/1 in epoch 224/1000 \n",
      "... loss: 0.20668429136276245\n",
      "\n",
      "batch: 1/1 in epoch 225/1000 \n",
      "... loss: 0.19978711009025574\n",
      "\n",
      "batch: 1/1 in epoch 226/1000 \n",
      "... loss: 0.19308701157569885\n",
      "\n",
      "batch: 1/1 in epoch 227/1000 \n",
      "... loss: 0.18658766150474548\n",
      "\n",
      "batch: 1/1 in epoch 228/1000 \n",
      "... loss: 0.180291086435318\n",
      "\n",
      "batch: 1/1 in epoch 229/1000 \n",
      "... loss: 0.17419832944869995\n",
      "\n",
      "batch: 1/1 in epoch 230/1000 \n",
      "... loss: 0.16830939054489136\n",
      "\n",
      "batch: 1/1 in epoch 231/1000 \n",
      "... loss: 0.1626233160495758\n",
      "\n",
      "batch: 1/1 in epoch 232/1000 \n",
      "... loss: 0.15713819861412048\n",
      "\n",
      "batch: 1/1 in epoch 233/1000 \n",
      "... loss: 0.15185147523880005\n",
      "\n",
      "batch: 1/1 in epoch 234/1000 \n",
      "... loss: 0.14675968885421753\n",
      "\n",
      "batch: 1/1 in epoch 235/1000 \n",
      "... loss: 0.1418592631816864\n",
      "\n",
      "batch: 1/1 in epoch 236/1000 \n",
      "... loss: 0.1371454894542694\n",
      "\n",
      "batch: 1/1 in epoch 237/1000 \n",
      "... loss: 0.13261377811431885\n",
      "\n",
      "batch: 1/1 in epoch 238/1000 \n",
      "... loss: 0.12825903296470642\n",
      "\n",
      "batch: 1/1 in epoch 239/1000 \n",
      "... loss: 0.12407587468624115\n",
      "\n",
      "batch: 1/1 in epoch 240/1000 \n",
      "... loss: 0.12005896866321564\n",
      "\n",
      "batch: 1/1 in epoch 241/1000 \n",
      "... loss: 0.1162024736404419\n",
      "\n",
      "batch: 1/1 in epoch 242/1000 \n",
      "... loss: 0.11250090599060059\n",
      "\n",
      "batch: 1/1 in epoch 243/1000 \n",
      "... loss: 0.10894858837127686\n",
      "\n",
      "batch: 1/1 in epoch 244/1000 \n",
      "... loss: 0.10553962737321854\n",
      "\n",
      "batch: 1/1 in epoch 245/1000 \n",
      "... loss: 0.10226882249116898\n",
      "\n",
      "batch: 1/1 in epoch 246/1000 \n",
      "... loss: 0.09913039952516556\n",
      "\n",
      "batch: 1/1 in epoch 247/1000 \n",
      "... loss: 0.09611885994672775\n",
      "\n",
      "batch: 1/1 in epoch 248/1000 \n",
      "... loss: 0.09322942793369293\n",
      "\n",
      "batch: 1/1 in epoch 249/1000 \n",
      "... loss: 0.09045636653900146\n",
      "\n",
      "batch: 1/1 in epoch 250/1000 \n",
      "... loss: 0.08779515326023102\n",
      "\n",
      "batch: 1/1 in epoch 251/1000 \n",
      "... loss: 0.08524081110954285\n",
      "\n",
      "batch: 1/1 in epoch 252/1000 \n",
      "... loss: 0.08278851211071014\n",
      "\n",
      "batch: 1/1 in epoch 253/1000 \n",
      "... loss: 0.08043397963047028\n",
      "\n",
      "batch: 1/1 in epoch 254/1000 \n",
      "... loss: 0.07817288488149643\n",
      "\n",
      "batch: 1/1 in epoch 255/1000 \n",
      "... loss: 0.07600102573633194\n",
      "\n",
      "batch: 1/1 in epoch 256/1000 \n",
      "... loss: 0.07391445338726044\n",
      "\n",
      "batch: 1/1 in epoch 257/1000 \n",
      "... loss: 0.0719093382358551\n",
      "\n",
      "batch: 1/1 in epoch 258/1000 \n",
      "... loss: 0.06998194009065628\n",
      "\n",
      "batch: 1/1 in epoch 259/1000 \n",
      "... loss: 0.06812883913516998\n",
      "\n",
      "batch: 1/1 in epoch 260/1000 \n",
      "... loss: 0.06634674966335297\n",
      "\n",
      "batch: 1/1 in epoch 261/1000 \n",
      "... loss: 0.06463239341974258\n",
      "\n",
      "batch: 1/1 in epoch 262/1000 \n",
      "... loss: 0.0629827007651329\n",
      "\n",
      "batch: 1/1 in epoch 263/1000 \n",
      "... loss: 0.06139497458934784\n",
      "\n",
      "batch: 1/1 in epoch 264/1000 \n",
      "... loss: 0.05986608564853668\n",
      "\n",
      "batch: 1/1 in epoch 265/1000 \n",
      "... loss: 0.05839372053742409\n",
      "\n",
      "batch: 1/1 in epoch 266/1000 \n",
      "... loss: 0.05697531998157501\n",
      "\n",
      "batch: 1/1 in epoch 267/1000 \n",
      "... loss: 0.05560824275016785\n",
      "\n",
      "batch: 1/1 in epoch 268/1000 \n",
      "... loss: 0.054290443658828735\n",
      "\n",
      "batch: 1/1 in epoch 269/1000 \n",
      "... loss: 0.053019747138023376\n",
      "\n",
      "batch: 1/1 in epoch 270/1000 \n",
      "... loss: 0.05179401487112045\n",
      "\n",
      "batch: 1/1 in epoch 271/1000 \n",
      "... loss: 0.05061119794845581\n",
      "\n",
      "batch: 1/1 in epoch 272/1000 \n",
      "... loss: 0.04946964234113693\n",
      "\n",
      "batch: 1/1 in epoch 273/1000 \n",
      "... loss: 0.04836726933717728\n",
      "\n",
      "batch: 1/1 in epoch 274/1000 \n",
      "... loss: 0.04730262979865074\n",
      "\n",
      "batch: 1/1 in epoch 275/1000 \n",
      "... loss: 0.04627401381731033\n",
      "\n",
      "batch: 1/1 in epoch 276/1000 \n",
      "... loss: 0.04528006166219711\n",
      "\n",
      "batch: 1/1 in epoch 277/1000 \n",
      "... loss: 0.04431898519396782\n",
      "\n",
      "batch: 1/1 in epoch 278/1000 \n",
      "... loss: 0.04338963329792023\n",
      "\n",
      "batch: 1/1 in epoch 279/1000 \n",
      "... loss: 0.04249049350619316\n",
      "\n",
      "batch: 1/1 in epoch 280/1000 \n",
      "... loss: 0.041620559990406036\n",
      "\n",
      "batch: 1/1 in epoch 281/1000 \n",
      "... loss: 0.04077842831611633\n",
      "\n",
      "batch: 1/1 in epoch 282/1000 \n",
      "... loss: 0.039963074028491974\n",
      "\n",
      "batch: 1/1 in epoch 283/1000 \n",
      "... loss: 0.039173245429992676\n",
      "\n",
      "batch: 1/1 in epoch 284/1000 \n",
      "... loss: 0.038408201187849045\n",
      "\n",
      "batch: 1/1 in epoch 285/1000 \n",
      "... loss: 0.0376666821539402\n",
      "\n",
      "batch: 1/1 in epoch 286/1000 \n",
      "... loss: 0.03694787621498108\n",
      "\n",
      "batch: 1/1 in epoch 287/1000 \n",
      "... loss: 0.03625074401497841\n",
      "\n",
      "batch: 1/1 in epoch 288/1000 \n",
      "... loss: 0.03557457774877548\n",
      "\n",
      "batch: 1/1 in epoch 289/1000 \n",
      "... loss: 0.034918561577796936\n",
      "\n",
      "batch: 1/1 in epoch 290/1000 \n",
      "... loss: 0.03428187221288681\n",
      "\n",
      "batch: 1/1 in epoch 291/1000 \n",
      "... loss: 0.03366374224424362\n",
      "\n",
      "batch: 1/1 in epoch 292/1000 \n",
      "... loss: 0.033063389360904694\n",
      "\n",
      "batch: 1/1 in epoch 293/1000 \n",
      "... loss: 0.032480388879776\n",
      "\n",
      "batch: 1/1 in epoch 294/1000 \n",
      "... loss: 0.03191390633583069\n",
      "\n",
      "batch: 1/1 in epoch 295/1000 \n",
      "... loss: 0.03136327862739563\n",
      "\n",
      "batch: 1/1 in epoch 296/1000 \n",
      "... loss: 0.030828122049570084\n",
      "\n",
      "batch: 1/1 in epoch 297/1000 \n",
      "... loss: 0.030307602137327194\n",
      "\n",
      "batch: 1/1 in epoch 298/1000 \n",
      "... loss: 0.029801446944475174\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/1 in epoch 299/1000 \n",
      "... loss: 0.029308868572115898\n",
      "\n",
      "batch: 1/1 in epoch 300/1000 \n",
      "... loss: 0.028829656541347504\n",
      "\n",
      "batch: 1/1 in epoch 301/1000 \n",
      "... loss: 0.02836325205862522\n",
      "\n",
      "batch: 1/1 in epoch 302/1000 \n",
      "... loss: 0.027909092605113983\n",
      "\n",
      "batch: 1/1 in epoch 303/1000 \n",
      "... loss: 0.0274667888879776\n",
      "\n",
      "batch: 1/1 in epoch 304/1000 \n",
      "... loss: 0.027035947889089584\n",
      "\n",
      "batch: 1/1 in epoch 305/1000 \n",
      "... loss: 0.02661624178290367\n",
      "\n",
      "batch: 1/1 in epoch 306/1000 \n",
      "... loss: 0.026207156479358673\n",
      "\n",
      "batch: 1/1 in epoch 307/1000 \n",
      "... loss: 0.025808418169617653\n",
      "\n",
      "batch: 1/1 in epoch 308/1000 \n",
      "... loss: 0.0254196897149086\n",
      "\n",
      "batch: 1/1 in epoch 309/1000 \n",
      "... loss: 0.025040578097105026\n",
      "\n",
      "batch: 1/1 in epoch 310/1000 \n",
      "... loss: 0.024670742452144623\n",
      "\n",
      "batch: 1/1 in epoch 311/1000 \n",
      "... loss: 0.024310076609253883\n",
      "\n",
      "batch: 1/1 in epoch 312/1000 \n",
      "... loss: 0.023957950994372368\n",
      "\n",
      "batch: 1/1 in epoch 313/1000 \n",
      "... loss: 0.023614434525370598\n",
      "\n",
      "batch: 1/1 in epoch 314/1000 \n",
      "... loss: 0.023279011249542236\n",
      "\n",
      "batch: 1/1 in epoch 315/1000 \n",
      "... loss: 0.022951453924179077\n",
      "\n",
      "batch: 1/1 in epoch 316/1000 \n",
      "... loss: 0.022631600499153137\n",
      "\n",
      "batch: 1/1 in epoch 317/1000 \n",
      "... loss: 0.02231910452246666\n",
      "\n",
      "batch: 1/1 in epoch 318/1000 \n",
      "... loss: 0.022013798356056213\n",
      "\n",
      "batch: 1/1 in epoch 319/1000 \n",
      "... loss: 0.021715573966503143\n",
      "\n",
      "batch: 1/1 in epoch 320/1000 \n",
      "... loss: 0.02142396941781044\n",
      "\n",
      "batch: 1/1 in epoch 321/1000 \n",
      "... loss: 0.021138932555913925\n",
      "\n",
      "batch: 1/1 in epoch 322/1000 \n",
      "... loss: 0.020860176533460617\n",
      "\n",
      "batch: 1/1 in epoch 323/1000 \n",
      "... loss: 0.020587649196386337\n",
      "\n",
      "batch: 1/1 in epoch 324/1000 \n",
      "... loss: 0.02032112330198288\n",
      "\n",
      "batch: 1/1 in epoch 325/1000 \n",
      "... loss: 0.020060308277606964\n",
      "\n",
      "batch: 1/1 in epoch 326/1000 \n",
      "... loss: 0.019805092364549637\n",
      "\n",
      "batch: 1/1 in epoch 327/1000 \n",
      "... loss: 0.01955542527139187\n",
      "\n",
      "batch: 1/1 in epoch 328/1000 \n",
      "... loss: 0.019311074167490005\n",
      "\n",
      "batch: 1/1 in epoch 329/1000 \n",
      "... loss: 0.01907169073820114\n",
      "\n",
      "batch: 1/1 in epoch 330/1000 \n",
      "... loss: 0.0188373401761055\n",
      "\n",
      "batch: 1/1 in epoch 331/1000 \n",
      "... loss: 0.018607906997203827\n",
      "\n",
      "batch: 1/1 in epoch 332/1000 \n",
      "... loss: 0.018383219838142395\n",
      "\n",
      "batch: 1/1 in epoch 333/1000 \n",
      "... loss: 0.018162989988923073\n",
      "\n",
      "batch: 1/1 in epoch 334/1000 \n",
      "... loss: 0.017947334796190262\n",
      "\n",
      "batch: 1/1 in epoch 335/1000 \n",
      "... loss: 0.01773596927523613\n",
      "\n",
      "batch: 1/1 in epoch 336/1000 \n",
      "... loss: 0.017528772354125977\n",
      "\n",
      "batch: 1/1 in epoch 337/1000 \n",
      "... loss: 0.017325755208730698\n",
      "\n",
      "batch: 1/1 in epoch 338/1000 \n",
      "... loss: 0.017126617953181267\n",
      "\n",
      "batch: 1/1 in epoch 339/1000 \n",
      "... loss: 0.016931485384702682\n",
      "\n",
      "batch: 1/1 in epoch 340/1000 \n",
      "... loss: 0.01674012653529644\n",
      "\n",
      "batch: 1/1 in epoch 341/1000 \n",
      "... loss: 0.016552306711673737\n",
      "\n",
      "batch: 1/1 in epoch 342/1000 \n",
      "... loss: 0.01636820286512375\n",
      "\n",
      "batch: 1/1 in epoch 343/1000 \n",
      "... loss: 0.016187526285648346\n",
      "\n",
      "batch: 1/1 in epoch 344/1000 \n",
      "... loss: 0.01601022109389305\n",
      "\n",
      "batch: 1/1 in epoch 345/1000 \n",
      "... loss: 0.015836283564567566\n",
      "\n",
      "batch: 1/1 in epoch 346/1000 \n",
      "... loss: 0.015665488317608833\n",
      "\n",
      "batch: 1/1 in epoch 347/1000 \n",
      "... loss: 0.015497947111725807\n",
      "\n",
      "batch: 1/1 in epoch 348/1000 \n",
      "... loss: 0.015333314426243305\n",
      "\n",
      "batch: 1/1 in epoch 349/1000 \n",
      "... loss: 0.015171706676483154\n",
      "\n",
      "batch: 1/1 in epoch 350/1000 \n",
      "... loss: 0.015013126656413078\n",
      "\n",
      "batch: 1/1 in epoch 351/1000 \n",
      "... loss: 0.014857280999422073\n",
      "\n",
      "batch: 1/1 in epoch 352/1000 \n",
      "... loss: 0.014704231172800064\n",
      "\n",
      "batch: 1/1 in epoch 353/1000 \n",
      "... loss: 0.014553859829902649\n",
      "\n",
      "batch: 1/1 in epoch 354/1000 \n",
      "... loss: 0.014406050555408001\n",
      "\n",
      "batch: 1/1 in epoch 355/1000 \n",
      "... loss: 0.014260921627283096\n",
      "\n",
      "batch: 1/1 in epoch 356/1000 \n",
      "... loss: 0.014118300750851631\n",
      "\n",
      "batch: 1/1 in epoch 357/1000 \n",
      "... loss: 0.013978126458823681\n",
      "\n",
      "batch: 1/1 in epoch 358/1000 \n",
      "... loss: 0.013840343803167343\n",
      "\n",
      "batch: 1/1 in epoch 359/1000 \n",
      "... loss: 0.013704834505915642\n",
      "\n",
      "batch: 1/1 in epoch 360/1000 \n",
      "... loss: 0.013571541756391525\n",
      "\n",
      "batch: 1/1 in epoch 361/1000 \n",
      "... loss: 0.013440700247883797\n",
      "\n",
      "batch: 1/1 in epoch 362/1000 \n",
      "... loss: 0.013311900198459625\n",
      "\n",
      "batch: 1/1 in epoch 363/1000 \n",
      "... loss: 0.013185202144086361\n",
      "\n",
      "batch: 1/1 in epoch 364/1000 \n",
      "... loss: 0.013060664758086205\n",
      "\n",
      "batch: 1/1 in epoch 365/1000 \n",
      "... loss: 0.012938052415847778\n",
      "\n",
      "batch: 1/1 in epoch 366/1000 \n",
      "... loss: 0.012817427515983582\n",
      "\n",
      "batch: 1/1 in epoch 367/1000 \n",
      "... loss: 0.012698845937848091\n",
      "\n",
      "batch: 1/1 in epoch 368/1000 \n",
      "... loss: 0.012582135386765003\n",
      "\n",
      "batch: 1/1 in epoch 369/1000 \n",
      "... loss: 0.012467117980122566\n",
      "\n",
      "batch: 1/1 in epoch 370/1000 \n",
      "... loss: 0.012354088947176933\n",
      "\n",
      "batch: 1/1 in epoch 371/1000 \n",
      "... loss: 0.012242812663316727\n",
      "\n",
      "batch: 1/1 in epoch 372/1000 \n",
      "... loss: 0.01213323324918747\n",
      "\n",
      "batch: 1/1 in epoch 373/1000 \n",
      "... loss: 0.012025347910821438\n",
      "\n",
      "batch: 1/1 in epoch 374/1000 \n",
      "... loss: 0.011918983422219753\n",
      "\n",
      "batch: 1/1 in epoch 375/1000 \n",
      "... loss: 0.011814551427960396\n",
      "\n",
      "batch: 1/1 in epoch 376/1000 \n",
      "... loss: 0.011711463332176208\n",
      "\n",
      "batch: 1/1 in epoch 377/1000 \n",
      "... loss: 0.011610073037445545\n",
      "\n",
      "batch: 1/1 in epoch 378/1000 \n",
      "... loss: 0.011510144919157028\n",
      "\n",
      "batch: 1/1 in epoch 379/1000 \n",
      "... loss: 0.011411680839955807\n",
      "\n",
      "batch: 1/1 in epoch 380/1000 \n",
      "... loss: 0.01131468079984188\n",
      "\n",
      "batch: 1/1 in epoch 381/1000 \n",
      "... loss: 0.011219203472137451\n",
      "\n",
      "batch: 1/1 in epoch 382/1000 \n",
      "... loss: 0.01112501323223114\n",
      "\n",
      "batch: 1/1 in epoch 383/1000 \n",
      "... loss: 0.0110322879627347\n",
      "\n",
      "batch: 1/1 in epoch 384/1000 \n",
      "... loss: 0.010940851643681526\n",
      "\n",
      "batch: 1/1 in epoch 385/1000 \n",
      "... loss: 0.010850703343749046\n",
      "\n",
      "batch: 1/1 in epoch 386/1000 \n",
      "... loss: 0.01076190359890461\n",
      "\n",
      "batch: 1/1 in epoch 387/1000 \n",
      "... loss: 0.01067439466714859\n",
      "\n",
      "batch: 1/1 in epoch 388/1000 \n",
      "... loss: 0.010587995871901512\n",
      "\n",
      "batch: 1/1 in epoch 389/1000 \n",
      "... loss: 0.010502889752388\n",
      "\n",
      "batch: 1/1 in epoch 390/1000 \n",
      "... loss: 0.010419013909995556\n",
      "\n",
      "batch: 1/1 in epoch 391/1000 \n",
      "... loss: 0.010336251929402351\n",
      "\n",
      "batch: 1/1 in epoch 392/1000 \n",
      "... loss: 0.010254604741930962\n",
      "\n",
      "batch: 1/1 in epoch 393/1000 \n",
      "... loss: 0.010174188762903214\n",
      "\n",
      "batch: 1/1 in epoch 394/1000 \n",
      "... loss: 0.01009482890367508\n",
      "\n",
      "batch: 1/1 in epoch 395/1000 \n",
      "... loss: 0.010016584768891335\n",
      "\n",
      "batch: 1/1 in epoch 396/1000 \n",
      "... loss: 0.009939218871295452\n",
      "\n",
      "batch: 1/1 in epoch 397/1000 \n",
      "... loss: 0.009863145649433136\n",
      "\n",
      "batch: 1/1 in epoch 398/1000 \n",
      "... loss: 0.009788010269403458\n",
      "\n",
      "batch: 1/1 in epoch 399/1000 \n",
      "... loss: 0.009713755920529366\n",
      "\n",
      "batch: 1/1 in epoch 400/1000 \n",
      "... loss: 0.009640557691454887\n",
      "\n",
      "batch: 1/1 in epoch 401/1000 \n",
      "... loss: 0.009568357840180397\n",
      "\n",
      "batch: 1/1 in epoch 402/1000 \n",
      "... loss: 0.009497098624706268\n",
      "\n",
      "batch: 1/1 in epoch 403/1000 \n",
      "... loss: 0.009426658973097801\n",
      "\n",
      "batch: 1/1 in epoch 404/1000 \n",
      "... loss: 0.009357337839901447\n",
      "\n",
      "batch: 1/1 in epoch 405/1000 \n",
      "... loss: 0.009288719855248928\n",
      "\n",
      "batch: 1/1 in epoch 406/1000 \n",
      "... loss: 0.009221101179718971\n",
      "\n",
      "batch: 1/1 in epoch 407/1000 \n",
      "... loss: 0.009154246188700199\n",
      "\n",
      "batch: 1/1 in epoch 408/1000 \n",
      "... loss: 0.009088331833481789\n",
      "\n",
      "batch: 1/1 in epoch 409/1000 \n",
      "... loss: 0.009023239836096764\n",
      "\n",
      "batch: 1/1 in epoch 410/1000 \n",
      "... loss: 0.008958970196545124\n",
      "\n",
      "batch: 1/1 in epoch 411/1000 \n",
      "... loss: 0.00889546424150467\n",
      "\n",
      "batch: 1/1 in epoch 412/1000 \n",
      "... loss: 0.008832722902297974\n",
      "\n",
      "batch: 1/1 in epoch 413/1000 \n",
      "... loss: 0.008770804852247238\n",
      "\n",
      "batch: 1/1 in epoch 414/1000 \n",
      "... loss: 0.008709651418030262\n",
      "\n",
      "batch: 1/1 in epoch 415/1000 \n",
      "... loss: 0.00864926166832447\n",
      "\n",
      "batch: 1/1 in epoch 416/1000 \n",
      "... loss: 0.008589519187808037\n",
      "\n",
      "batch: 1/1 in epoch 417/1000 \n",
      "... loss: 0.008530599996447563\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b0df69a53cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"... done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-b0df69a53cac>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(net_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mnet_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLin_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrain_loader_debug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_debug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../logs/cross_debug\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader_debug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../logs/cross_debug\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-55f73449e131>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, net, epochs, criterion, print_every, save_name, cuda, lr)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 log(\"batch: {}/{} in epoch {}/{} \\n... loss: {}\\n\".\n\u001b[1;32m     20\u001b[0m                     \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     save_name + \"_train_log\")\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# save network after every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d8a2ad879029>\u001b[0m in \u001b[0;36mlog\u001b[0;34m(summary, file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run(net_name): \n",
    "    # create variables \n",
    "    print(\"creating variables\")\n",
    "    emotion_dataset = [\"emotion_classification_1_clean.csv\", \"emotion_classification_2_clean.csv\", \"emotion_classification_3_clean.csv\", \"emotion_classification_4_clean.csv\", \"emotion_classification_5_clean.csv\", \"emotion_classification_6_clean.csv\", \"emotion_classification_7_clean.csv\", \"emotion_classification_8_clean.csv\"]\n",
    "    tweet_dataset = [\"crowdflower_clean.csv\", \"emoint_clean.csv\", \"tec_clean.csv\"]\n",
    "    act_function = torch.sigmoid\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    batch_size = 25\n",
    "    \n",
    "    if net_name == \"net_lin_emotion_full\":\n",
    "        print(\"-------- net_lin_emotion_full\")\n",
    "        net_full = Lin_Net(8, 4, 64, 64, act_function)\n",
    "        train_loader_emotion_full, test_loader_emotion_full = make_data(emotion_dataset, \"full\", batch_size)\n",
    "        plot = train(train_loader_emotion_full, net_full, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_emotion_full, net_full, criterion, 1000, \"../logs/cross_emotion_full\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"net_lin_emotion_nolex\": \n",
    "        print(\"-------- net_lin_emotion_nolex\")\n",
    "        net_half = Lin_Net(4, 4, 64, 64, act_function)\n",
    "        train_loader_emotion_nolex, test_loader_emotion_nolex = make_data(emotion_dataset, \"nolex\", batch_size)\n",
    "        plot = train(train_loader_emotion_nolex, net_half, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_emotion_nolex, net_half, criterion, 1000, \"../logs/cross_emotion_nolex\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"net_lin_emotion_lex\":\n",
    "        print(\"-------- net_lin_emotion_lex\")\n",
    "        net_half = Lin_Net(4, 4, 64, 64, act_function)\n",
    "        train_loader_emotion_lex, test_loader_emotion_lex = make_data(emotion_dataset, \"lex\", batch_size)\n",
    "        plot = train(train_loader_emotion_lex, net_half, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_emotion_lex, net_half, criterion, 1000, \"../logs/cross_emotion_lex\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"net_lin_tweet_full\":\n",
    "        print(\"-------- net_lin_tweet_full\")\n",
    "        net_full = Lin_Net(8, 4, 64, 64, act_function)\n",
    "        train_loader_tweet_full, test_loader_tweet_full = make_data(tweet_dataset, \"full\", batch_size)\n",
    "        plot = train(train_loader_tweet_full, net_full, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_tweet_full, net_full, criterion, 1000, \"../logs/cross_tweet_full\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"net_lin_tweet_nolex\":\n",
    "        print(\"-------- net_lin_tweet_nolex\")\n",
    "        net_half = Lin_Net(4, 4, 64, 64, act_function)\n",
    "        train_loader_tweet_nolex, test_loader_tweet_nolex = make_data(tweet_dataset, \"nolex\", batch_size)\n",
    "        plot = train(train_loader_tweet_nolex, net_half, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_tweet_nolex, net_half, criterion, 1000, \"../logs/net_lin_tweet_nolex\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"net_lin_tweet_lex\":\n",
    "        print(\"-------- net_lin_tweet_lex\")\n",
    "        net_half = Lin_Net(4, 4, 64, 64, act_function)\n",
    "        train_loader_tweet_lex, test_loader_tweet_lex = make_data(tweet_dataset, \"lex\", batch_size)\n",
    "        plot = train(train_loader_tweet_lex, net_half, 100, criterion, 5000, \"../logs/cross_\"+net_name, cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_tweet_lex, net_half, criterion, 1000, \"../logs/net_lin_tweet_lex\")\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    elif net_name == \"debug\":\n",
    "        # debug set\n",
    "        net_full = Lin_Net(8, 4, 64, 64, act_function)\n",
    "        train_loader_debug, test_loader_debug = make_data(emotion_dataset, \"full\", batch_size, True)\n",
    "        plot = train(train_loader_debug, net_full, 1000, criterion, 100, \"../logs/cross_debug\", cuda, 0.1)\n",
    "        plot = plt.plot([item[0] for item in plot], [item[1] for item in plot])\n",
    "        test(test_loader_debug, net_full, criterion, 100, \"../logs/cross_debug\", cuda)\n",
    "        plt.savefig(\"cross_\"+net_name+\".png\")\n",
    "    print(\"... done\")\n",
    "    \n",
    "run(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
