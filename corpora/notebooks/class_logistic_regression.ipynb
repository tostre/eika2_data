{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lex_data(dataset_name, feature_set):\n",
    "    print(\"loading lex data for\", dataset_name)\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")\n",
    "    \n",
    "    targets = dataset[\"a\"]\n",
    "    inputs = dataset[feature_set]\n",
    "    #inputs = dataset[[\"wc\", \"ewc\", \"hc\", \"ac\", \"fc\"]]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y    \n",
    "\n",
    "def load_vector_data(dataset_name):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", delimiter=\",\").astype(str)[\"a\"].tolist() \n",
    "    \n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    \n",
    "    # build model over sentences (size=dimension of word vectors), convert sentences to vectors\n",
    "    vector_model = FastText(size=32, window=3, min_count=1)\n",
    "    vector_model.build_vocab(inputs)  \n",
    "    vector_model.train(sentences=inputs, total_examples=len(inputs), total_words=vector_model.corpus_total_words, epochs=10)\n",
    "    inputs = [vector_model.wv[sample] for sample in inputs]\n",
    "    \n",
    "    # split data and return\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y    \n",
    "\n",
    "def make_topic_data(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"].tolist()\n",
    "    # remove placeholders from the stems dataset\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    dic = gs.corpora.Dictionary(dataset)\n",
    "    corpus = [dic.doc2bow(sample) for sample in dataset]\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100, chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "    \n",
    "    vecs = []\n",
    "    # for every sentence in the dataset\n",
    "    for i, sample in enumerate(dataset):\n",
    "        # get the vector-representations from the doc\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        # get the topics from the document (they are ordered by the topic ic)\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        # write the probability for every topic into a single list\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        # append the prob-vector for this sentence into the all-vectors-list\n",
    "        vecs.append(topic_vec)\n",
    "    dataset = vecs\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset, targets, test_size=0.2)\n",
    "    return dic, corpus, lda_model, train_x, test_x, train_y, test_y\n",
    "\n",
    "def classify_with_lr(train_x, test_x, train_y, test_y): \n",
    "    print(\"building lr model\")\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\")\n",
    "    print(\"... training model\")\n",
    "    lr.fit(train_x, train_y)\n",
    "    print(\"... calcularing score\")\n",
    "    pred_y = lr.predict(test_x)\n",
    "    # model metadata\n",
    "    score, f1_scoore = lr.score(train_x, train_y), f1_score(test_y, pred_y, average=\"weighted\")\n",
    "    return (test_y, pred_y, score, f1_scoore), lr.coef_ \n",
    "    \n",
    "def draw_confusion_matrix(dataset_name, feature_set_name, test_y, pred_y, score, f1_scoore): \n",
    "    fig = plt.figure()\n",
    "    hm = sn.heatmap(confusion_matrix(test_y, pred_y), fmt=\"d\", linewidth=0.5, annot=True, square=True, xticklabels=[\"h\", \"s\", \"a\", \"f\"], yticklabels=[\"h\", \"s\", \"a\", \"f\"], cmap=\"PuRd\")\n",
    "    ax1 = fig.add_axes(hm)\n",
    "    ax1.set(xlabel=\"predicted\", ylabel=\"target\")\n",
    "    desc = \"dataset: {} ({})\\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, score, f1_scoore)\n",
    "    fig.text(0.5, -0.1, desc, ha='center')\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/cm_lr_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def draw_coefficients_plot(dataset_name, feature_set_name, coefficients):\n",
    "    num_features = len(coefficients[0])\n",
    "    fig = plt.figure()\n",
    "    for i, item in enumerate(coefficients):\n",
    "        plt.plot(range(len(item)), item.T, \".\")#, label=classes[i]\n",
    "    desc = \"dataset: {} ({})\".format(dataset_name, feature_set_name)\n",
    "    fig.text(0.5, -0.05, desc, ha='center')\n",
    "    plt.xticks(range(0, num_features), features.get(feature_set_name), rotation=90)\n",
    "    plt.hlines(0, 0, num_features, linestyle=\"dotted\")\n",
    "    #plt.legend()#loc=1\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/coef_lr_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"emotion\", \"norm_emotion\", \"tweet\", \"norm_tweet\"]\n",
    "classes = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "norm_emotion_full = [\"wc\", \"ewc\", \"hc\", \"ac\", \"fc\"]\n",
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 8,\n",
    "    \"norm_emotion\": 8,\n",
    "    \"norm_test\": 8,\n",
    "    \"test\": 8\n",
    "}\n",
    "features = {\n",
    "    \"full\": [\"wc\", \"uwc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"nolex\": [\"wc\", \"uwc\", \"ewc\", \"cpc\"],\n",
    "    \"lex\": [\"hc\", \"sc\", \"ac\", \"fc\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logrec over features- and vec-data\n",
    "all_results = []\n",
    "coefficients = []\n",
    "\n",
    "for dataset in [\"norm_tweet\"]: \n",
    "    for key, feature_set in features.items(): \n",
    "        results, coef = classify_with_lr(*load_lex_data(dataset, feature_set))\n",
    "        all_results.append([dataset, key, *results])\n",
    "        coefficients.append(coef)\n",
    "    #results, coef = classify_with_lr(*load_vector_data(dataset))\n",
    "    #all_results.append([dataset, \"vec\", *results])\n",
    "    #coefficients.append(coef)\n",
    "\n",
    "for index, result in enumerate(all_results): \n",
    "    with open(\"../img/report_lr_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "        print((result[0] + \"_\" + result[1] + \" (\" + str(result[5]) + \"):\\n\" + \n",
    "          classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "    draw_coefficients_plot(result[0], result[1], coefficients[index])\n",
    "    draw_confusion_matrix(*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading topic data for norm_emotion\n"
     ]
    }
   ],
   "source": [
    "# train loistic regression over the topic distributions\n",
    "all_results = []\n",
    "coefficients = []\n",
    "for dataset_name in [\"norm_emotion\"]: \n",
    "    dic, corpus, lda_model, train_x, test_x, train_y, test_y = make_topic_data(dataset_name, num_topics_dict.get(dataset_name))\n",
    "    results, coef = classify_with_lr(train_x, test_x, train_y, test_y)\n",
    "    all_results.append([dataset_name, \"topics\", *results])\n",
    "    coefficients.append(coef)\n",
    "    \n",
    "for index, result in enumerate(all_results): \n",
    "    #with open(\"../img/report_lr_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "    #    print((result[0] + \"_\" + result[1] + \" (\" + str(result[5]) + \"):\\n\" + \n",
    "    #      classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "    # dataset_name, feature_set_name, coefficients\n",
    "    draw_confusion_matrix(*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
