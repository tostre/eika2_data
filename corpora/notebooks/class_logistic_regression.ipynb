{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_lex_data(dataset_name, feature_set):\n",
    "    print(\"loading lex data for\", dataset_name)\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", dtype = types)\n",
    "    \n",
    "    targets = dataset[\"a\"]\n",
    "    inputs = dataset[feature_set]\n",
    "    #inputs = dataset[[\"wc\", \"ewc\", \"hc\", \"ac\", \"fc\"]]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y    \n",
    "\n",
    "def load_vector_data(dataset_name, bgr=False):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist()\n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", delimiter=\",\", dtype = types).astype(str)[\"a\"].tolist() \n",
    "    vector_model = FastText.load(\"../models/word_embeddings/\" + dataset_name + \"_fasttext\")\n",
    "    # replace placeholders (\" \"), make one-string-sentences\n",
    "    print(\"... replacing placeholders\")\n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    inputs = [\" \".join(sentence) for sentence in sentences]\n",
    "    tokenized = sentences\n",
    "    if bgr:\n",
    "        bigram = Phraser.load(\"../models/bigrams/bigram_\" + dataset_name + \".pkl\")\n",
    "        bigrammed = [bigram[sentence] for sentence in sentences]\n",
    "        tokenized = bigrammed\n",
    "    inputs = [np.sum(vector_model.wv[sent], 0).tolist() if sent else np.zeros(32) for sent in tokenized]   \n",
    "    inputs = np.array(inputs)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(inputs, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y  \n",
    "\n",
    "def load_topic_data(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\", dtype = types)[\"a\"].tolist()\n",
    "    # remove placeholders from the stems dataset\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    dic = gs.corpora.Dictionary(dataset)\n",
    "    corpus = [dic.doc2bow(sample) for sample in dataset]\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore.load(\"../models/topic_models/\" + dataset_name + \"_ldamodel\")\n",
    "    \n",
    "    vecs = []\n",
    "    # for every sentence in the dataset make topic vectors\n",
    "    for i, sample in enumerate(dataset):\n",
    "        # get the vector-representations from the doc\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        # get the topics from the document (they are ordered by the topic ic)\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        # write the probability for every topic into a single list\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        # append the prob-vector for this sentence into the all-vectors-list\n",
    "        vecs.append(topic_vec)\n",
    "    dataset = vecs\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset, targets, test_size=0.2)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def classify_with_lr(dataset_name, feature_set_name, train_x, test_x, train_y, test_y): \n",
    "    print(\"building lr model\")\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\", tol=0.0001, n_jobs=-1)\n",
    "    print(\"... training model\")\n",
    "    lr.fit(train_x, train_y)\n",
    "    print(\"... calcularing score\")\n",
    "    pred_y = lr.predict(test_x)\n",
    "    # model metadata\n",
    "    score, f1_scoore = lr.score(train_x, train_y), f1_score(test_y, pred_y, average=\"weighted\")\n",
    "    dump(lr, \"../models/logistic_regression/\" + dataset_name + \"_\" + feature_set_name + \"_logistic_regression.joblib\") \n",
    "    return (test_y, pred_y, score, f1_scoore), lr.coef_ \n",
    "    \n",
    "def draw_confusion_matrix(dataset_name, feature_set_name, test_y, pred_y, score, f1_scoore, num_topics=None): \n",
    "    print(feature_set_name)\n",
    "    fig = plt.figure()\n",
    "    hm = sn.heatmap(confusion_matrix(test_y, pred_y), fmt=\"d\", linewidth=0.5, annot=True, square=True, xticklabels=[\"h\", \"s\", \"a\", \"f\"], yticklabels=[\"h\", \"s\", \"a\", \"f\"], cmap=\"PuRd\")\n",
    "    ax1 = fig.add_axes(hm)\n",
    "    ax1.set(xlabel=\"predicted\", ylabel=\"target\")\n",
    "    if feature_set_name == \"topics\": desc = \"dataset: {} ({}), trained with {} topics\\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, num_topics, round(score,2), round(f1_scoore,2))\n",
    "    else: desc = \"dataset: {} ({}) \\nscore: {}, f1_score: {}\".format(dataset_name, feature_set_name, round(score,2), round(f1_scoore,2)) \n",
    "    fig.text(0.5, -0.1, desc, ha='center')\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/cm_lr_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def draw_coefficients_plot(dataset_name, feature_set_name, coefficients):\n",
    "    num_features = len(coefficients[0])\n",
    "    fig = plt.figure()\n",
    "    for i, item in enumerate(coefficients):\n",
    "        plt.plot(range(len(item)), item.T, \".\")#, label=classes[i]\n",
    "    desc = \"dataset: {} ({})\".format(dataset_name, feature_set_name)\n",
    "    fig.text(0.5, -0.05, desc, ha='center')\n",
    "    plt.xticks(range(0, num_features), features.get(feature_set_name), rotation=90)\n",
    "    plt.hlines(0, 0, num_features, linestyle=\"dotted\")\n",
    "    #plt.legend()#loc=1\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/coef_lr_\" + dataset_name + \"_\" + feature_set_name + \".png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"emotion\", \"norm_emotion\", \"tweet\", \"norm_tweet\"]\n",
    "classes = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "norm_emotion_full = [\"wc\", \"ewc\", \"hc\", \"ac\", \"fc\"]\n",
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 79,\n",
    "    \"norm_emotion\": 186,\n",
    "    \"test\": 10\n",
    "}\n",
    "features = {\n",
    "    \"full\": [\"wc\", \"uwc\", \"ewc\", \"cpc\", \"hc\", \"sc\", \"ac\", \"fc\"],\n",
    "    \"nolex\": [\"wc\", \"uwc\", \"ewc\", \"cpc\"],\n",
    "    \"lex\": [\"hc\", \"sc\", \"ac\", \"fc\"]\n",
    "}\n",
    "types = {\n",
    "    \"text\": object, \n",
    "    \"a\": int, \n",
    "    \"wc\": float,\n",
    "    \"uwc\": float,\n",
    "    \"ewc\": float,\n",
    "    \"cpc\": float,\n",
    "    \"hc\": float,\n",
    "    \"sc\": float,\n",
    "    \"ac\": float,\n",
    "    \"c\": float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lex data for norm_tweet\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_tweet\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_tweet\n",
      "... replacing placeholders\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_tweet\n",
      "... replacing placeholders\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading topic data for norm_tweet\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_emotion\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_emotion\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading lex data for norm_emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcel/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_emotion\n",
      "... replacing placeholders\n",
      "building lr model\n",
      "... training model\n",
      "... calcularing score\n",
      "loading vector data for norm_emotion\n",
      "... replacing placeholders\n"
     ]
    }
   ],
   "source": [
    "#train logrec over features\n",
    "all_results = []\n",
    "coefficients = []\n",
    "topics = []\n",
    "\n",
    "for dataset in [\"norm_tweet\", \"norm_emotion\"]:\n",
    "    topics.append(num_topics_dict[dataset])\n",
    "    # lex datasets\n",
    "    for key, feature_set in features.items(): \n",
    "        results, coef = classify_with_lr(dataset, key, *load_lex_data(dataset, feature_set))\n",
    "        all_results.append([dataset, key, *results])\n",
    "        coefficients.append(coef)\n",
    "    # unigram dataset\n",
    "    results, coef = classify_with_lr(dataset, \"vec-unigram\", *load_vector_data(dataset))\n",
    "    all_results.append([dataset, \"vec-unigram\", *results])\n",
    "    coefficients.append(coef)\n",
    "    # bigram dataset\n",
    "    results, coef = classify_with_lr(dataset,  \"vec-bigram\", *load_vector_data(dataset, True))\n",
    "    all_results.append([dataset, \"vec-bigram\", *results])\n",
    "    coefficients.append(coef)\n",
    "    # topic dataset\n",
    "    results, coef = classify_with_lr(dataset, \"topics\", *load_topic_data(dataset, num_topics_dict[dataset]))\n",
    "    all_results.append([dataset, \"topics\", *results])\n",
    "    coefficients.append(coef)\n",
    "\n",
    "for index, result in enumerate(all_results): \n",
    "    with open(\"../reports/report_lr_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "        print((result[0] + \"(\" + result[1] + \"), f1_score: \" + str(result[5]) + \"):\\n\\n\" + \n",
    "          classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "    draw_coefficients_plot(result[0], result[1], coefficients[index])\n",
    "    draw_confusion_matrix(*result, num_topics_dict[result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
