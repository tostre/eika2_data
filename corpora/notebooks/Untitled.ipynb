{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips: https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565\n",
    "# https://miguel-data-sc.github.io/2017-11-05-first/\n",
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(D.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = torch.from_numpy(x_tensor)\n",
    "        self.y = torch.from_numpy(y_tensor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def make_data(dataset, features, batch_size, debug=False):\n",
    "    datasets = []\n",
    "    for file in dataset:\n",
    "        datasets.append(pd.read_csv(\"../\" + file))\n",
    "    dataset = pd.concat(datasets, axis=0, ignore_index=True)\n",
    "    # make train and test sets\n",
    "    target = dataset[\"affect\"]\n",
    "    tmp_x, test_x, tmp_y, test_y = train_test_split(dataset[features], target, test_size=0.2)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(tmp_x, tmp_y, test_size=0.2)\n",
    "    # make data loaders\n",
    "    train_data = MyDataset(train_x.to_numpy(), train_y.to_numpy())\n",
    "    val_data = MyDataset(val_x.to_numpy(), val_y.to_numpy())\n",
    "    test_data = MyDataset(test_x.to_numpy(), test_y.to_numpy())\n",
    "    train_loader = DataLoader(train_data, batch_size)\n",
    "    val_loader = DataLoader(val_data, batch_size)\n",
    "    test_loader = DataLoader(test_data, 1)\n",
    "    return (train_loader, val_loader, test_loader)\n",
    "\n",
    "def log(net, summary, epoch, train_loss, val_loss, save_name):\n",
    "    # print diagram\n",
    "    if \"train\" in save_name:\n",
    "        plt.clf()\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.plot(train_loss, label=\"train_loss\")\n",
    "        plt.plot(val_loss, label=\"test_loss\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(\"../img/\" + save_name + str(epoch) + \".png\")\n",
    "    # write in log\n",
    "    log = open(\"../logs/\" + save_name + \".txt\", \"a\")\n",
    "    log.write(summary)\n",
    "    log.close()\n",
    "    print(summary)\n",
    "    # save network\n",
    "    torch.save(net.state_dict(), \"../nets/\" + save_name + str(epoch) + \".pt\")  \n",
    "    \n",
    "def update_lr(loss_diff=0, init=False):\n",
    "\tif init: return 0.1\n",
    "\t\n",
    "\t# if loss between x und x: return y\n",
    "\t# if loss between z and z: return w\n",
    "\t\n",
    "def use_gpu(cuda, net, inputs, targets):\n",
    "\tif cuda:\n",
    "\t\tnet = net.cuda()\n",
    "\t\tinputs = inputs.cuda()\n",
    "\t\ttargets = targets.cuda()\n",
    "\treturn net, inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Net definition #####\n",
    "\n",
    "class Lin_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(Lin_Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.lin1(x))\n",
    "        x = torch.sigmoid(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loader, cuda, criterion, optimizer, scheduler):\n",
    "    net.train()\n",
    "    for batch, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.float(), targets.view(targets.shape[0],1).float()\n",
    "        net, inputs, targets = use_gpu(cuda, net, inputs, targets)\n",
    "        pred = net(inputs)\n",
    "        loss = criterion(net(inputs), targets)\n",
    "        # backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # give update\n",
    "        if (batch % 100) == 0:\n",
    "            print(\"... train batch {}/{}, lr:{}\".format(batch, len(loader), lr))\n",
    "    return net, loss.item()\n",
    "    \n",
    "def val(net, loader, cuda, criterion):\n",
    "    net.eval()\n",
    "    for batch, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.float(), targets.float().view(targets.shape[0],1)\n",
    "        net, inputs, targets = use_gpu(cuda, net, inputs, targets)\n",
    "        loss = criterion(net(inputs), targets)\n",
    "        # give update\n",
    "        if (batch % 100) == 0:\n",
    "            print(\"...... val batch {}/{}\".format(batch, len(loader)))\n",
    "    return net, loss.item()\n",
    "\n",
    "def test(net, loader, cuda, criterion):\n",
    "    net.eval()\n",
    "    loss_sum, correct, correct2 = 0, 0, 0\n",
    "    confusion = []\n",
    "    for batch, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        net, inputs, targets = use_gpu(cuda, net, inputs, targets)\n",
    "        pred = net(inputs)\n",
    "        pred_class = round(pred.item())\n",
    "        loss_sum += criterion(pred, targets).item()\n",
    "        confusion.append([targets.item(), pred_class])\n",
    "        # give update\n",
    "        if (batch % 100 == 0):\n",
    "            print(\"...... test batch {}/{}\".format(batch, len(loader)))\n",
    "    correct = sum((pair.count(pair[0]) == len(pair)) for pair in confusion)\n",
    "    # return avg loss, avg correct, confusion\n",
    "    return (loss_sum / len(test_loader)), (correct / len(test_loader)), confusion\n",
    "\n",
    "\n",
    "def fit(net, data, cuda, criterion, epochs, log_every, save_name):\n",
    "    open(\"../logs/\" + save_name + \"_train.txt\", \"w\").close()\n",
    "    open(\"../logs/\" + save_name + \"_test.txt\", \"w\").close()\n",
    "    optimizer = optim.SGD(net.parameters(), 0.1, momentum=0.5)\n",
    "    #scheduler = optim.lr_scheduler.LambdaLR(optimizer, len(data[0]))\n",
    "    scheduler = None\n",
    "    running_train_loss, running_val_loss = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #scheduler.step()\n",
    "        net, train_loss = train(net, data[0], cuda, criterion, optimizer, scheduler)\n",
    "        running_train_loss.append(train_loss)\n",
    "        net, val_loss =  val(net, data[1], cuda, criterion)\n",
    "        running_val_loss.append(val_loss)\n",
    "        \n",
    "        if (epoch % log_every[0]) == 0:\n",
    "            summary = \"{}: epoch {}/{}:\\t train_loss: {}\\t val_loss: {}\".format(datetime.datetime.now(), epoch, epochs, train_loss, val_loss)\n",
    "            log(net, summary, epoch, running_train_loss, running_val_loss, save_name + \"_train\")\n",
    "    \n",
    "    avg_loss, avg_correct, confusion = test(net, data[2], cuda, criterion)\n",
    "    summary = \"{}: avg_loss: {} avg_correct: {}\\n\\n{}\".format(datetime.datetime.now(), avg_loss, avg_correct, confusion)\n",
    "    log(net, summary, epoch, running_train_loss, running_val_loss, save_name + \"_test\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables\n",
      "... done\n"
     ]
    }
   ],
   "source": [
    "# create variables \n",
    "print(\"creating variables\")\n",
    "emotion_dataset = [\"emotion_classification_1_clean.csv\", \"emotion_classification_2_clean.csv\", \"emotion_classification_3_clean.csv\", \"emotion_classification_4_clean.csv\", \"emotion_classification_5_clean.csv\", \"emotion_classification_6_clean.csv\", \"emotion_classification_7_clean.csv\", \"emotion_classification_8_clean.csv\"]\n",
    "tweet_dataset = [\"crowdflower_clean.csv\", \"emoint_clean.csv\", \"tec_clean.csv\"]\n",
    "full_features = [\"word_count\", \"upper_word_count\", \"ent_word_count\", \"h_count\", \"s_count\", \"a_count\", \"f_count\", \"cons_punct_count\"]\n",
    "nolex_features = [\"word_count\", \"upper_word_count\", \"ent_word_count\", \"cons_punct_count\"]\n",
    "lex_features = [\"h_count\", \"s_count\", \"a_count\", \"f_count\"]\n",
    "log_every = [50, 2500] # epochs and batches\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "cuda = torch.cuda.is_available()\n",
    "criterion = nn.MSELoss()\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- net_lin_emotion_full\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1fb3861fab33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLin_Net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lin_emotion_full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-938e3e7c5c5d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(net, data, cuda, criterion, epochs, log_every, save_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mrunning_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-938e3e7c5c5d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, loader, cuda, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# give update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"... train batch {}/{}, lr:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"-------- net_lin_emotion_full\")\n",
    "net = Lin_Net(8, 1, 64)\n",
    "data = make_data(tweet_dataset, full_features, batch_size)\n",
    "fit(net, data, cuda, criterion, epochs, log_every, \"lin_emotion_full\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"-------- net_lin_emotion_full\")\n",
    "net_full = Lin_Net(8, 1, 64)\n",
    "train_loader_emotion_full, test_loader_emotion_full = make_data(emotion_dataset, \"full\", batch_size)\n",
    "train(train_loader_emotion_full, net_full, epochs, print_every, \"mse_emotion_full\", cuda)\n",
    "#test(test_loader_emotion_full, net_full, print_every_epoch0, \"mse_emotion_full\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_nolex\")\n",
    "net_half = Lin_Net(4, 1, 64)\n",
    "train_loader_emotion_nolex, test_loader_emotion_nolex = make_data(emotion_dataset, \"nolex\", batch_size)\n",
    "train(train_loader_emotion_nolex, net_half, epochs, print_every, \"mse_emotion_nolex\", cuda)\n",
    "#test(test_loader_emotion_nolex, net_half, print_every_epoch0, \"mse_emotion_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_emotion_lex\")\n",
    "net_half = Lin_Net(4, 1, 64)\n",
    "train_loader_emotion_lex, test_loader_emotion_lex = make_data(emotion_dataset, \"lex\", batch_size)\n",
    "train(train_loader_emotion_lex, net_half, epochs, print_every, \"mse_emotion_lex\", cuda)\n",
    "#test(test_loader_emotion_lex, net_half, print_every_epoch0, \"mse_emotion_lex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_full\")\n",
    "net_full = Lin_Net(8, 1, 64)\n",
    "train_loader_tweet_full, test_loader_tweet_full = make_data(tweet_dataset, \"full\", batch_size)\n",
    "train(train_loader_tweet_full, net_full, epochs, print_every, \"mse_tweet_full\", cuda)\n",
    "#test(test_loader_tweet_full, net_full, print_every_epoch0, \"mse_tweet_full\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_nolex\")\n",
    "net_half = Lin_Net(4, 1, 64)\n",
    "train_loader_tweet_nolex, test_loader_tweet_nolex = make_data(tweet_dataset, \"nolex\", batch_size)\n",
    "train(train_loader_tweet_nolex, net_half, epochs, print_every, \"mse_tweet_nolex\", cuda)\n",
    "#test(test_loader_tweet_nolex, net_half, print_every_epoch0, \"mse_tweet_nolex\")\n",
    "\n",
    "print(\"-------- net_lin_tweet_lex\")\n",
    "net_half = Lin_Net(4, 1, 64)\n",
    "train_loader_tweet_lex, test_loader_tweet_lex = make_data(tweet_dataset, \"lex\", batch_size)\n",
    "train(train_loader_tweet_lex, net_half, epochs, print_every, \"mse_tweet_lex\", cuda)\n",
    "#test(test_loader_tweet_lex, net_half, print_every_epoch0, \"mse_tweet_lex\")\n",
    "print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
