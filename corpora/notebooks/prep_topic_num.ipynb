{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_data(dataset_name):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist() \n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    #sentences_whole = [\" \".join(sentence) for sentence in sentences_split]\n",
    "    #tokens = [token for sentence in sentences_split for token in sentence]\n",
    "    dic = gs.corpora.Dictionary(sentences)\n",
    "    corpus = [dic.doc2bow(sample) for sample in sentences]\n",
    "    #print(\"--- sentences_split: \\n\", sentences_split, \"\\n\")\n",
    "    #print(\"--- sentences_whole: \\n\", sentences_whole, \"\\n\")\n",
    "    #print(\"--- tokens: \\n\", tokens, \"\\n\")\n",
    "    return sentences, dic, corpus\n",
    "\n",
    "def visualize_lda(model, corpus, dic):\n",
    "    pvis.enable_notebook()\n",
    "    vis = pvis.gensim.prepare(model, corpus, dic)\n",
    "    vis.show()\n",
    "\n",
    "def get_coherence_score(model, sentences, dic):\n",
    "    # the higher the better it is, nutzen um versch. modelle zu vergleichen (mit untersch. topic-anzah√∂)\n",
    "    coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=model, texts=sentences, dictionary=dic, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    #print('\\nCoherence Score: ', coherence_lda)\n",
    "    return coherence_score\n",
    "\n",
    "def draw_plot(dataset_name, x, y, best_coherence, best_num_topics):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.set(xlabel=\"num_topics\", ylabel=\"coherence\")\n",
    "    desc = \"dataset: {}\\nbest coherence: {}, with topics: {}\".format(dataset_name, best_coherence, best_num_topics)\n",
    "    fig.text(0.5, -0.07, desc, ha='center')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/num_topics_\" + dataset_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def find_best_topic_num(dataset_name, lim_low, lim_high):\n",
    "    coherences = []\n",
    "    models = []\n",
    "    sentences, dic, corpus = make_data(dataset_name)\n",
    "    for i in range(lim_low, lim_high+1):\n",
    "        print(dataset_name + \"... loop {} / {}\".format(i, lim_high))\n",
    "        #lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dic, num_topics=i, random_state=100,\n",
    "        #                               update_every=1, chunksize=100, passes=10, per_word_topics=True)\n",
    "        lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=i, random_state=100,\n",
    "                                       chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "        models.append(lda_model)\n",
    "        coherences.append(get_coherence_score(lda_model, sentences, dic))\n",
    "    max_coherence_index = coherences.index(max(coherences))\n",
    "    draw_plot(dataset_name, rst(ange(lim_low, len(coherences)+lim_low)), coherences, max(coherences), max_coherence_index+lim_low)\n",
    "    models[max_coherence_index].save(\"../models/tm_\" + dataset_name + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"test\"]\n",
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 8,\n",
    "    \"norm_emotion\": 8,\n",
    "    \"norm_test\": 8,\n",
    "    \"test\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vector data for norm_tweet\n"
     ]
    }
   ],
   "source": [
    "# find the optimal number of topics for each dataset\n",
    "for dataset_name in datasets: \n",
    "    find_best_topic_num(dataset_name, 5, 8)\n",
    "sentences, dic, corpus = make_topic_data(\"norm_tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#topics = lda_model.show_topics()\n",
    "#for topic in topics[:5]: \n",
    "#    print(topic)\n",
    "\n",
    "#print(lda_model.get_topic_terms(topicid=1, topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 topics\n",
    "#print(lda_model.show_topics(num_topics=10))\n",
    "# show all tokens that are part of a topic (only the top 2 words)\n",
    "#print(lda_model.get_topic_terms(topicid=0, topn=2))\n",
    "# get the word for a id in the dic\n",
    "#print(id2word[143])\n",
    "# print corpus\n",
    "#print(corpus[:1])\n",
    "# print corpus but with names, not with ids\n",
    "#print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "#visualize_lda(lda_model, corpus, dic)\n",
    "#lda_model.save(\"location.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pvis.enable_notebook()\n",
    "#vis = pvis.gensim.prepare(lda_model, corpus, dic)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_doc = [dic.doc2bow(sample) for sample in sentences_split[:1]]\n",
    "# ein neues doc muss ein satz sein (also eine liste)\n",
    "#new_doc2 = dic.doc2bow(*sentences_split[:1])\n",
    "#print(new_doc)\n",
    "#print(new_doc2)\n",
    "# get topics from a new document (fremd am besten)\n",
    "#top = lda_model.get_document_topics(new_doc, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "#top2 = lda_model.get_document_topics(new_doc2, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "# zeige alle topics in dem document\n",
    "#for i, x in enumerate(top):\n",
    "#    print(x)\n",
    "#for i, x in enumerate(top2):\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
