{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.models\n",
    "import gensim.corpora\n",
    "import gensim as gs\n",
    "import pyLDAvis as pvis\n",
    "import pyLDAvis.gensim\n",
    "import gensim.models.coherencemodel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_data(dataset_name):\n",
    "    print(\"loading vector data for\", dataset_name)\n",
    "    sentences = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\", delimiter=\",\").astype(str).values.tolist() \n",
    "    for index, sample in enumerate(sentences): \n",
    "            sentences[index] = list(filter((\" \").__ne__, sample))\n",
    "    #sentences_whole = [\" \".join(sentence) for sentence in sentences_split]\n",
    "    #tokens = [token for sentence in sentences_split for token in sentence]\n",
    "    dic = gs.corpora.Dictionary(sentences)\n",
    "    corpus = [dic.doc2bow(sample) for sample in sentences]\n",
    "    #print(\"--- sentences_split: \\n\", sentences_split, \"\\n\")\n",
    "    #print(\"--- sentences_whole: \\n\", sentences_whole, \"\\n\")\n",
    "    #print(\"--- tokens: \\n\", tokens, \"\\n\")\n",
    "    return sentences, dic, corpus\n",
    "\n",
    "def visualize_lda(model, corpus, dic):\n",
    "    pvis.enable_notebook()\n",
    "    vis = pvis.gensim.prepare(model, corpus, dic)\n",
    "    vis.show()\n",
    "\n",
    "def get_coherence_score(model, sentences, dic):\n",
    "    # the higher the better it is, nutzen um versch. modelle zu vergleichen (mit untersch. topic-anzah√∂)\n",
    "    coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=model, texts=sentences, dictionary=dic, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    #print('\\nCoherence Score: ', coherence_lda)\n",
    "    return coherence_score\n",
    "\n",
    "def draw_plot(dataset_name, x, y, best_coherence, best_num_topics):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.set(xlabel=\"num_topics\", ylabel=\"coherence\")\n",
    "    desc = \"dataset: {}\\nbest coherence: {}, with topics: {}\".format(dataset_name, best_coherence, best_num_topics)\n",
    "    fig.text(0.5, -0.07, desc, ha='center')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(\"../img/num_topics_\" + dataset_name + \".png\", bbox_inches=\"tight\")\n",
    "    \n",
    "def find_best_topic_num(dataset_name, lim_low, lim_high):\n",
    "    coherences = []\n",
    "    models = []\n",
    "    sentences, dic, corpus = make_data(dataset_name)\n",
    "    for i in range(lim_low, lim_high+1):\n",
    "        print(dataset_name + \"... loop {} / {}\".format(i, lim_high))\n",
    "        #lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dic, num_topics=i, random_state=100,\n",
    "        #                               update_every=1, chunksize=100, passes=10, per_word_topics=True)\n",
    "        lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=i, random_state=100,\n",
    "                                       chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "        models.append(lda_model)\n",
    "        coherences.append(get_coherence_score(lda_model, sentences, dic))\n",
    "    max_coherence_index = coherences.index(max(coherences))\n",
    "    draw_plot(dataset_name, rst(ange(lim_low, len(coherences)+lim_low)), coherences, max(coherences), max_coherence_index+lim_low)\n",
    "    models[max_coherence_index].save(\"../models/tm_\" + dataset_name + \".model\")\n",
    "\n",
    "def classify_with_lr(train_x, test_x, train_y, test_y): \n",
    "    print(\"building lr model\")\n",
    "    lr = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\")\n",
    "    print(\"... training model\")\n",
    "    lr.fit(train_x, train_y)\n",
    "    print(\"... calcularing score\")\n",
    "    pred_y = lr.predict(test_x)\n",
    "    # model metadata\n",
    "    score, f1_scoore = lr.score(train_x, train_y), f1_score(test_y, pred_y, average=\"weighted\")\n",
    "    return (test_y, pred_y, score, f1_scoore), lr.coef_ \n",
    "\n",
    "def make_topic_data(dataset_name, num_topics):\n",
    "    print(\"loading topic data for\", dataset_name)\n",
    "    # load inputs and labels\n",
    "    dataset = pd.read_csv(\"../cleaned/\" + dataset_name + \"_stems.csv\").astype(str).values.tolist() \n",
    "    targets = pd.read_csv(\"../cleaned/\" + dataset_name + \"_clean.csv\")[\"a\"].tolist()\n",
    "    # remove placeholders from the stems dataset\n",
    "    for index, sample in enumerate(dataset): \n",
    "            dataset[index] = list(filter((\" \").__ne__, sample))\n",
    "    # create dic, copora and lda-model\n",
    "    dic = gs.corpora.Dictionary(sentences)\n",
    "    corpus = [dic.doc2bow(sample) for sample in sentences]\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100,\n",
    "                                           chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n",
    "    \n",
    "    vecs = []\n",
    "    # for every sentence in the dataset\n",
    "    for i, sample in enumerate(dataset):\n",
    "        # get the vector-representations from the doc\n",
    "        sentence = dic.doc2bow(dataset[i])\n",
    "        # get the topics from the document (they are ordered by the topic ic)\n",
    "        topics = lda_model.get_document_topics(sentence, minimum_probability=0.0)\n",
    "        # write the probability for every topic into a single list\n",
    "        topic_vec = [topics[i][1] for i in range(num_topics)] \n",
    "        # append the prob-vector for this sentence into the all-vectors-list\n",
    "        vecs.append(topic_vec)\n",
    "    dataset = vecs\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset, targets, test_size=0.2)\n",
    "    return dic, corpus, lda_model, train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"test\"]\n",
    "num_topics_dict = {\n",
    "    \"norm_tweet\": 8,\n",
    "    \"norm_emotion\": 8,\n",
    "    \"norm_test\": 8,\n",
    "    \"test\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vector data for norm_tweet\n"
     ]
    }
   ],
   "source": [
    "# find the optimal number of topics for each dataset\n",
    "for dataset_name in datasets: \n",
    "    find_best_topic_num(dataset_name, 5, 8)\n",
    "sentences, dic, corpus = make_data(\"norm_tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "loading topic data for test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-51:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/home/marcel/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/marcel/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/marcel/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 334, in worker_e_step\n",
      "    chunk_no, chunk, worker_lda = input_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 93, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c84155305027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_topic_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-17d226c4dc07>\u001b[0m in \u001b[0;36mmake_topic_data\u001b[0;34m(dataset_name, num_topics)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus, id2word=dic, num_topics=num_topics, random_state=100,\n\u001b[0;32m---> 73\u001b[0;31m                                            chunksize=100, passes=10, per_word_topics=True)#update_every=1, \n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training LDA model using %i processes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[0;34m(self, chunk, total_docs)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mperwordbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         logger.info(\n\u001b[1;32m    823\u001b[0m             \u001b[0;34m\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mbound\u001b[0;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# E[log p(beta | eta) - log q (beta | lambda)]; assumes eta is a scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_lambda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mElogbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lambda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# train loistic regression over the topic distributions\n",
    "all_results = []\n",
    "coefficients = []\n",
    "for dataset_name in datasets: \n",
    "    dic, corpus, lda_model, train_x, test_x, train_y, test_y = make_topic_data(\"test\", num_topics_dict.get(dataset_name))\n",
    "    results, coef = classify_with_lr(*load_lex_data(dataset, feature_set))\n",
    "    all_results.append([dataset_name, *results])\n",
    "    coefficients.append(coef)\n",
    "    \n",
    "# print reports, make graphics\n",
    "for index, result in enumerate(all_results): \n",
    "    with open(\"../img/report_lr_\" + result[0] + \"_\"  + result[1] + \".txt\", 'w') as f:\n",
    "        print((result[0] + \"_\" + result[1] + \" (\" + str(result[5]) + \"):\\n\" + \n",
    "          classification_report(result[2], result[3],target_names=classes)), file=f)\n",
    "    draw_coefficients_plot(result[0], re\n",
    "\n",
    "#topics = lda_model.show_topics()\n",
    "#for topic in topics[:5]: \n",
    "#    print(topic)\n",
    "\n",
    "#print(lda_model.get_topic_terms(topicid=1, topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.003907854), (1, 0.003907854), (2, 0.003907854), (3, 0.003907854), (4, 0.003907854), (5, 0.003907854), (6, 0.003907854), (7, 0.003907854), (8, 0.003907854), (9, 0.003907854), (10, 0.003907854), (11, 0.87885654), (12, 0.003907854), (13, 0.003907854), (14, 0.003907854), (15, 0.003907854), (16, 0.003907854), (17, 0.003907854), (18, 0.003907854), (19, 0.003907854), (20, 0.003907854), (21, 0.003907854), (22, 0.003907854), (23, 0.003907854), (24, 0.003907854), (25, 0.003907854), (26, 0.003907854), (27, 0.003907854), (28, 0.003907854), (29, 0.003907854), (30, 0.003907854), (31, 0.003907854)]\n",
      "[(746, 0.1665889), (604, 0.078676336), (628, 0.062612906), (556, 0.046343762), (40, 0.03341817), (1532, 0.029774545), (2371, 0.02411643), (2297, 0.022226531), (962, 0.02126315), (2056, 0.019251674)]\n"
     ]
    }
   ],
   "source": [
    "# Converting Topics to Feature Vectors\n",
    "#new_doc = [dic.doc2bow(sample) for sample in sentences_split[:1]]\n",
    "# ein neues doc muss ein satz sein (also eine liste)\n",
    "train_vecs = []\n",
    "for i, sentence in enumerate(sentences[:1]):\n",
    "    #print(sentences[i])\n",
    "    new_doc = dic.doc2bow(sentences[i])\n",
    "    top_topics = lda_model.get_document_topics(new_doc, minimum_probability=0.0)\n",
    "    print(top_topics)\n",
    "    print(lda_model.get_topic_terms(0))\n",
    "    topic_vec = [top_topics[i][1] for i in range(20)] # das hier anpassen\n",
    "    #topic_vec.extend([rev_train.iloc[i].real_counts]) # counts of reviews for restaurant\n",
    "    #topic_vec.extend([len(rev_train.iloc[i].text)]) # length review\n",
    "    train_vecs.append(topic_vec)\n",
    "    #print(topic_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainings, testdaten speichern\n",
    "# f√ºr jeden datensatz ein model speichern\n",
    "    # iterieren mit unterschiedlicher topic-anzahl\n",
    "    \n",
    "# gucken welche topic-ids mit den vier Klassen √ºbereinstimmen\n",
    "    # vlt die w√∂rter der klasse in den lexika suchen? score ausrechnen (w√∂rter in lexikon/l√§nge der topic-w√∂rter?)\n",
    "    # das aber nur zus√§tzlich zum manuellen machen\n",
    "    \n",
    "# testdaten laufen lassen\n",
    "    # gucken welches die h√∂chste topic-id ist\n",
    "    # schauen ob sie der id entspricht die ich oben aufgestellt habe und klassifikation danach machen\n",
    "    # confusion matrix etc berechnen\n",
    "\n",
    "# f√ºr jeden unbekannten satz den ich eingebe:\n",
    "    # in bow umrechnen\n",
    "    # topic-scores der vier klassen berechhnen\n",
    "    # klassifizieren\n",
    "    \n",
    "# den kram aufschreiben :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 10 topics\n",
    "#print(lda_model.show_topics(num_topics=10))\n",
    "# show all tokens that are part of a topic (only the top 2 words)\n",
    "#print(lda_model.get_topic_terms(topicid=0, topn=2))\n",
    "# get the word for a id in the dic\n",
    "#print(id2word[143])\n",
    "# print corpus\n",
    "#print(corpus[:1])\n",
    "# print corpus but with names, not with ids\n",
    "#print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "#visualize_lda(lda_model, corpus, dic)\n",
    "#lda_model.save(\"location.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pvis.enable_notebook()\n",
    "#vis = pvis.gensim.prepare(lda_model, corpus, dic)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_doc = [dic.doc2bow(sample) for sample in sentences_split[:1]]\n",
    "# ein neues doc muss ein satz sein (also eine liste)\n",
    "#new_doc2 = dic.doc2bow(*sentences_split[:1])\n",
    "#print(new_doc)\n",
    "#print(new_doc2)\n",
    "# get topics from a new document (fremd am besten)\n",
    "#top = lda_model.get_document_topics(new_doc, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "#top2 = lda_model.get_document_topics(new_doc2, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "# zeige alle topics in dem document\n",
    "#for i, x in enumerate(top):\n",
    "#    print(x)\n",
    "#for i, x in enumerate(top2):\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
