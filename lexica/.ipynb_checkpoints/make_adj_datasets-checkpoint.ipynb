{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lex_happiness = pd.read_csv(\"clean_happiness.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_sadness = pd.read_csv(\"clean_sadness.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_anger = pd.read_csv(\"clean_anger.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_fear = pd.read_csv(\"clean_fear.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "list_happiness = lex_happiness[\"stems\"].tolist()\n",
    "list_sadness = lex_sadness[\"stems\"].tolist()\n",
    "list_anger = pd.Series(lex_anger[\"stems\"].tolist())\n",
    "list_fear = lex_fear[\"stems\"].tolist()\n",
    "\n",
    "emotions = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "datasets = [lex_happiness, lex_sadness, lex_anger, lex_fear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dataset in enumerate(datasets):\n",
    "    new_dataset = dataset.copy()\n",
    "    new_dataset = new_dataset.drop(labels=[\"stems\", \"affect\"], axis=1)\n",
    "    \n",
    "    \n",
    "    text_rows = [row[1][\"text\"] for row in dataset.iterrows()]\n",
    "    doc = nlp(\" \".join(text_rows))\n",
    "    pos_rows = [token.pos for token in doc]\n",
    "    new_dataset[\"pos\"] = pos_rows\n",
    "    \n",
    "    \n",
    "    for index2,row in new_dataset.iterrows():\n",
    "        if row[2] != 84:\n",
    "            new_dataset = new_dataset.drop(index=index2, axis=0)\n",
    "    new_dataset = new_dataset.drop(labels=\"pos\", axis=1)\n",
    "    \n",
    "    \n",
    "    new_dataset.to_csv(\"clean_\" + emotions[index] + \"_adj.csv\", index=False, float_format='%.3f')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 90, 'det'), ('car', 92, 'nsubj'), ('is', 100, 'ROOT'), ('a', 90, 'det'), ('car', 92, 'attr'), ('with', 85, 'prep'), ('lights', 92, 'pobj')]\n",
      "i1: [1, 6]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#doc = nlp(\"I ate a banana\")\n",
    "#displacy.render(doc, style=\"dep\")\n",
    "doc = nlp(\"This car is a car with lights\")\n",
    "highest_score = 0.29\n",
    "#displacy.render(doc, style=\"dep\")\n",
    "\n",
    "bot_output_dict = {\n",
    "    \"text\": [token.text for token in doc],\n",
    "    \"pos\": [token.pos for token in doc],\n",
    "    \"dep\": [token.dep_ for token in doc]\n",
    "}\n",
    "\n",
    "print([(token.text, token.pos, token.dep_) for token in doc])\n",
    "\n",
    "\n",
    "if bot_output_dict[\"pos\"][0] == 92:\n",
    "    num_nouns = bot_output_dict[\"pos\"].count(92) - 1\n",
    "else: \n",
    "    num_nouns = bot_output_dict[\"pos\"].count(92)\n",
    "    \n",
    "num_nouns *= highest_score\n",
    "num_nouns = round(num_nouns)\n",
    "    \n",
    "\n",
    "indices = []\n",
    "\n",
    "if num_nouns > 0:\n",
    "    indices = [i for i, item in enumerate(bot_output_dict[\"text\"]) \n",
    "               if bot_output_dict[\"pos\"][i] == 92\n",
    "              and (bot_output_dict[\"dep\"][i] == \"nsubj\" \n",
    "                   or bot_output_dict[\"dep\"][i] == \"dobj\"\n",
    "                  or bot_output_dict[\"dep\"][i] == \"pobj\")]\n",
    "\n",
    "    print(\"i1:\", indices)\n",
    "    \n",
    "    \n",
    "    \n",
    "# randomly select num_nouns from list\n",
    "random.shuffle(indices)\n",
    "indices_to_replace = indices[:num_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.986, 0.969, 0.954, 0.922, 0.922, 0.922, 0.912, 0.907, 0.906, 0.879, 0.879, 0.877, 0.875, 0.868, 0.863, 0.859, 0.859, 0.853, 0.833, 0.833, 0.828, 0.824, 0.818, 0.818, 0.818, 0.818, 0.816, 0.814, 0.812, 0.812, 0.812, 0.812, 0.812, 0.812, 0.804, 0.797, 0.789, 0.788, 0.781, 0.781, 0.781, 0.779, 0.773, 0.773, 0.766, 0.766, 0.766, 0.761, 0.757, 0.75, 0.75, 0.742, 0.742, 0.742, 0.742, 0.736, 0.734, 0.734, 0.734, 0.728, 0.727, 0.727, 0.727, 0.727, 0.725, 0.723, 0.721, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.712, 0.712, 0.71, 0.703, 0.697, 0.688, 0.688, 0.688, 0.682, 0.682, 0.682, 0.672, 0.672, 0.667, 0.667, 0.66, 0.656, 0.641, 0.641, 0.641, 0.641, 0.636, 0.625, 0.625, 0.625, 0.625, 0.625, 0.621, 0.616, 0.613, 0.609, 0.609, 0.609, 0.609, 0.609, 0.606, 0.603, 0.601, 0.591, 0.591, 0.591, 0.588, 0.588, 0.583, 0.579, 0.578, 0.578, 0.578, 0.578, 0.576, 0.576, 0.576, 0.574, 0.562, 0.562, 0.562, 0.562, 0.561, 0.559, 0.551, 0.547, 0.547, 0.547, 0.546, 0.545, 0.544, 0.531, 0.531, 0.531, 0.531, 0.531, 0.531, 0.531, 0.53, 0.529, 0.519, 0.516, 0.516, 0.516, 0.516, 0.516, 0.516, 0.516, 0.515, 0.515, 0.515, 0.515, 0.515, 0.514, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "[0.696, 0.679, 0.664, 0.632, 0.632, 0.632, 0.622, 0.617, 0.616, 0.589, 0.589, 0.587, 0.585, 0.578, 0.573, 0.569, 0.569, 0.563, 0.543, 0.543, 0.538, 0.534, 0.528, 0.528, 0.528, 0.528, 0.526, 0.524, 0.522, 0.522, 0.522, 0.522, 0.522, 0.522, 0.514, 0.507, 0.499, 0.498, 0.491, 0.491, 0.491, 0.489, 0.483, 0.483, 0.476, 0.476, 0.476, 0.471, 0.467, 0.46, 0.46, 0.452, 0.452, 0.452, 0.452, 0.446, 0.444, 0.444, 0.444, 0.438, 0.437, 0.437, 0.437, 0.437, 0.435, 0.433, 0.431, 0.429, 0.429, 0.429, 0.429, 0.429, 0.429, 0.422, 0.422, 0.42, 0.413, 0.407, 0.398, 0.398, 0.398, 0.392, 0.392, 0.392, 0.382, 0.382, 0.377, 0.377, 0.37, 0.366, 0.351, 0.351, 0.351, 0.351, 0.346, 0.335, 0.335, 0.335, 0.335, 0.335, 0.331, 0.326, 0.323, 0.319, 0.319, 0.319, 0.319, 0.319, 0.316, 0.313, 0.311, 0.301, 0.301, 0.301, 0.298, 0.298, 0.293, 0.289, 0.288, 0.288, 0.288, 0.288, 0.286, 0.286, 0.286, 0.284, 0.272, 0.272, 0.272, 0.272, 0.271, 0.269, 0.261, 0.257, 0.257, 0.257, 0.256, 0.255, 0.254, 0.241, 0.241, 0.241, 0.241, 0.241, 0.241, 0.241, 0.24, 0.239, 0.229, 0.226, 0.226, 0.226, 0.226, 0.226, 0.226, 0.226, 0.225, 0.225, 0.225, 0.225, 0.225, 0.224, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21, 0.21]\n",
      "{6: 'soulful', 1: 'satisfied'}\n"
     ]
    }
   ],
   "source": [
    "# now look for an an adjective\n",
    "lexicon = pd.read_csv(\"clean_happiness_adj.csv\", delimiter=\",\", dtype={\"text\": str, \"intensity\": float}, float_precision='round_trip')\n",
    "\n",
    "# make it to list\n",
    "lexicon_text_list = lexicon[\"text\"].tolist()\n",
    "lexicon_intensity_list = lexicon[\"intensity\"].tolist()\n",
    "print(lexicon_intensity_list)\n",
    "# errechne die scores, wenn \n",
    "scores_dict = [round(abs(intensity - highest_score), 3) for intensity in lexicon_intensity_list]\n",
    "print(scores_dict)\n",
    "\n",
    "new_adjectives = {}\n",
    "\n",
    "for number in indices: \n",
    "    new_adjective_index = scores_dict.index(min(scores_dict))\n",
    "    scores_dict[new_adjective_index] = 99\n",
    "    new_adjectives[number] = lexicon_text_list[new_adjective_index] \n",
    "\n",
    "    \n",
    "    \n",
    "print(new_adjectives)\n",
    "\n",
    "#min = min([abs(highest_score - item) for i, item in enumerate(lexicon_intensity_list)])\n",
    "#print(min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'car', 'is', 'a', 'car', 'with', 'lights']\n",
      "['This', 'car', 'is', 'a', 'car', 'with', 'lights']\n"
     ]
    }
   ],
   "source": [
    "bot_output_dict[\"text\"] = [token.text for token in doc]\n",
    "print(bot_output_dict[\"text\"])\n",
    "\n",
    "for key, item in new_adjectives.items(): \n",
    "    bot_output_dict[\"text\"].insert(key, item)\n",
    "    \n",
    "print(bot_output_dict[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
