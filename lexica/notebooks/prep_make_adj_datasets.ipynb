{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lex_happiness = pd.read_csv(\"clean_happiness.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_sadness = pd.read_csv(\"clean_sadness.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_anger = pd.read_csv(\"clean_anger.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "lex_fear = pd.read_csv(\"clean_fear.csv\", delimiter=\",\", dtype={\"text\": str, \"affect\": str, \"stems\": str})\n",
    "list_happiness = lex_happiness[\"stems\"].tolist()\n",
    "list_sadness = lex_sadness[\"stems\"].tolist()\n",
    "list_anger = pd.Series(lex_anger[\"stems\"].tolist())\n",
    "list_fear = lex_fear[\"stems\"].tolist()\n",
    "\n",
    "emotions = [\"happiness\", \"sadness\", \"anger\", \"fear\"]\n",
    "datasets = [lex_happiness, lex_sadness, lex_anger, lex_fear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dataset in enumerate(datasets):\n",
    "    new_dataset = dataset.copy()\n",
    "    new_dataset = new_dataset.drop(labels=[\"stems\", \"affect\"], axis=1)\n",
    "    \n",
    "    \n",
    "    text_rows = [row[1][\"text\"] for row in dataset.iterrows()]\n",
    "    doc = nlp(\" \".join(text_rows))\n",
    "    pos_rows = [token.pos for token in doc]\n",
    "    new_dataset[\"pos\"] = pos_rows\n",
    "    \n",
    "    \n",
    "    for index2,row in new_dataset.iterrows():\n",
    "        if row[2] != 84:\n",
    "            new_dataset = new_dataset.drop(index=index2, axis=0)\n",
    "    new_dataset = new_dataset.drop(labels=\"pos\", axis=1)\n",
    "    \n",
    "    \n",
    "    new_dataset.to_csv(\"clean_\" + emotions[index] + \"_adj.csv\", index=False, float_format='%.3f')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3e9faa9b26354bec97d0e6a80f3c3913-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">house</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">green</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3e9faa9b26354bec97d0e6a80f3c3913-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 90, 'det'), ('car', 92, 'nsubj'), ('is', 100, 'ROOT'), ('a', 90, 'det'), ('car', 92, 'attr'), ('with', 85, 'prep'), ('lights', 92, 'pobj')]\n",
      "i1: [1, 6]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#doc = nlp(\"I ate a banana\")\n",
    "#displacy.render(doc, style=\"dep\")\n",
    "doc = nlp(\"This car is a car with lights\")\n",
    "highest_score = 0.29\n",
    "displacy.render(nlp(\"The house is green\"), style=\"dep\")\n",
    "\n",
    "bot_output_dict = {\n",
    "    \"text\": [token.text for token in doc],\n",
    "    \"pos\": [token.pos for token in doc],\n",
    "    \"dep\": [token.dep_ for token in doc]\n",
    "}\n",
    "\n",
    "print([(token.text, token.pos, token.dep_) for token in doc])\n",
    "\n",
    "\n",
    "if bot_output_dict[\"pos\"][0] == 92:\n",
    "    num_nouns = bot_output_dict[\"pos\"].count(92) - 1\n",
    "else: \n",
    "    num_nouns = bot_output_dict[\"pos\"].count(92)\n",
    "    \n",
    "num_nouns *= highest_score\n",
    "num_nouns = round(num_nouns)\n",
    "    \n",
    "\n",
    "indices = []\n",
    "\n",
    "if num_nouns > 0:\n",
    "    indices = [i for i, item in enumerate(bot_output_dict[\"text\"]) \n",
    "               if bot_output_dict[\"pos\"][i] == 92\n",
    "              and (bot_output_dict[\"dep\"][i] == \"nsubj\" \n",
    "                   or bot_output_dict[\"dep\"][i] == \"dobj\"\n",
    "                  or bot_output_dict[\"dep\"][i] == \"pobj\")]\n",
    "\n",
    "    print(\"i1:\", indices)\n",
    "    \n",
    "    \n",
    "    \n",
    "# randomly select num_nouns from list\n",
    "random.shuffle(indices)\n",
    "indices_to_replace = indices[:num_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "# now look for an an adjective\n",
    "lexicon = pd.read_csv(\"clean_happiness_adj.csv\", delimiter=\",\", dtype={\"text\": str, \"intensity\": float}, float_precision='round_trip')\n",
    "lexicon_text_list = lexicon[\"text\"].tolist()\n",
    "lexicon_intensity_list = lexicon[\"intensity\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.0\n",
      "[1]\n",
      "My visionary car is a soulful house with satisfied lights\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def insert_adjectives(bot_output, lex, list_text, list_score, highest_score):\n",
    "    doc = nlp(bot_output)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    dep = [token.dep_ for token in doc]\n",
    "    noun_indices = []\n",
    "    \n",
    "    for index, token in enumerate(doc):\n",
    "        pass\n",
    "        #print(index, token, pos[index], dep[index])\n",
    "    \n",
    "    # save indices of words that could receive an adjective\n",
    "    noun_indices = [index for index, token in enumerate(doc) if pos[index] == \"NOUN\" and (dep[index] == \"nsubj\" or dep[index] == \"dobj\" or dep[index] == \"pobj\" or dep[index] == \"attr\")]\n",
    "    if 0 in noun_indices: noun_indices.remove(0)\n",
    "    noun_indices = noun_indices[:int(abs(round(len(noun_indices) * highest_score, 0)))]\n",
    "    \n",
    "    # calculate the distances of the intensity score and the highest emotion score\n",
    "    distances = [round(abs(row[\"intensity\"] -highest_score), 3) for index, row in lex.iterrows()]\n",
    "    lex[\"distances\"] = distances\n",
    "    lex = lex.sort_values(\"distances\", ascending=True)\n",
    "    \n",
    "    # get the new adjectives and make replacement dictionary from indices and text\n",
    "    adjectives = lex[\"text\"][:len(noun_indices)].tolist()\n",
    "    adjectives = dict(zip(noun_indices, adjectives))\n",
    "    # order the entries in descending order or else the replacement in output will shift\n",
    "    adjectives = collections.OrderedDict(sorted(adjectives.items(), reverse=True))\n",
    "    \n",
    "    \n",
    "    # make output to list, insert adjectives and convert back to string \n",
    "    bot_output = bot_output.split(\" \")\n",
    "    for index, adjective in adjectives.items(): \n",
    "        bot_output.insert(index, adjective)\n",
    "    \n",
    "    return \" \".join(bot_output)\n",
    "    \n",
    "    \n",
    "bot_output = \"My car is a house with lights\"\n",
    "print(insert_adjectives(bot_output, lexicon, lexicon_text_list, lexicon_intensity_list, 0.32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
